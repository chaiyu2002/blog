<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL 的 DDL 的一些整理]]></title>
    <url>%2Fmysql%2Fmysql-ddl-note.html</url>
    <content type="text"><![CDATA[概要MySQL：DDL 的一些整理。 博客IT老兵博客。 前言关于 MYSQL 对于表结构的修改，一直存在一些函数的地方，这里，参考一篇外贴，做一下好好的整理。 （上面这句话中还存在一个错别字，函数，应该是含糊，这说明这篇文章后来没有再仔细阅读过。2020-03-05） 正文 MySQL ALTER TABLE: ALTER vs CHANGE vs MODIFY COLUMNWhenever I have to change a column in MySQL (which isn’t that often), I always forget the difference between ALTER COLUMN, CHANGE COLUMN, and MODIFY COLUMN. Here’s a handy reference. 这哥们也遇到了对于 alter，change 和 modify 的困扰，所以做了一个整理。 ALTER COLUMNUsed to set or remove the default value for a column. Example: 12ALTER TABLE MyTable ALTER COLUMN foo SET DEFAULT 'bar';ALTER TABLE MyTable ALTER COLUMN foo DROP DEFAULT; alter 常常用来设置或者移除一列的默认值。 CHANGE COLUMNUsed to rename a column, change its datatype, or move it within the schema. Example: 12ALTER TABLE MyTable CHANGE COLUMN foo bar VARCHAR(32) NOT NULL FIRST;ALTER TABLE MyTable CHANGE COLUMN foo bar VARCHAR(32) NOT NULL AFTER baz; change 用来重命名一列，修改数据类型，或者在模式中移动它。 MODIFY COLUMNUsed to do everything CHANGE COLUMN can, but without renaming the column. Example: 1ALTER TABLE MyTable MODIFY COLUMN foo VARCHAR(32) NOT NULL AFTER baz; modify 除了不能重命名一个列，可以做 change 的所有工作。 官网的摘录： CHANGE:Can rename a column and change its definition, or both.Has more capability than MODIFY, but at the expense of convenience for some operations.change 可以修改一列和修改它的定义，或者二者。比 modify 的用处更多，但是牺牲了一些便捷性？CHANGE：requires naming the column twice if not renaming it.With FIRST or AFTER, can reorder columns.MODIFY:Can change a column definition but not its name.More convenient than CHANGE to change a column definition without renaming it.With FIRST or AFTER, can reorder columns.ALTER: Used only to change a column default value. 总结感觉这么总结，还是有些内容没有整理清楚，先放一放，等到有时间，还需要想一下，有条理地整理一下。 参考https://hoelz.ro/ref/mysql-alter-table-alter-change-modify-columnhttps://dev.mysql.com/doc/refman/5.7/en/alter-table.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis 的 mapper-locations 引发的]]></title>
    <url>%2Fjava%2Fmybatis-mapper-locations.html</url>
    <content type="text"><![CDATA[概要MyBatis 的 mapper-locations 引发的。 博客博客地址：IT老兵驿站 正文之前做项目，用的是 eclipse，项目是 gradle 管理的 SpringMVC 的项目，使用到 MyBatis，Mapper 的 Java文件和实现的 xml 文件放在一个目录下就可以，不需要做额外的工作。 最近使用 Idea 开发 Maven 管理的 SpringBoot 项目，发现 xml 文件和 Mapper 文件没有放在一起，xml 文件单独放在了 resources 目录下面，这种放置方式，让我感觉很不习惯，接口和实现放在了两个不同的地方。 于是，我把 xml 移动到了 Mapper 的目录下，这个时候，问题产生了，编译时找不到 xml 文件了，我觉得这可能是 SpringBoot 的问题，根据这个查找关键字，找到了 Mybatis 手册，这里面需要修改一项配置： mapper-locations Locations of Mapper xml config file. 因为使用的文件是 application.properties，所以相对应的配置项就是 mybatis.mapper-locations，如果是别的配置文件（xml 或者 yml），配置项的名称又有区别。（这里其实就是 Java 项目的一个难点，发展的时间长了，太多沉淀下来的东西，原因已经不被人知道了） 将这个地方修改成 xml 所在的包，编译，发现仍然报告错误，这个时候，在网上没有了对于这个问题明确的答案了，只能自己尝试了。 参考这里，换了一种写法：这种写法，说是 Ant-style， 从 Ant 借鉴过来的，可以扫描所有 sample.config.mappers 下面的包。 查了 Ant 的官网，Ant-style 好像仅仅是说 “*” 是会递归当前目录及子目录，而没有说“classpath” 的意思。 继续检索，在这里找到了说法：这里的意思是说，classpath*:conf/appContext.xml 是指所有在 classpath 下面的 conf 目录下的 appContext.xml 都会被检索，并且合并为一个文件。 而 Spring 的官网的 resources 的介绍对此有很详细的讲解。 但是，哪怕使用了上面这样带有通配符的方式，还是找不到 xml 文件，这个时候，我开始怀疑 Idea 并没有把 xml 拷贝到被查找的目标位置，于是，我手动把 xml 拷贝到 target 下面相应的路径下，这时，编译成功。 带着这个结论，再去研究 Maven 的文档（而另外一篇对于 pom 的学习，刚好就忽略了对于 resources 的学习，认为不重要），大概就明白了，Idea 在执行 Maven 的编译，并没有拷贝 resources 目录之外的资源，需要手动配置一下，但是感觉 eclipse 是默认就会拷贝过去的。 总结这个问题，大体搞明白了，还有一些细节，还需要进一步细化。发现，Java 的很多问题，有可能会牵扯到好几个框架，牵扯到一些历史原因，想找到原因，还真是不简单 参考https://stackoverflow.com/questions/3294423/spring-classpath-prefix-differencehttps://maven.apache.org/pom.html#Resourceshttps://docs.spring.io/spring/docs/3.0.x/spring-framework-reference/html/resources.html#resources-classpath-wildcardshttps://mybatis.org/spring/factorybean.htmlhttps://mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Maven</tag>
        <tag>mapper-locations</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马丁富勒微服务论文学习]]></title>
    <url>%2Fsoa%2Fmicroservice-article-note.html</url>
    <content type="text"><![CDATA[前言过去几年，对 SOA 的几本书一直没有读明白，在工作中，也一直在不断去思考和研究，现在更多的是遇到微服务这个概念，所以需要好好研究一下微服务。 研究微服务，自然躲不开要读一读马丁富勒的这篇论文了，之前读过几遍，但是感觉还是有必要记录一下笔记。 其实在参考里面的这篇原文的地址里面，已经有了中文的翻译，不过感觉还是应该自己详细研读一下，更得来的比较深刻。 自然，对这么有深度的文章的学习，也不可能一蹴而就，只能是愚公移山式地，逐步完成。 博客博客地址：IT老兵驿站。 正文 a definition of this new architectural termThe term “Microservice Architecture” has sprung up over the last few years to describe a particular way of designing software applications as suites of independently deployable services. While there is no precise definition of this architectural style, there are certain common characteristics around organization around business capability, automated deployment, intelligence in the endpoints, and decentralized control of languages and data. 微服务架构这个术语再过去一些年很风靡，尽管对其还没有一个精确的定义，但是对于业务能力、自动化部署、节点的智能、语言和数据的去中心化的控制却有了相当的共识。 “Microservices” - yet another new term on the crowded streets of software architecture. Although our natural inclination is to pass such things by with a contemptuous glance, this bit of terminology describes a style of software systems that we are finding more and more appealing. We’ve seen many projects use this style in the last few years, and results so far have been positive, so much so that for many of our colleagues this is becoming the default style for building enterprise applications. Sadly, however, there’s not much information that outlines what the microservice style is and how to do it. “Although our natural inclination is to pass such things by with a contemptuous glance” —- 无论中国人，还是外国人，对待新事物的态度看来是一致的，就是投以轻蔑的一瞥。 In short, the microservice architectural style [1] is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. 微服务架构是一种方法，用来开发由一套小的 service 组成的一个应用，每一个 service 运行在它自己的进程中，互相通信使用一种轻量级的机制，一般是 HTTP resource API。 这些服务都是围绕着业务来构建，独立部署。 几乎没有中心化管理。 To start explaining the microservice style it’s useful to compare it to the monolithic style: a monolithic application built as a single unit. Enterprise Applications are often built in three main parts: a client-side user interface (consisting of HTML pages and javascript running in a browser on the user’s machine) a database (consisting of many tables inserted into a common, and usually relational, database management system), and a server-side application. The server-side application will handle HTTP requests, execute domain logic, retrieve and update data from the database, and select and populate HTML views to be sent to the browser. This server-side application is a monolith - a single logical executable[2]. Any changes to the system involve building and deploying a new version of the server-side application. “monolithic style”：传统的风格，一体化编程，三层结构，这里可以参考一本书《企业级架构》，那里讲了基本的企业级三层架构，一般的 Java 项目都是遵循着那本书的规则。 Such a monolithic server is a natural way to approach building such a system. All your logic for handling a request runs in a single process, allowing you to use the basic features of your language to divide up the application into classes, functions, and namespaces. With some care, you can run and test the application on a developer’s laptop, and use a deployment pipeline to ensure that changes are properly tested and deployed into production. You can horizontally scale the monolith by running many instances behind a load-balancer. 传统的实现方式。你的所有的对于处理一个请求的逻辑都运行在一个单独的进程中（其实，这一点对于 Java 来说，是一个线程），允许你把应用程序分割为 classes，functions，namespaces。 Monolithic applications can be successful, but increasingly people are feeling frustrations with them - especially as more applications are being deployed to the cloud . Change cycles are tied together - a change made to a small part of the application, requires the entire monolith to be rebuilt and deployed. Over time it’s often hard to keep a good modular structure, making it harder to keep changes that ought to only affect one module within that module. Scaling requires scaling of the entire application rather than parts of it that require greater resource. 这种方式的问题：一个小的修改，需要这个项目完全地被重新构建和部署。很难保持一个好的模块结构，使得一些修改，本该只影响一个模块的，却影响到了多个模块—-这里是说这种三层架构的设计问题吧，在这种三层架构内，修改逻辑，往往是从 view 到 model 层都要修改，感觉是一种有些“垂直化”的修改，势必容易影响原本的别的逻辑—-这是我的理解，还需要继续向下研究。 These frustrations have led to the microservice architectural style: building applications as suites of services. As well as the fact that services are independently deployable and scalable, each service also provides a firm module boundary, even allowing for different services to be written in different programming languages. They can also be managed by different teams . 那么微服务的架构风格是：每一个 service 可以独立部署、可扩展，提供一个稳定的模块边界线。 We do not claim that the microservice style is novel or innovative, its roots go back at least to the design principles of Unix. But we do think that not enough people consider a microservice architecture and that many software developments would be better off if they used it. 微服务不是一种创新，对它的寻根可以追溯到 Unix 的设计理念。 Characteristics of a Microservice Architecture 微服务架构的特征We cannot say there is a formal definition of the microservices architectural style, but we can attempt to describe what we see as common characteristics for architectures that fit the label. As with any definition that outlines common characteristics, not all microservice architectures have all the characteristics, but we do expect that most microservice architectures exhibit most characteristics. While we authors have been active members of this rather loose community, our intention is to attempt a description of what we see in our own work and in similar efforts by teams we know of. In particular we are not laying down some definition to conform to. 微服务的正式的定义其实没有定论，我们只是尝试去描述一些公共的特征。 Componentization via Services 通过服务模块化For as long as we’ve been involved in the software industry, there’s been a desire to build systems by plugging together components, much in the way we see things are made in the physical world. During the last couple of decades we’ve seen considerable progress with large compendiums of common libraries that are part of most language platforms. When talking about components we run into the difficult definition of what makes a component. Our definition is that a component is a unit of software that is independently replaceable and upgradeable. Microservice architectures will use libraries, but their primary way of componentizing their own software is by breaking down into services. We define libraries as components that are linked into a program and called using in-memory function calls, while services are out-of-process components who communicate with a mechanism such as a web service request, or remote procedure call. (This is a different concept to that of a service object in many OO programs [3].) 这三段讲了一下 component 和 service 的区别，或者说 Componentization 的演进之路。 One main reason for using services as components (rather than libraries) is that services are independently deployable. If you have an application [4] that consists of a multiple libraries in a single process, a change to any single component results in having to redeploy the entire application. But if that application is decomposed into multiple services, you can expect many single service changes to only require that service to be redeployed. That’s not an absolute, some changes will change service interfaces resulting in some coordination, but the aim of a good microservice architecture is to minimize these through cohesive service boundaries and evolution mechanisms in the service contracts. 独立部署（independently deployable）是使用 service 来作为 components 的一个主要原因（这里是跟把一个 library 作为 component 来对比的）。其实这里的核心是如何设计包含很好的 cohesive service boundaries 凝聚的服务边界和 evolution mechanisms 进化机制的 service contracts 服务合约。 Another consequence of using services as components is a more explicit component interface. Most languages do not have a good mechanism for defining an explicit Published Interface. Often it’s only documentation and discipline that prevents clients breaking a component’s encapsulation, leading to overly-tight coupling between components. Services make it easier to avoid this by using explicit remote call mechanisms. Using services like this does have downsides. Remote calls are more expensive than in-process calls, and thus remote APIs need to be coarser-grained, which is often more awkward to use. If you need to change the allocation of responsibilities between components, such movements of behavior are harder to do when you’re crossing process boundaries. At a first approximation, we can observe that services map to runtime processes, but that is only a first approximation. A service may consist of multiple processes that will always be developed and deployed together, such as an application process and a database that’s only used by that service. service 和 runtime processes （运行时进程）的关系。At a first approximation—在最初的一种近似下，service 被映射成一个运行时进程，但是这也只能是一种近似。 Organized around Business Capabilities 围绕业务能力来组织When looking to split a large application into parts, often management focuses on the technology layer, leading to UI teams, server-side logic teams, and database teams. When teams are separated along these lines, even simple changes can lead to a cross-team project taking time and budgetary approval. A smart team will optimise around this and plump for the lesser of two evils - just force the logic into whichever application they have access to. Logic everywhere in other words. This is an example of Conway’s Law[5] in action. The microservice approach to division is different, splitting up into services organized around business capability. Such services take a broad-stack implementation of software for that business area, including user-interface, persistant storage, and any external collaborations. Consequently the teams are cross-functional, including the full range of skills required for the development: user-experience, database, and project management. One company organised in this way is www.comparethemarket.com. Cross functional teams are responsible for building and operating each product and each product is split out into a number of individual services communicating via a message bus. Large monolithic applications can always be modularized around business capabilities too, although that’s not the common case. Certainly we would urge a large team building a monolithic application to divide itself along business lines. The main issue we have seen here, is that they tend to be organised around too many contexts. If the monolith spans many of these modular boundaries it can be difficult for individual members of a team to fit them into their short-term memory. Additionally we see that the modular lines require a great deal of discipline to enforce. The necessarily more explicit separation required by service components makes it easier to keep the team boundaries clear. 这里讲了两个事情，但中心思想是一个，围绕着业务来组织，但是实现却截然相反。 这个概念和我15年写的一篇博客有相同的观点，技术是围绕着业务来组建的，而团队是围绕着业务来组建的。 这两个概念应该怎么理解呢？ 团队围绕着业务来组织，那么就不应该分成那么多个部门，那么多层级，10年在一家支付公司工作的时候，一个新业务的实现，从上到下，要有副总裁、总监、研发部门和业务部门的经理一起参与；横向呢，要有好几个部门的人打交道，那个时候的工作效率极其低下，各个部门不能说各怀心思吧，但总是会维护各自的利益，而一个业务还要副总裁去批示，这个流程，几乎赶上政府部门了。 后来去到互联网公司，这个公司里面，分成了五十几条产品线，没有那么多的自上而下的职级划分，明显效率就高了很多—-不过这家公司，现在也已经走上了层层职级划分的老路。 收藏过一篇腾讯的吴宵光的文章，讲了腾讯事业部的建立过程，其思路也是这样，围绕着业务来组织。 而对于程序来说，围绕着业务来组织，则正好相反，是讲每个 service 就围绕着一个业务来组织，清清楚楚、明明白白，互不影响，互不扯皮。 How big is a microservice?Although “microservice” has become a popular name for this architectural style, its name does lead to an unfortunate focus on the size of service, and arguments about what constitutes “micro”. In our conversations with microservice practitioners, we see a range of sizes of services. The largest sizes reported follow Amazon’s notion of the Two Pizza Team (i.e. the whole team can be fed by two pizzas), meaning no more than a dozen people. On the smaller size scale we’ve seen setups where a team of half-a-dozen would support half-a-dozen services. This leads to the question of whether there are sufficiently large differences within this size range that the service-per-dozen-people and service-per-person sizes shouldn’t be lumped under one microservices label. At the moment we think it’s better to group them together, but it’s certainly possible that we’ll change our mind as we explore this style further. 这里涉及到了一个题外话，微服务应该有多大？为什么叫微服务呢？这里是按照参与服务开发的任务来介绍了一下大小，最大没有超过12个人，小的六个人。 Products not Projects 产品而不是项目Most application development efforts that we see use a project model: where the aim is to deliver some piece of software which is then considered to be completed. On completion the software is handed over to a maintenance organization and the project team that built it is disbanded. Microservice proponents tend to avoid this model, preferring instead the notion that a team should own a product over its full lifetime. A common inspiration for this is Amazon’s notion of “you build, you run it” where a development team takes full responsibility for the software in production. This brings developers into day-to-day contact with how their software behaves in production and increases contact with their users, as they have to take on at least some of the support burden. The product mentality, ties in with the linkage to business capabilities. Rather than looking at the software as a set of functionality to be completed, there is an on-going relationship where the question is how can software assist its users to enhance the business capability. There’s no reason why this same approach can’t be taken with monolithic applications, but the smaller granularity of services can make it easier to create the personal relationships between service developers and their users. 这段讲的挺有意思，以产品的概念而不是项目去重新设计开发、交付、维护，不是说交付给别人，而是谁开发，谁维护。 Smart endpoints and dumb pipes 智能的终端和哑管道When building communication structures between different processes, we’ve seen many products and approaches that stress putting significant smarts into the communication mechanism itself. A good example of this is the Enterprise Service Bus (ESB), where ESB products often include sophisticated facilities for message routing, choreography, transformation, and applying business rules. 这里提到了 ESB，我并不太了解 ESB，但是参考维基和这里的概念，ESB 是一种通讯机制（遵循了 SOA 的设计理念），融入了很多重要的，意义重大的智能（smarts），例如消息路由，编排，转换，和应用业务规则—-而微服务的设计理念则不是这样，这样就说明了微服务和 SOA 还有有些区别的。 The microservice community favours an alternative approach: smart endpoints and dumb pipes. Applications built from microservices aim to be as decoupled and as cohesive as possible - they own their own domain logic and act more as filters in the classical Unix sense - receiving a request, applying logic as appropriate and producing a response. These are choreographed using simple RESTish protocols rather than complex protocols such as WS-Choreography or BPEL or orchestration by a central tool. 微服务采用了一种替代方案：智能的端点（endpoints）和哑管道，这里的管道即是指上面的 ESB 所承载的作用，通讯设施，业务是内聚的，而痛心机制是业务无知的，使用了 RESTish 的协议，而不是 WS-Choreography 或者 BPEL 或者被中心工具来编排。 The two protocols used most commonly are HTTP request-response with resource API’s and lightweight messaging[8]. The best expression of the first is Microservice teams use the principles and protocols that the world wide web (and to a large extent, Unix) is built on. Often used resources can be cached with very little effort on the part of developers or operations folk. The second approach in common use is messaging over a lightweight message bus. The infrastructure chosen is typically dumb (dumb as in acts as a message router only) - simple implementations such as RabbitMQ or ZeroMQ don’t do much more than provide a reliable asynchronous fabric - the smarts still live in the end points that are producing and consuming messages; in the services. In a monolith, the components are executing in-process and communication between them is via either method invocation or function call. The biggest issue in changing a monolith into microservices lies in changing the communication pattern. A naive conversion from in-memory method calls to RPC leads to chatty communications which don’t perform well. Instead you need to replace the fine-grained communication with a coarser -grained approach. 传输消息的装置，应该做到对业务无知，有一个单词是讲无知论的，我之前一直不理解这个单词的意思，现在越来越明白—-agnostic。 Microservices and SOAWhen we’ve talked about microservices a common question is whether this is just Service Oriented Architecture (SOA) that we saw a decade ago. There is merit to this point, because the microservice style is very similar to what some advocates of SOA have been in favor of. The problem, however, is that SOA means too many different things, and that most of the time that we come across something called “SOA” it’s significantly different to the style we’re describing here, usually due to a focus on ESBs used to integrate monolithic applications. In particular we have seen so many botched implementations of service orientation - from the tendency to hide complexity away in ESB’s [6], to failed multi-year initiatives that cost millions and deliver no value, to centralised governance models that actively inhibit change, that it is sometimes difficult to see past these problems. Certainly, many of the techniques in use in the microservice community have grown from the experiences of developers integrating services in large organisations. The Tolerant Reader pattern is an example of this. Efforts to use the web have contributed, using simple protocols is another approach derived from these experiences - a reaction away from central standards that have reached a complexity that is, frankly, breathtaking. (Any time you need an ontology to manage your ontologies you know you are in deep trouble.) This common manifestation of SOA has led some microservice advocates to reject the SOA label entirely, although others consider microservices to be one form of SOA [7], perhaps service orientation done right. Either way, the fact that SOA means such different things means it’s valuable to have a term that more crisply defines this architectural style. 这里讲了微服务和 SOA 的一些关系，还没有明确的一个定论。有人认为微服务是一种 SOA，维基上也是这么说的；另外一种声音是说微服务不同于 SOA。如果从上面的 ESB 这块来看，微服务和 SOA 是不同的。 Decentralized Governance 去中心的管理One of the consequences of centralised governance is the tendency to standardise on single technology platforms. Experience shows that this approach is constricting - not every problem is a nail and not every solution a hammer. We prefer using the right tool for the job and while monolithic applications can take advantage of different languages to a certain extent, it isn’t that common. Splitting the monolith’s components out into services we have a choice when building each of them. You want to use Node.js to standup a simple reports page? Go for it. C++ for a particularly gnarly near-real-time component? Fine. You want to swap in a different flavour of database that better suits the read behaviour of one component? We have the technology to rebuild him. Of course, just because you can do something, doesn’t mean you should - but partitioning your system in this way means you have the option. Teams building microservices prefer a different approach to standards too. Rather than use a set of defined standards written down somewhere on paper they prefer the idea of producing useful tools that other developers can use to solve similar problems to the ones they are facing. These tools are usually harvested from implementations and shared with a wider group, sometimes, but not exclusively using an internal open source model. Now that git and github have become the de facto version control system of choice, open source practices are becoming more and more common in-house . Netflix is a good example of an organisation that follows this philosophy. Sharing useful and, above all, battle-tested code as libraries encourages other developers to solve similar problems in similar ways yet leaves the door open to picking a different approach if required. Shared libraries tend to be focused on common problems of data storage, inter-process communication and as we discuss further below, infrastructure automation. For the microservice community, overheads are particularly unattractive. That isn’t to say that the community doesn’t value service contracts. Quite the opposite, since there tend to be many more of them. It’s just that they are looking at different ways of managing those contracts. Patterns like Tolerant Reader and Consumer-Driven Contracts are often applied to microservices. These aid service contracts in evolving independently. Executing consumer driven contracts as part of your build increases confidence and provides fast feedback on whether your services are functioning. Indeed we know of a team in Australia who drive the build of new services with consumer driven contracts. They use simple tools that allow them to define the contract for a service. This becomes part of the automated build before code for the new service is even written. The service is then built out only to the point where it satisfies the contract - an elegant approach to avoid the ‘YAGNI’[9] dilemma when building new software. These techniques and the tooling growing up around them, limit the need for central contract management by decreasing the temporal coupling between services. Many languages, many optionsThe growth of JVM as a platform is just the latest example of mixing languages within a common platform. It’s been common practice to shell-out to a higher level language to take advantage of higher level abstractions for decades. As is dropping down to the metal and writing performance sensitive code in a lower level one. However, many monoliths don’t need this level of performance optimisation nor are DSL’s and higher level abstractions that common (to our dismay). Instead monoliths are usually single language and the tendency is to limit the number of technologies in use [10]. Perhaps the apogee of decentralised governance is the build it / run it ethos popularised by Amazon. Teams are responsible for all aspects of the software they build including operating the software 24/7. Devolution of this level of responsibility is definitely not the norm but we do see more and more companies pushing responsibility to the development teams. Netflix is another organisation that has adopted this ethos[11]. Being woken up at 3am every night by your pager is certainly a powerful incentive to focus on quality when writing your code. These ideas are about as far away from the traditional centralized governance model as it is possible to be. 这里是讲，你可以使用各种语言和工具去实现服务，只要你觉得它合适。 Decentralized Data Management 去中心化的数据管理Decentralization of data management presents in a number of different ways. At the most abstract level, it means that the conceptual model of the world will differ between systems. This is a common issue when integrating across a large enterprise, the sales view of a customer will differ from the support view. Some things that are called customers in the sales view may not appear at all in the support view. Those that do may have different attributes and (worse) common attributes with subtly different semantics. 大同世界里面，大家因为立场不同，看待一个问题的角度都是不同的，那么对于不同的系统来说，看待和处理数据的方式也是不同的。 This issue is common between applications, but can also occur within applications, particular when that application is divided into separate components. A useful way of thinking about this is the Domain-Driven Design notion of Bounded Context. DDD divides a complex domain up into multiple bounded contexts and maps out the relationships between them. This process is useful for both monolithic and microservice architectures, but there is a natural correlation between service and context boundaries that helps clarify, and as we describe in the section on business capabilities, reinforce the separations. DDD Domain-Driven Design notion of Bounded Context 的设计概念。 As well as decentralizing decisions about conceptual models, microservices also decentralize data storage decisions. While monolithic applications prefer a single logical database for persistant data, enterprises often prefer a single database across a range of applications - many of these decisions driven through vendor’s commercial models around licensing. Microservices prefer letting each service manage its own database, either different instances of the same database technology, or entirely different database systems - an approach called Polyglot Persistence. You can use polyglot persistence in a monolith, but it appears more frequently with microservices. Decentralizing responsibility for data across microservices has implications for managing updates. The common approach to dealing with updates has been to use transactions to guarantee consistency when updating multiple resources. This approach is often used within monoliths. Using transactions like this helps with consistency, but imposes significant temporal coupling, which is problematic across multiple services. Distributed transactions are notoriously difficult to implement and as a consequence microservice architectures emphasize transactionless coordination between services, with explicit recognition that consistency may only be eventual consistency and problems are dealt with by compensating operations. Choosing to manage inconsistencies in this way is a new challenge for many development teams, but it is one that often matches business practice. Often businesses handle a degree of inconsistency in order to respond quickly to demand, while having some kind of reversal process to deal with mistakes. The trade-off is worth it as long as the cost of fixing mistakes is less than the cost of lost business under greater consistency. Infrastructure Automation 基础设施的自动化Infrastructure automation techniques have evolved enormously over the last few years - the evolution of the cloud and AWS in particular has reduced the operational complexity of building, deploying and operating microservices. Many of the products or systems being build with microservices are being built by teams with extensive experience of Continuous Delivery and it’s precursor, Continuous Integration. Teams building software this way make extensive use of infrastructure automation techniques. This is illustrated in the build pipeline shown below. Since this isn’t an article on Continuous Delivery we will call attention to just a couple of key features here. We want as much confidence as possible that our software is working, so we run lots of automated tests. Promotion of working software ‘up’ the pipeline means we automate deployment to each new environment. A monolithic application will be built, tested and pushed through these environments quite happlily. It turns out that once you have invested in automating the path to production for a monolith, then deploying more applications doesn’t seem so scary any more. Remember, one of the aims of CD is to make deployment boring, so whether its one or three applications, as long as its still boring it doesn’t matter[12]. Another area where we see teams using extensive infrastructure automation is when managing microservices in production. In contrast to our assertion above that as long as deployment is boring there isn’t that much difference between monoliths and microservices, the operational landscape for each can be strikingly different. 这里讲的似乎是要使用 CI/CD 这样的基础设施，这块其实无关于是微服务还是 monolithic 的服务。 Design for failure 为失败做好设计A consequence of using services as components, is that applications need to be designed so that they can tolerate the failure of services. Any service call could fail due to unavailability of the supplier, the client has to respond to this as gracefully as possible. This is a disadvantage compared to a monolithic design as it introduces additional complexity to handle it. The consequence is that microservice teams constantly reflect on how service failures affect the user experience. Netflix’s Simian Army induces failures of services and even datacenters during the working day to test both the application’s resilience and monitoring. This kind of automated testing in production would be enough to give most operation groups the kind of shivers usually preceding a week off work. This isn’t to say that monolithic architectural styles aren’t capable of sophisticated monitoring setups - it’s just less common in our experience. Since services can fail at any time, it’s important to be able to detect the failures quickly and, if possible, automatically restore service. Microservice applications put a lot of emphasis on real-time monitoring of the application, checking both architectural elements (how many requests per second is the database getting) and business relevant metrics (such as how many orders per minute are received). Semantic monitoring can provide an early warning system of something going wrong that triggers development teams to follow up and investigate. This is particularly important to a microservices architecture because the microservice preference towards choreography and event collaboration leads to emergent behavior. While many pundits praise the value of serendipitous emergence, the truth is that emergent behavior can sometimes be a bad thing. Monitoring is vital to spot bad emergent behavior quickly so it can be fixed. Synchronous calls considered harmful 同步调用被认为是有害的Any time you have a number of synchronous calls between services you will encounter the multiplicative effect of downtime. Simply, this is when the downtime of your system becomes the product of the downtimes of the individual components. You face a choice, making your calls asynchronous or managing the downtime. At www.guardian.co.uk they have implemented a simple rule on the new platform - one synchronous call per user request while at Netflix, their platform API redesign has built asynchronicity into the API fabric. 任何时候，如果在服务间有很多的同步调用，你将会遭遇很多宕机时间（downtime）的影响，一个独立组件的宕机就会造成整个系统的宕机。 Monoliths can be built to be as transparent as a microservice - in fact, they should be. The difference is that you absolutely need to know when services running in different processes are disconnected. With libraries within the same process this kind of transparency is less likely to be useful. Microservice teams would expect to see sophisticated monitoring and logging setups for each individual service such as dashboards showing up/down status and a variety of operational and business relevant metrics. Details on circuit breaker status, current throughput and latency are other examples we often encounter in the wild. Evolutionary Design 进化的设计Microservice practitioners, usually have come from an evolutionary design background and see service decomposition as a further tool to enable application developers to control changes in their application without slowing down change. Change control doesn’t necessarily mean change reduction - with the right attitudes and tools you can make frequent, fast, and well-controlled changes to software. 这段很值得回味，通常微服务的实践者，都是来自于一个进化的设计背景（理解和支持进化设计的概念的背景），会把服务的分解看作一个更进一步的工具来让程序开发者去控制变化而不用减慢变化的速度。 Whenever you try to break a software system into components, you’re faced with the decision of how to divide up the pieces - what are the principles on which we decide to slice up our application? The key property of a component is the notion of independent replacement and upgradeability[13] - which implies we look for points where we can imagine rewriting a component without affecting its collaborators. Indeed many microservice groups take this further by explicitly expecting many services to be scrapped rather than evolved in the longer term. 无论什么时候，你打算去把一个软件系统拆分成组件，你都要面对一个决定，如何去分解–我们决定去分解我们的应用的原则是什么呢？关键点是独立的可替换和更新。 The Guardian website is a good example of an application that was designed and built as a monolith, but has been evolving in a microservice direction. The monolith still is the core of the website, but they prefer to add new features by building microservices that use the monolith’s API. This approach is particularly handy for features that are inherently temporary, such as specialized pages to handle a sporting event. Such a part of the website can quickly be put together using rapid development languages, and removed once the event is over. We’ve seen similar approaches at a financial institution where new services are added for a market opportunity and discarded after a few months or even weeks. This emphasis on replaceability is a special case of a more general principle of modular design, which is to drive modularity through the pattern of change [14]. You want to keep things that change at the same time in the same module. Parts of a system that change rarely should be in different services to those that are currently undergoing lots of churn. If you find yourself repeatedly changing two services together, that’s a sign that they should be merged. Putting components into services adds an opportunity for more granular release planning. With a monolith any changes require a full build and deployment of the entire application. With microservices, however, you only need to redeploy the service(s) you modified. This can simplify and speed up the release process. The downside is that you have to worry about changes to one service breaking its consumers. The traditional integration approach is to try to deal with this problem using versioning, but the preference in the microservice world is to only use versioning as a last resort. We can avoid a lot of versioning by designing services to be as tolerant as possible to changes in their suppliers. Are Microservices the Future? 微服务是我们的未来吗Our main aim in writing this article is to explain the major ideas and principles of microservices. By taking the time to do this we clearly think that the microservices architectural style is an important idea - one worth serious consideration for enterprise applications. We have recently built several systems using the style and know of others who have used and favor this approach. Those we know about who are in some way pioneering the architectural style include Amazon, Netflix, The Guardian, the UK Government Digital Service, realestate.com.au, Forward and comparethemarket.com. The conference circuit in 2013 was full of examples of companies that are moving to something that would class as microservices - including Travis CI. In addition there are plenty of organizations that have long been doing what we would class as microservices, but without ever using the name. (Often this is labelled as SOA - although, as we’ve said, SOA comes in many contradictory forms. [15]) Despite these positive experiences, however, we aren’t arguing that we are certain that microservices are the future direction for software architectures. While our experiences so far are positive compared to monolithic applications, we’re conscious of the fact that not enough time has passed for us to make a full judgement. Our colleague Sam Newman spent most of 2014 working on a book that captures our experiences with building microservices. This should be your next step if you want a deeper dive into the topic. Often the true consequences of your architectural decisions are only evident several years after you made them. We have seen projects where a good team, with a strong desire for modularity, has built a monolithic architecture that has decayed over the years. Many people believe that such decay is less likely with microservices, since the service boundaries are explicit and hard to patch around. Yet until we see enough systems with enough age, we can’t truly assess how microservice architectures mature. There are certainly reasons why one might expect microservices to mature poorly. In any effort at componentization, success depends on how well the software fits into components. It’s hard to figure out exactly where the component boundaries should lie. Evolutionary design recognizes the difficulties of getting boundaries right and thus the importance of it being easy to refactor them. But when your components are services with remote communications, then refactoring is much harder than with in-process libraries. Moving code is difficult across service boundaries, any interface changes need to be coordinated between participants, layers of backwards compatibility need to be added, and testing is made more complicated. Another issue is If the components do not compose cleanly, then all you are doing is shifting complexity from inside a component to the connections between components. Not just does this just move complexity around, it moves it to a place that’s less explicit and harder to control. It’s easy to think things are better when you are looking at the inside of a small, simple component, while missing messy connections between services. Finally, there is the factor of team skill. New techniques tend to be adopted by more skillful teams. But a technique that is more effective for a more skillful team isn’t necessarily going to work for less skillful teams. We’ve seen plenty of cases of less skillful teams building messy monolithic architectures, but it takes time to see what happens when this kind of mess occurs with microservices. A poor team will always create a poor system - it’s very hard to tell if microservices reduce the mess in this case or make it worse. One reasonable argument we’ve heard is that you shouldn’t start with a microservices architecture. Instead begin with a monolith, keep it modular, and split it into microservices once the monolith becomes a problem. (Although this advice isn’t ideal, since a good in-process interface is usually not a good service interface.) So we write this with cautious optimism. So far, we’ve seen enough about the microservice style to feel that it can be a worthwhile road to tread. We can’t say for sure where we’ll end up, but one of the challenges of software development is that you can only make decisions based on the imperfect information that you currently have to hand. 总结这篇文章内容太多，只能是一天一天聚沙成塔、集腋成裘地去理解，没打算完整地去翻译，翻译得过细，再读时，感觉会忽略原文，还是记录一些从阅读原文中获得的一些理解更为合理。 参考https://martinfowler.com/articles/microservices.htmlhttps://www.w3.org/TR/ws-chor-model/ 介绍了 WS-Choreographyhttps://en.wikipedia.org/wiki/Business_Process_Execution_Language 介绍了 BPEL]]></content>
      <categories>
        <category>SOA</category>
      </categories>
      <tags>
        <tag>microservice</tag>
        <tag>soa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pom 文件学习（未完待续）]]></title>
    <url>%2Fjava%2Fjava-maven-pom-note.html</url>
    <content type="text"><![CDATA[前言大概几十年前搞一个什么项目，研究 Makefile，看一些帖子说有 ant 这么一个东西，比 Makefile 要方便，然后又接触到了 mvn， 使用过 mvn install 这个命令—-现在好像这个 install 命令都不怎么看得到了 —- 一晃这么多年，又要和它打交道了，不过最近这一年来做 maven 项目，不知道看了多少遍官网了，感觉还是没有梳理清楚，这说明 maven 还真不是那么简单就能掌握的，老办法，还是要做做笔记整理整理，这篇文章记录对 pom 文件的学习。 对于我来说，学习的思路，都是充分阅读官网（见参考），这个网页十分长，估计要分多次来记录笔记了。 博客博客地址：IT老兵驿站。 正文先看一个 pom 文件。 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- The Basics --&gt; &lt;groupId&gt;...&lt;/groupId&gt; &lt;artifactId&gt;...&lt;/artifactId&gt; &lt;version&gt;...&lt;/version&gt; &lt;packaging&gt;...&lt;/packaging&gt; &lt;dependencies&gt;...&lt;/dependencies&gt; &lt;parent&gt;...&lt;/parent&gt; &lt;dependencyManagement&gt;...&lt;/dependencyManagement&gt; &lt;modules&gt;...&lt;/modules&gt; &lt;properties&gt;...&lt;/properties&gt; &lt;!-- Build Settings --&gt; &lt;build&gt;...&lt;/build&gt; &lt;reporting&gt;...&lt;/reporting&gt; &lt;!-- More Project Information --&gt; &lt;name&gt;...&lt;/name&gt; &lt;description&gt;...&lt;/description&gt; &lt;url&gt;...&lt;/url&gt; &lt;inceptionYear&gt;...&lt;/inceptionYear&gt; &lt;licenses&gt;...&lt;/licenses&gt; &lt;organization&gt;...&lt;/organization&gt; &lt;developers&gt;...&lt;/developers&gt; &lt;contributors&gt;...&lt;/contributors&gt; &lt;!-- Environment Settings --&gt; &lt;issueManagement&gt;...&lt;/issueManagement&gt; &lt;ciManagement&gt;...&lt;/ciManagement&gt; &lt;mailingLists&gt;...&lt;/mailingLists&gt; &lt;scm&gt;...&lt;/scm&gt; &lt;prerequisites&gt;...&lt;/prerequisites&gt; &lt;repositories&gt;...&lt;/repositories&gt; &lt;pluginRepositories&gt;...&lt;/pluginRepositories&gt; &lt;distributionManagement&gt;...&lt;/distributionManagement&gt; &lt;profiles&gt;...&lt;/profiles&gt;&lt;/project&gt; 这是一个标准的 pom 文件的格式，摘自官网。 整篇文章就是围绕上面的这个格式来解释，分为了4个部分。 Basics 基础部分Maven Coordinates 坐标groupId、artifactId、version 是所有需要的字段，它们也可以从一个 parent 文件中继承，这个时候它们可以不被明确定义。这三个字段扮演的就像地址和时间戳，它们在 Maven 项目中就像坐标系统一样，在一个仓库里面指定了一个特定的位置。 groupId：在一个项目或者一个组织中是唯一的。 artifactId：制品Id是指项目的名字。 version：版本。 packaging打包的目标，可用的有 pom、jar、maven-plugin、ejb、war、ear、rar。默认是 jar。 POM 关系 One powerful aspect of Maven is its handling of project relationships: this includes dependencies (and transitive dependencies), inheritance, and aggregation (multi-module projects). Maven 一个很强的地方是它的对于项目关系的处理，包含了依赖（transitive dependencies，在这里可能应该理解为传递依赖），继承，和聚合（多模块项目）。 Dependency management has a long tradition of being a complicated mess for anything but the most trivial of projects. “Jarmageddon” quickly ensues as the dependency tree becomes large and complicated. “Jar Hell” follows, where versions of dependencies on one system are not equivalent to the versions developed with, either by the wrong version given, or conflicting versions between similarly named jars.Maven solves both problems through a common local repository from which to link projects correctly, versions and all. 这里面，Jarmageddon 和 Jar Hell 应该是两种常见的项目依赖的问题，不过我暂时没有找到准确的解释，先放一下。 Dependencies这块可是复杂了，内容太多了。123456789101112131415161718&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; ... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt; ...&lt;/project&gt; 这是一个依赖的模板，这块内容非常多，看了几遍，暂时没有想到应该如何总结，可能回头单独总结了，尤其版本这块，和 js 的依赖关系是一样的，有各种规则，包含以下这些字段。 groupId, artifactId, version: type: scope: systemPath: optional: Dependency Version Requirement Specification 依赖版本需求规格Version Order Specification 版本的顺序规格Version Order Testing 版本顺序测试Exclusions 排除Inheritance 继承 One powerful addition that Maven brings to build management is the concept of project inheritance. Although in build systems such as Ant, inheritance can certainly be simulated, Maven has gone the extra step in making project inheritance explicit to the project object model. Maven 带来的一个强大的功能就是项目的继承。1234567891011&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-parent&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt;&lt;/project&gt; The packaging type required to be pom for parent and aggregation (multi-module) projects. These types define the goals bound to a set of lifecycle stages. For example, if packaging is jar, then the package phase will execute the jar:jar goal. Now we may add values to the parent POM, which will be inherited by its children. Most elements from the parent POM are inherited by its children, including: 上面这段 xml 里面的 packaging 类型需要设置成 pom 类型，对于 parent 或者 aggregation 项目。这个类型定义了目标被约束在一套的生命阶段中。举例来说，如果 packaging 是 jar，那么这个打包阶段将会执行 jar:jar 目标。 groupIdversiondescriptionurlinceptionYearorganizationlicensesdeveloperscontributorsmailingListsscmissueManagementciManagementpropertiesdependencyManagementdependenciesrepositoriespluginRepositoriesbuildplugin executions with matching idsplugin configurationetc.reportingprofiles 上面这些属性是可以被继承的。 artifactIdnameprerequisites 上面这些是不可以被继承的。123456789101112131415&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-parent&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;relativePath&gt;../my-parent&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;my-project&lt;/artifactId&gt;&lt;/project&gt; Notice the relativePath element. It is not required, but may be used as a signifier to Maven to first search the path given for this project’s parent, before searching the local and then remote repositories. relativePath 不是必须的，但是它可以作为一个记号，告诉 Maven 首先去哪里去搜索项目的 parent，可以是一个相对的路径，而不是去本地的 Maven 缓存或者远程的仓库中去寻找。 The Super POM 超级 POM这里的概念和 Java 语言的概念是一样的，所有的 pom 都是继承自一个超级 pom，这个超级 pom 的内容态度，这里不复述了，参考这里。 Dependency Management 依赖管理 Besides inheriting certain top-level elements, parents have elements to configure values for child POMs and transitive dependencies. One of those elements is dependencyManagement. 包含某些顶级元素在内，parent 可以给一些元素配置值来给 child POM 使用和实现一些依赖的传递。其中一个元素就是 dependencyManagement。 dependencyManagement: is used by a POM to help manage dependency information across all of its children. If the my-parent project uses dependencyManagement to define a dependency on junit:junit:4.12, then POMs inheriting from this one can set their dependency giving the groupId=junit and artifactId=junit only and Maven will fill in the version set by the parent. The benefits of this method are obvious. Dependency details can be set in one central location, which propagates to all inheriting POMs. dependencyManagement：被 POM 用于在所有的孩子之间去帮助管理依赖信息。如果 my-parent 项目使用了 dependencyManagement 去定义一个依赖 junit:junit:4.12， 那么继承这个 POM 的孩子只用设置依赖 groupId=junit 和 artifactId=junit，不用设置版本。 Note that the version and scope of artifacts which are incorporated from transitive dependencies are also controlled by version specifications in a dependency management section. This can lead to unexpected consequences. Consider a case in which your project uses two dependences, dep1 and dep2. dep2 in turn also uses dep1, and requires a particular minimum version to function. If you then use dependencyManagement to specify an older version, dep2 will be forced to use the older version, and fail. So, you must be careful to check the entire dependency tree to avoid this problem; mvn dependency:tree is helpful. 注意在 dependency management 部分里面的这个 version 和制品的 scope（被传递合成进来的）也是被 version specifications 所控制。这有可能会引起一些预料外的结果。考虑一个问题，你的项目使用了2个依赖，dep1 和 dep2，dep2 也使用了 dep1，需要一个特别的小版本。如果你使用了 dependencyManagement 去指定一个更早的版本，dep2 将会被强迫使用这个较早的版本，并且会失败。所以，你必须小心地去检查完全的依赖树（entire dependency tree）去避免这个问题；mvn dependency:tree 会有帮助。 Aggregation (or Multi-Module) 聚合（或者多模块） A project with modules is known as a multimodule, or aggregator project. 带有模块的项目被称为一个 multimodule，或者 aggregator 项目。这一部分好像暂时没有太多需要记录的。 Properties 属性 Properties are the last required piece to understand POM basics. Maven properties are value placeholder, like properties in Ant. Their values are accessible anywhere within a POM by using the notation ${X}, where X is the property. Or they can be used by plugins as default values, for example: Properties 可以在一个 POM 文件的任意一个位置被访问到，通过使用 ${X}， X 就是这个属性。 12345678910&lt;project&gt; ... &lt;properties&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;/properties&gt; ...&lt;/project&gt; They come in five different styles:env.X: Prefixing a variable with “env.” will return the shell’s environment variable. For example, ${env.PATH} contains the PATH environment variable.Note: While environment variables themselves are case-insensitive on Windows, lookup of properties is case-sensitive. In other words, while the Windows shell returns the same value for %PATH% and %Path%, Maven distinguishes between ${env.PATH} and ${env.Path}. The names of environment variables are normalized to all upper-case for the sake of reliability. env.X：返回 shell 的环境变量。这个应该是在启动时读入虚拟机的。 project.x: A dot (.) notated path in the POM will contain the corresponding element’s value. For example: 1.0 is accessible via ${project.version}. project.x：在 POM 文件中的以点表示的路径，访问的是对应的元素的值。 settings.x: A dot (.) notated path in the settings.xml will contain the corresponding element’s value. For example: false is accessible via ${settings.offline}. settings.x：在 settings.xml 的原理同上面。 Java System Properties: All properties accessible via java.lang.System.getProperties() are available as POM properties, such as ${java.home}. Java System Properties：Java 系统属性。 x: Set within a element in the POM. The value of value may be used as ${someVar}. x：表示在 POM 文件的 元素内的值。 Build Settings 构建配置 Beyond the basics of the POM given above, there are two more elements that must be understood before claiming basic competency of the POM. They are the build element, that handles things like declaring your project’s directory structure and managing plugins; and the reporting element, that largely mirrors the build element for reporting purposes. 除了上面的配置，还有两个元素需要理解，build 和 reporting。build 元素，处理生命你的项目目录，管理插件这些事情；reporting 元素，映射了用于报告目的的构建元素；（这句话没有太理解，这个元素到底是做什么用的） 这两个元素似乎是作为插件来管理的，参考这里，父元素底下的元素是给这些插件使用的。 Build According to the POM 4.0.0 XSD, the build element is conceptually divided into two parts: there is a BaseBuild type which contains the set of elements common to both build elements (the top-level build element under project and the build element under profiles, covered below); and there is the Build type, which contains the BaseBuild set as well as more elements for the top level definition. Let us begin with an analysis of the common elements between the two. 根据 POM 4.0.0 XSD， 这个 build 元素概念上分为两部分：在 project 下面的顶级元素 build，和在 profiles 里面的 build 元素。123456789101112131415&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;!-- "Project Build" contains more elements than just the BaseBuild set --&gt; &lt;build&gt;...&lt;/build&gt; &lt;profiles&gt; &lt;profile&gt; &lt;!-- "Profile Build" contains a subset of "Project Build"s elements --&gt; &lt;build&gt;...&lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; 上面这两个 build 部分应该怎么理解呢？ The BaseBuild Element Set BaseBuild 元素集123456789&lt;build&gt; &lt;defaultGoal&gt;install&lt;/defaultGoal&gt; &lt;directory&gt;$&#123;basedir&#125;/target&lt;/directory&gt; &lt;finalName&gt;$&#123;artifactId&#125;-$&#123;version&#125;&lt;/finalName&gt; &lt;filters&gt; &lt;filter&gt;filters/filter1.properties&lt;/filter&gt; &lt;/filters&gt; ...&lt;/build&gt; defaultGoal：默认目标。directory：构建输出的目标目录，默认是在 ${basedir}/target。finalName：最终构建的目标的名字。filter：定义了哪些 *.properties 包含了 properties 的列表。默认的 filter 目录是 ${basedir}/src/main/filters/。 上面就是 BaseBuild 元素集。 除了这个，还有 Resources，Plugins，Plugin Management，Resources 比较简单，配置资源的一些属性，后面这两项内容比较多，看了几遍，还没有看明白。 More Project Information 更多的项目信息Environment Settings 环境设置总结pom 的官网这篇文章非常长，就是因为太长了，所以，其实我断断续续，或者说断章取义地看过不少遍，但总是感觉没有看明白，还是需要系统地、具体地梳理一遍。 未完待续…… 参考https://maven.apache.org/pom.htmlhttps://www.cnblogs.com/AlanLee/p/6132976.html 这个帖子和下面这个帖子讲的不错，不过又有一样的地方，不知道是谁借鉴谁https://www.cnblogs.com/javahr/p/9328483.htmlhttps://www.cnblogs.com/yjmyzz/p/3495762.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>maven</tag>
        <tag>pom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英语中 i.e. 的用法]]></title>
    <url>%2FEnglish%2Fie-note.html</url>
    <content type="text"><![CDATA[前言英语中总会出现i.e.这个字样，以前查过一次，好像就是举例子的意思，最近又忘记了，记一下笔记，深化一下。 博客原帖收藏于IT老兵博客 正文 used especially in writing before a piece of information that makes the meaning of something clearer or shows its true meaning:The hotel is closed during low season, i.e. from October to March.The price must be more realistic, i.e. lower. So it is used to make the meaning more clearer. abbreviation for id est (= Latin for “that is”) Its meaning is “that is”. 参考https://dictionary.cambridge.org/dictionary/english/ie]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>English</tag>
        <tag>i.e.</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-merge 的用法总结]]></title>
    <url>%2Fgit%2Fgit-merge-note.html</url>
    <content type="text"><![CDATA[前言git-merge 有可能是最难搞清楚的，又是最常遇到的一个命令，其实这么想来，原来 SVN 时代、 CVS 时代，冲突都是一个不好解决的问题；或者说，冲突是最难自动解决的一个问题了，无论是在代码中，还是实际生活中。 博客原帖收藏于IT老兵驿站。 正文git-merge 使用来把两个或更多的开发历史合并。 参考里面是官网的讲解，这个帖子，我看了可能至少有五六遍了，但是要是说完全掌握明白，好像还是没有。 这两天又阅读了几遍，似乎看明白了一点。 git merge [-n] [–stat] [–no-commit] [–squash] [–[no-]edit] [–no-verify] [-s ] [-X ] [-S[]] [–[no-]allow-unrelated-histories] [–[no-]rerere-autoupdate] [-m ] [-F ] […​]git merge (–continue | –abort | –quit) 仅仅就这个命令形式，我就看过了好多遍，就没太看明白。 第一行是 git merge 的基本使用方式，而我理解和所使用到的方式就是在一个分支上，可以把另外一个分支的内容合并过，但是这里都没有出现。 Incorporates changes from the named commits (since the time their histories diverged from the current branch) into the current branch. This command is used by git pull to incorporate changes from another repository and can be used by hand to merge changes from one branch into another. 这里写的是，merge 是把那些从当前分支开始分出去的提交合并到当前分支。这也是 git pull 所使用的方式。从这个角度来说，倒是能说的通上面的问题。 第二行是如果你执行了上面的第一行命令，可能就会处于一种需要选择的状态，谈不上进退两难，但我之前的情况是更多是要选择后退。 Warning: Running git merge with non-trivial uncommitted changes is discouraged: while possible, it may leave you in a state that is hard to back out of in the case of a conflict. 有趣的是，这段话提醒了我，造成这个问题的原因有可能是因为有太多的内容需要合并—-那其实思路还是一样（和写代码很多地方的思路是一样的），就是小步地提交，频繁地合并。 -s –strategy=Use the given merge strategy; can be supplied more than once to specify them in the order they should be tried. If there is no -s option, a built-in list of strategies is used instead (git merge-recursive when merging a single head, git merge-octopus otherwise).-X –strategy-option=Pass merge strategy specific option through to the merge strategy. 这段是需要搞明白的内容，合并的策略是什么，就是 git 怎么去合并呢？git是有一些内建策略的。 确认了策略，然后可以确认策略的选项。 另外一篇文章《Git：merge的时候全部采用某一个端的文件》是原来总结的一篇文章，总结了一种 merge 的用法，那篇文章只是从很小的一个片段去总结了一下。 Recursive git merge -s recursive branch1 branch2 This operates on twoheads. Recursive is the default merge strategy when pulling or mergingone branch. Additionally this can detect and handle merges involving renames, but currently cannot make use of detected copies. This is the default merge strategy when pulling or merging one branch. Resolve git merge -s resolve branch1 branch2 This can only resolve two heads using a 3-way merge algorithm. It tries to carefully detect cris-cross merge ambiguities and is considered generally safe and fast. 上面是两种最常见的合并策略，还没有完全看明白区别。 参考https://git-scm.com/docs/git-mergehttps://www.atlassian.com/git/tutorials/using-branches/merge-strategy]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git merge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何根据 inode 删除一个文件]]></title>
    <url>%2Flinux%2Flinux-delete-by-inode-note.html</url>
    <content type="text"><![CDATA[概要如何根据 inode 删除一个文件 博客原帖收藏于IT老兵博客 前言有的时候，在 Linux 下没有简单地使用 rm 来删除一个文件，因为这些文件名前面带有一些特殊字符，这个时候就得使用 inode 来删除一个文件。 正文1stat &#123;file-name&#125; 或者 1ls -il &#123;file-name&#125; 在上面两种情况下，都可以看到 inode 的值。 然后像下面这样使用 find 命令，可以删除掉该文件： 1find . -inum [inode-number] -exec rm -i &#123;&#125; \; 这种用法是 find 命令的一种高级用法，这里也稍微复习一下这种用法： -exec：find命令将匹配的文件执行该参数所给出的shell命令。相应的命令的形式为’command’ {} \;，注意{}和\;之间的空格。 总结对这个方法，使用了不下几十遍了，现在感觉，记录了一遍笔记，才隐隐约约感觉记住了，搞明白了。 参考https://www.cyberciti.biz/tips/delete-remove-files-with-inode-number.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>rm</tag>
        <tag>inode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再识 ES]]></title>
    <url>%2Fes%2Frestudy-es-note.html</url>
    <content type="text"><![CDATA[前言14年创业时，学习过 ES，15年到了厦门，又复习了一遍，现在用到了，所以叫再识。 暂时缺乏一个很好的思路去记录这个笔记，那么还是按照最老实的方式来记录，就是对官网的文章一篇一篇地去学习，记录笔记。 博客IT老兵博客。 正文 Elasticsearch is a distributed document store. Instead of storing information as rows of columnar data, Elasticsearch stores complex data structures that have been serialized as JSON documents. When you have multiple Elasticsearch nodes in a cluster, stored documents are distributed across the cluster and can be accessed immediately from any node. ES 存储数据不是用行列式来存储—-这里说的应该是类似 MySQL 那种存储方式。ES 是把数据结构序列化成 JSON 文档来保存。 如果你的 ES 节点位于一个集群里面，那么这些存储的数据是分布在这个集群里面的—-那么每个节点上面是完整的吗？ When a document is stored, it is indexed and fully searchable in near real-time—​within 1 second. Elasticsearch uses a data structure called an inverted index that supports very fast full-text searches. An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in. 当一条文档被存储，它会被索引化和完全可搜索化在1秒之内。ES 使用一个反向索引来支持快速的全文检索。 An index can be thought of as an optimized collection of documents and each document is a collection of fields, which are the key-value pairs that contain your data. By default, Elasticsearch indexes all data in every field and each indexed field has a dedicated, optimized data structure. For example, text fields are stored in inverted indices, and numeric and geo fields are stored in BKD trees. The ability to use the per-field data structures to assemble and return search results is what makes Elasticsearch so fast. 一个 index 可以被认为是一个优化过的文档集合，其中的每一条文档是一个字段的集合。 Elasticsearch also has the ability to be schema-less, which means that documents can be indexed without explicitly specifying how to handle each of the different fields that might occur in a document. When dynamic mapping is enabled, Elasticsearch automatically detects and adds new fields to the index. This default behavior makes it easy to index and explore your data—​just start indexing documents and Elasticsearch will detect and map booleans, floating point and integer values, dates, and strings to the appropriate Elasticsearch datatypes. ES 具有模式无关的能力，这个是说，哪怕你新增了字段—-等于说是违反了之前的模式—-它也能处理索引化。 Ultimately, however, you know more about your data and how you want to use it than Elasticsearch can. You can define rules to control dynamic mapping and explicitly define mappings to take full control of how fields are stored and indexed. 然而，你对于你的数据知道的更多，更清楚应该如何让 ES 去使用它，所以如何做映射，应该是由你来明确定义。 Defining your own mappings enables you to: Distinguish between full-text string fields and exact value string fieldsPerform language-specific text analysisOptimize fields for partial matchingUse custom date formatsUse data types such as geo_point and geo_shape that cannot be automatically detectedIt’s often useful to index the same field in different ways for different purposes. For example, you might want to index a string field as both a text field for full-text search and as a keyword field for sorting or aggregating your data. Or, you might choose to use more than one language analyzer to process the contents of a string field that contains user input. The analysis chain that is applied to a full-text field during indexing is also used at search time. When you query a full-text field, the query text undergoes the same analysis before the terms are looked up in the index. 在做索引化时，分析链被用于全文字段，同时也被用于搜索的时候。当你查询一个全文字段，查询文本也在被做分析，在这些术语在被查找之前。 总结这里涉及到一个单词 indices， 这个单词是 index 的另外一种复数形式，底下的参考里面有讲。 参考https://www.elastic.co/guide/en/elasticsearch/reference/current/documents-indices.htmlhttps://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-creationhttps://www.nasdaq.com/articles/indexes-or-indices-whats-the-deal-2016-05-12]]></content>
      <categories>
        <category>ES</category>
      </categories>
      <tags>
        <tag>ES</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell 里面的 ";" "&&" 和 "||"]]></title>
    <url>%2Flinux%2Fshell-difference-semicolon-double%20ampersand.html</url>
    <content type="text"><![CDATA[概要shell 里面的 “;” “&amp;&amp;” 和 “||” 博客博客地址：IT老兵驿站 正文shell 下面的分号”；” “&amp;&amp;” 和 “||” 的区别 。 “；” semicolon 分号表示无论前面执行是否成功，都会执行后面的语句。“&amp;&amp;” double ampersand 表示逻辑与的关系，前面的语句成功了，才会执行后面的语句。“||” double vertical bar 表示逻辑或的关系。 参考https://unix.stackexchange.com/questions/187145/whats-the-difference-between-semicolon-and-double-ampersandhttps://www.javatpoint.com/linux-double-vertical-bar]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>semicolon</tag>
        <tag>double ampersand</tag>
        <tag>double vertical bar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 的 module 和 package]]></title>
    <url>%2Fpython%2Fpython-package-module-note.html</url>
    <content type="text"><![CDATA[概要Python 的 module 和 package。 博客博客地址：IT老兵驿站。 前言学 Python 也有一段时间了，一直没有搞清楚 module 和 package 的区别和概念，走了一些弯路，所以需要做做笔记，总结一下。 本文结合着对于这篇文章的阅读，进行记录笔记，不打算全文翻译，那样没有意义，扼要地记录关键的，感觉需要记录的内容。 正文编写 module 的方式 There are actually three different ways to define a module in Python: A module can be written in Python itself.A module can be written in C and loaded dynamically at run-time, like the re (regular expression) module.A built-in module is intrinsically contained in the interpreter, like the itertools module. 一共3种方式，Python 自己编写，由C 语言编写，动态加载，由解释器内部包含的内建 module。 编写方式 Here, the focus will mostly be on modules that are written in Python. The cool thing about modules written in Python is that they are exceedingly straightforward to build. All you need to do is create a file that contains legitimate Python code and then give the file a name with a .py extension. That’s it! No special syntax or voodoo is necessary. 非常简单，就是编写一个合法的 Python 的代码，以 .py 的扩展名结束，就是这么简单。 module 搜索目录 The directory from which the input script was run or the current directory if the interpreter is being run interactively The list of directories contained in the PYTHONPATH environment variable, if it is set. (The format for PYTHONPATH is OS-dependent but should mimic the PATH environment variable.) An installation-dependent list of directories configured at the time Python is installed 引入一个 module，是需要从一些位置去搜索，当前运行目录，PYTHONPATH 变量配置的目录，安装依赖的目录。 The resulting search path is accessible in the Python variable sys.path, which is obtained from a module named sys: 12345&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path['', 'C:\\Users\\john\\Documents\\Python\\doc', 'C:\\Python36\\Lib\\idlelib','C:\\Python36\\python36.zip', 'C:\\Python36\\DLLs', 'C:\\Python36\\lib','C:\\Python36', 'C:\\Python36\\lib\\site-packages'] 这些搜索路径会保存在 sys.path 里面，可以通过上面的方式获得。 换一个角度来说，想知道被导入的一个 module 的路径，可以通过以下的方式： 1234567&gt;&gt;&gt; import mod&gt;&gt;&gt; mod.__file__'C:\\Users\\john\\mod.py'&gt;&gt;&gt; import re&gt;&gt;&gt; re.__file__'C:\\Python36\\lib\\re.py' 总结这篇文章其实看了有些时候了，当时感觉基本掌握了，直到今天，认真地做了一下笔记，逐字逐句地阅读了一番，感觉又明白了不少东西。 参考https://realpython.com/python-modules-packages/]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>module</tag>
        <tag>package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记：聚合之管道操作符]]></title>
    <url>%2Fmongodb%2Fmongodb-collection-aggregation-stage-operator.html</url>
    <content type="text"><![CDATA[前言这里整理一下 MongoDB 的聚合的操作符，这些操作符代表着聚合操作的一个阶段。 db.collection.aggregate( [ { }, … ] ) 如上，聚合操作可以包含多个阶段。 博客IT老兵博客。 正文以下的操作符会逐步整理，先整理已经用到的，可能是比较常用的。 阶段（stage） 描述 $match 过滤器，设定匹配条件，符合条件的记录会被过滤出来。 $group 对输入的数据，根据特定的id表达式进行分组，并且应用累加表达式。会输出id域（表现为_id，针对前面）和累加域（如果前面指定）。 $sort 对输入结果进行排序。 $limit 对输入结果设定数值限制。 $unwind 解构数组，对数组中的每一条记录单独生成一条记录。 参考https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>aggregate</tag>
        <tag>聚合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建一个 Nexus 的 Maven 仓库]]></title>
    <url>%2Fjava%2Fjava-maven-nexus-note.html</url>
    <content type="text"><![CDATA[概要搭建一个 nexus 的 maven 仓库。 博客博客地址：IT老兵驿站。 前言今年开始深度编写 Java 代码，这样就势必会遇到搭建自己的 Maven 仓库，但是 Maven 这个东西，似乎不是那么简单，可以一蹴而就的，所以，需要好好记录一下学习的笔记。 本文记录一下使用 Nexus 来搭建 Maven 的仓库的笔记，感觉不是太好记，内容太多，先走一步，看一步吧，记录一下，总比不记录要好，看来记录笔记，是有很大的学问的。 正文安装和启动参考这里，安装和启动，和其他的 Java 项目一样，都是非常简单的。 Maven Repository Format 格式Maven 仓库的格式的规范。 Version policy 版本策略ReleaseMaven 仓库可以被配置为 Release 版本策略。 SnapshotSnapshot 版本策略。 Mixed混合策略。 Release 和 Snapshot 是 Maven 的一个概念，参考官网： What is a SNAPSHOT version?Notice the value of the version tag in the pom.xml file shown below has the suffix: -SNAPSHOT. 在 pom.xml 中会有带有-SNAPSHOT后缀的标记。 12345678&lt;project xmlns="http://maven.apache.org/POM/4.0.0" ... &lt;groupId&gt;...&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; ... &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;Maven Quick Start Archetype&lt;/name&gt; ... The SNAPSHOT value refers to the ‘latest’ code along a development branch, and provides no guarantee the code is stable or unchanging. Conversely, the code in a ‘release’ version (any version value without the suffix SNAPSHOT) is unchanging.In other words, a SNAPSHOT version is the ‘development’ version before the final ‘release’ version. The SNAPSHOT is “older” than its release.During the release process, a version of x.y-SNAPSHOT changes to x.y. The release process also increments the development version to x.(y+1)-SNAPSHOT. For example, version 1.0-SNAPSHOT is released as version 1.0, and the new development version is version 1.1-SNAPSHOT. SNAPSHOT 表示开发分支的最后代码，不保证代码是固定的或者不变。而位于 release 版本里面的则表示不会被修改了。 Layout policy 层级策略Maven 仓库定义了默认的目录结构，以下的参数表示允许不允许破坏这个默认的规则。 Permissive允许。 Strict严格遵守。 Proxying Maven Repositories 代理 Maven 仓库 A default installation of Nexus Repository Manager includes a proxy repository configured to access the Central Repository via HTTPS using the URL https://repo1.maven.org/maven2/. To reduce duplicate downloads and improve download speeds for your developers and CI servers, you should proxy all other remote repositories you access as proxy repositories as well. Nexus 默认会包含一个代理仓库，可以配置去访问中心仓库 https://repo1.maven.org/maven2/。这样相当于是一级缓存，不用每次都去从远程去拉取。 Hosting Maven Repositories 自主 Maven 仓库这个地方不知道怎么去翻译这个 hosting。 这个方式可以存放第三方的库。默认会有两个仓库 maven-releases 和 maven-snapshots， 和上面的原理一样。 Grouping Maven Repositories 对仓库分组可以把代理的和自主的仓库通过这种方式统一暴露出去。 上述这几个过程，在这个帖子都配有图来讲解，这里就不再赘述。 配置客户端把以下内容配置在~/.m2/settings.xml文件中，这是配置访问你的仓库的凭证，并且告诉 mvn，你的仓库是作为中心仓库的一个镜像。12345678910111213141516171819202122232425262728&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.1.0 http://maven.apache.org/xsd/settings-1.1.0.xsd"&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;central&lt;/name&gt; &lt;url&gt;http://your-host:8081/repository/maven-group/&lt;/url&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt; 配置项目配置项目依赖： 1234567891011&lt;project ...&gt; ... &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;maven-group&lt;/id&gt; &lt;url&gt;http://your-host:8081/repository/maven-group/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt;&lt;/project&gt; 配置发布（mvn deploy）：123456789101112131415&lt;project ...&gt; ... &lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://your-host:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://your-host:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt;&lt;/project&gt; 总结至此，大体将这个过程梳理一遍，断断续续花了不少时间，Maven 的细节比较多，还是需要时间的。 参考https://blog.sonatype.com/using-nexus-3-as-your-repository-part-1-maven-artifactshttps://help.sonatype.com/repomanager3/formats/maven-repositories#MavenRepositories-ProxyingMavenRepositories]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>nexus</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python的异步编程介绍（MD）]]></title>
    <url>%2Fpython%2Fpython-async-program.html</url>
    <content type="text"><![CDATA[概要Git：Python的异步编程介绍（MD）。 博客博客地址：IT老兵驿站。 前言这里翻译和学习一篇介绍Python的异步编程的文章，在网上找了半天，感觉这篇写的很好，把几种实现方案都举了例子，而且列出了优劣。 之前的文章采用了富文本编辑，不太方便，这次改为MD格式。 正文 介绍 Introduction Asynchronous programming is a type of parallel programming in which a unit of work is allowed to run separately from the primary application thread. When the work is complete, it notifies the main thread about completion or failure of the worker thread. There are numerous benefits to using it, such as improved application performance and enhanced responsiveness. 异步编程是一种并行编程，这里支持一个单元的工作被允许从主程序中分离出来。 Asynchronous programming has been gaining a lot of attention in the past few years, and for good reason. Although it can be more difficult than the traditional linear style, it is also much more efficient. For example, instead of waiting for an HTTP request to finish before continuing execution, with Python async coroutines you can submit the request and do other work that’s waiting in a queue while waiting for the HTTP request to finish. 例如，与其等待一个 HTTP request 的返回，然后去继续后面的工作，就可以使用 Python async coroutines， 你提交了请求，然后就可以去做别的事情了，当请求回来的时候，它们会在一个队列里面等待。 Asynchronicity seems to be a big reason why Node.js so popular for server-side programming. Much of the code we write, especially in heavy IO applications like websites, depends on external resources. This could be anything from a remote database call to POSTing to a REST service. As soon as you ask for any of these resources, your code is waiting around with nothing to do. With asynchronous programming, you allow your code to handle other tasks while waiting for these other resources to respond. 异步性可能就是 Node.js 这么风靡的一个原因。 How Does Python Do Multiple Things At Once? 1. Multiple Processes 多进程方式 The most obvious way is to use multiple processes. From the terminal, you can start your script two, three, four…ten times and then all the scripts are going to run independently or at the same time. The operating system that’s underneath will take care of sharing your CPU resources among all those instances. Alternately you can use the multiprocessing library which supports spawning processes as shown in the example below. 多进程的实现方式是最常见的方式。从终端，你可以开启多份脚本去独立运行，这是一种方式；还有一种方式，就是语言层面的支持。 多进程实现的代码如下。 from multiprocessing import Process def print_func(continent='Asia'): print('The name of continent is : ', continent) if __name__ == "__main__": # confirms that the code is under main function names = ['America', 'Europe', 'Africa'] procs = [] proc = Process(target=print_func) # instantiating without any argument procs.append(proc) proc.start() # instantiating process with arguments for name in names: # print(name) proc = Process(target=print_func, args=(name,)) procs.append(proc) proc.start() # complete the processes for proc in procs: proc.join() Output: The name of continent is : Asia The name of continent is : America The name of continent is : Europe The name of continent is : Africa 2. Multiple Threads 多线程方式 The next way to run multiple things at once is to use threads. A thread is a line of execution, pretty much like a process, but you can have multiple threads in the context of one process and they all share access to common resources. But because of this, it’s difficult to write a threading code. And again, the operating system is doing all the heavy lifting on sharing the CPU, but the global interpreter lock (GIL) allows only one thread to run Python code at a given time even when you have multiple threads running code. So, In CPython, the GIL prevents multi-core concurrency. Basically, you’re running in a single core even though you may have two or four or more. 多线程是在一个进程上下文中存在的，这样访问一些公共的资源是很容易的。但也同样是因为这一点，编写多线程代码是有些困难的。 多线程实现代码如下，这里有一个问题， GIL 是什么。 import threading def print_cube(num): """ function to print cube of given num """ print("Cube: {}".format(num * num * num)) def print_square(num): """ function to print square of given num """ print("Square: {}".format(num * num)) if __name__ == "__main__": # creating thread t1 = threading.Thread(target=print_square, args=(10,)) t2 = threading.Thread(target=print_cube, args=(10,)) # starting thread 1 t1.start() # starting thread 2 t2.start() # wait until thread 1 is completely executed t1.join() # wait until thread 2 is completely executed t2.join() # both threads completely executed print("Done!") view rawgistfile1.txt hosted with ❤ by GitHub Output : Square: 100 Cube: 1000 Done! 3. Coroutines using yield 使用协程 Coroutines are generalization of subroutines. They are used for cooperative multitasking where a process voluntarily yield (give away) control periodically or when idle in order to enable multiple applications to be run simultaneously. Coroutines are similar to generators but with few extra methods and slight change in how we use yield statement. Generators produce data for iteration while coroutines can also consume data. 使用 yield 方式的协程。 def print_name(prefix): print("Searching prefix:{}".format(prefix)) try : while True: # yeild used to create coroutine name = (yield) if prefix in name: print(name) except GeneratorExit: print("Closing coroutine!!") corou = print_name("Dear") corou.__next__() corou.send("James") corou.send("Dear James") corou.close() Output : Searching prefix:Dear Dear James Closing coroutine!! 4. Asynchronous Programming 异步编程 The fourth way is an asynchronous programming, where the OS is not participating. As far as OS is concerned you’re going to have one process and there’s going to be a single thread within that process, but you’ll be able to do multiple things at once. So, what’s the trick? 异步编程，这里 OS 并没有参与—-那应该只是语言自身的实现了。 使用asyncio的协程。 The answer is asyncio asyncio is the new concurrency module introduced in Python 3.4. It is designed to use coroutines and futures to simplify asynchronous code and make it almost as readable as synchronous code as there are no callbacks. asyncio uses different constructs: event loops, coroutines and futures. An event loop manages and distributes the execution of different tasks. It registers them and handles distributing the flow of control between them. Coroutines (covered above) are special functions that work similarly to Python generators, on await they release the flow of control back to the event loop. A coroutine needs to be scheduled to run on the event loop, once scheduled coroutines are wrapped in Tasks which is a type of Future. Futures represent the result of a task that may or may not have been executed. This result may be an exception. 上面这段是关键，asyncio使用了三个结构：event loops，coroutines 和 futures。 Using Asyncio, you can structure your code so subtasks are defined as coroutines and allows you to schedule them as you please, including simultaneously. Coroutines contain yield points where we define possible points where a context switch can happen if other tasks are pending, but will not if no other task is pending. A context switch in asyncio represents the event loop yielding the flow of control from one coroutine to the next. In the example, we run 3 async tasks that query Reddit separately, extract and print the JSON. We leverage aiohttp which is a http client library ensuring even the HTTP request runs asynchronously. import signal import sys import asyncio import aiohttp import json loop = asyncio.get_event_loop() client = aiohttp.ClientSession(loop=loop) async def get_json(client, url): async with client.get(url) as response: assert response.status == 200 return await response.read() async def get_reddit_top(subreddit, client): data1 = await get_json(client, 'https://www.reddit.com/r/' + subreddit + '/top.json?sort=top&amp;t=day&amp;limit=5') j = json.loads(data1.decode('utf-8')) for i in j['data']['children']: score = i['data']['score'] title = i['data']['title'] link = i['data']['url'] print(str(score) + ': ' + title + ' (' + link + ')') print('DONE:', subreddit + '\n') def signal_handler(signal, frame): loop.stop() client.close() sys.exit(0) signal.signal(signal.SIGINT, signal_handler) asyncio.ensure_future(get_reddit_top('python', client)) asyncio.ensure_future(get_reddit_top('programming', client)) asyncio.ensure_future(get_reddit_top('compsci', client)) loop.run_forever() view rawgistfile1.txt hosted with ❤ by GitHub Output : 50: Undershoot: Parsing theory in 1965 (http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/07/knuth_1965_2.html) 12: Question about best-prefix/failure function/primal match table in kmp algorithm (https://www.reddit.com/r/compsci/comments/8xd3m2/question_about_bestprefixfailure_functionprimal/) 1: Question regarding calculating the probability of failure of a RAID system (https://www.reddit.com/r/compsci/comments/8xbkk2/question_regarding_calculating_the_probability_of/) DONE: compsci 336: /r/thanosdidnothingwrong -- banning people with python (https://clips.twitch.tv/AstutePluckyCocoaLitty) 175: PythonRobotics: Python sample codes for robotics algorithms (https://atsushisakai.github.io/PythonRobotics/) 23: Python and Flask Tutorial in VS Code (https://code.visualstudio.com/docs/python/tutorial-flask) 17: Started a new blog on Celery - what would you like to read about? (https://www.python-celery.com) 14: A Simple Anomaly Detection Algorithm in Python (https://medium.com/@mathmare_/pyng-a-simple-anomaly-detection-algorithm-2f355d7dc054) DONE: python 1360: git bundle (https://dev.to/gabeguz/git-bundle-2l5o) 1191: Which hashing algorithm is best for uniqueness and speed? Ian Boyd's answer (top voted) is one of the best comments I've seen on Stackexchange. (https://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed) 430: ARM launches “Facts” campaign against RISC-V (https://riscv-basics.com/) 244: Choice of search engine on Android nuked by “Anonymous Coward” (2009) (https://android.googlesource.com/platform/packages/apps/GlobalSearch/+/592150ac00086400415afe936d96f04d3be3ba0c) 209: Exploiting freely accessible WhatsApp data or “Why does WhatsApp web know my phone’s battery level?” (https://medium.com/@juan_cortes/exploiting-freely-accessible-whatsapp-data-or-why-does-whatsapp-know-my-battery-level-ddac224041b4) DONE: programming view rawgistfile1.txt hosted with ❤ by GitHub 上面这段代码有点过时了，现在已经不是太推荐这种用法了，不再直接对 loop 进行操作，而是对 task 进行处理，参考官网，但是这种用法可能是最常见的一种用法。 Using Redis and Redis Queue RQ 老版本Python的办法，不支持 asyncio 和 aiohttp 的版本怎么办，使用Redis 和 RQ。下面的文章意义已经不大了，就不过多记录了。 Using asyncio and aiohttp may not always be in an option especially if you are using older versions of python. Also, there will be scenarios when you would want to distribute your tasks across different servers. In that case we can leverage RQ (Redis Queue). It is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis — a key/value data store. In the example below, we have queued a simple function count_words_at_url using redis. from mymodule import count_words_at_url from redis import Redis from rq import Queue q = Queue(connection=Redis()) job = q.enqueue(count_words_at_url, 'http://nvie.com') ******mymodule.py****** import requests def count_words_at_url(url): """Just an example function that's called async.""" resp = requests.get(url) print( len(resp.text.split())) return( len(resp.text.split())) view rawgistfile1.txt hosted with ❤ by GitHub Output: 15:10:45 RQ worker 'rq:worker:EMPID18030.9865' started, version 0.11.0 15:10:45 *** Listening on default... 15:10:45 Cleaning registries for queue: default 15:10:50 default: mymodule.count_words_at_url('http://nvie.com') (a2b7451e-731f-4f31-9232-2b7e3549051f) 322 15:10:51 default: Job OK (a2b7451e-731f-4f31-9232-2b7e3549051f) 15:10:51 Result is kept for 500 seconds view rawgistfile1.txt hosted with ❤ by GitHub Conclusion 结论 Let’s take a classical example, a chess exhibition where one of the best chess players competes against a lot of people. And if there are 24 games with 24 people to play with and the chess master plays with all of them synchronically, it’ll take at least 12 hours (taking into account that the average game takes 30 moves, the chess master thinks for 5 seconds to come up with a move and the opponent — for approximately 55 seconds). But using the asynchronous mode gives chess master the opportunity to make a move and leave the opponent thinking while going to the next one and making a move there. This way a move on all 24 games can be done in 2 minutes and all of them can be won in just one hour. So, this is what’s meant when people talk about asynchronous being really fast. It’s this kind of fast. Chess master doesn’t play chess faster, the time is just more optimized and it’s not get wasted on waiting around. This is how it works. In this analogy, the chess master will be our CPU and the idea is that we wanna make sure that the CPU doesn’t wait or waits the least amount of time possible. It’s about always finding something to do. A practical definition of Async is that it’s a style of concurrent programming in which tasks release the CPU during waiting periods, so that other tasks can use it. In Python there are several ways to achieve concurrency , based on our requirement, code flow, data manipulation , architecture design and use cases we can select any of these methods. 参考href=”https://medium.com/velotio-perspectives/an-introduction-to-asynchronous-programming-in-python-af0189a88bbb&quot; rel=”nofollow”&gt;https://medium.com/velotio-perspectives/an-introduction-to-asynchronous-programming-in-python-af0189a88bbbhttps://docs.python.org/3/library/asyncio-task.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>异步编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim 的移动]]></title>
    <url>%2Flinux%2Fvim-copy-note.html</url>
    <content type="text"><![CDATA[概述vim 的移动。 博客博客地址：IT老兵驿站。 前言大概是在10年，还是11年，还在拉卡啦的时候，开始完整学习vim的手册，记得是一直到360，大概把手册看了几遍，不过因为需要记忆的内容实在是太多了，或者是因为懒惰，很多命令还是没有学明白，主要是没有去实践，就忘记了。其实，当时也记了笔记，不过笔记本早都找不到了，也是没有掌握记笔记的要领。现在重新记录一下。这篇笔记也比较难一气呵成，能写多少写多少，之后在实际工作中，逐渐补充完成。 正文单词移动w：向前移动到下一个单词的开始。W：向前移动到下一个单词的开始（ white-space separated WORDs，例如special/separated/words）。b：向后移动到下一个单词的开始。B：向后移动到下一个单词的开始（ white-space separated WORDs，例如special/separated/words）。e：向前移动到下一个单词的结束。E：向前移动到下一个单词的结束（ white-space separated WORDs，例如special/separated/words）。ge：向后移动到下一个单词的结束。gE：向后移动到下一个单词的结束（ white-space separated WORDs，例如special/separated/words）。 使用vim实践一把，感觉印象深刻了很多。 移动到行首或者行尾$：移动到行尾。^：移动到行首的非空字符上。0：移动到行首。 $同时接受数字参数，1$表示移动到第一行行尾，2$表示移动到第二行行尾。 移动到一个字符上f：向前移动到某个字符上，例如： 123To err is human. To really foul up you need a computer. ---------&gt;---------------&gt; fh fy f 可以带数字，就是移动到第几次该字符的出现位置上。F：vise versa（反之亦然）。t：向前移动到某个字符前一个位置。T：vise versa（反之亦然）。 参考http://vimdoc.sourceforge.net/htmldoc/usr_03.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中如何创建自定义的注解学习笔记（MD版）]]></title>
    <url>%2Fjava%2Fjava-annotation-note.html</url>
    <content type="text"><![CDATA[概要Java中如何创建自定义的注解学习笔记（MD版）。 博客博客地址：IT老兵驿站。 前言记得这篇笔记还是在泉州的龙玲酒店记录的，是一个周六的晚上，坐飞机从上海到泉州，从笔记中能勾起一些旅游的回忆，感觉很丰富。 这次重新修改它，是因为感觉对 Java 的注解还是没有搞明白，还需要再花点气力。 这篇文章之前使用的是富文本编辑的，现在感觉迁移起来太痛苦，所以改为 Markdown 来记录。 关于 Java 的注解，我一直在用，没有太搞明白它的原理，至于如何自定义一个注解，就更不明白了。其实参考的这篇文章，之前看过一遍，当时以为看懂了，但是最近在工作中去印证的时候，发现对注解还是不理解，所以这两天又再看了一遍，感觉这下又懂了一些。 本文针对着原文的段落进行备注和记录笔记。 正文 This comprehensive look at annotations in Java not only goes into how to create them but also advise on how to use them and how they’re processed by the JVM. 这篇文章主旨不光是讲如何创建注解，还包括如何使用它们，以及它们在 JVM 上是如何处理的。 Annotations are a powerful part of Java, but most times we tend to be the users rather than the creators of annotations. For example, it is not difficult to find Java source code that includes the @Override annotation processed by the Java compiler, the @Autowired annotation used by the Spring framework, or the @Entity annotation used by the Hibernate framework, but rarely do we see custom annotations. While custom annotations are an often-overlooked aspect of the Java language, they can be a very useful asset in developing readable code and just as importantly, useful in understanding how many common frameworks, such as Spring or Hibernate, succinctly accomplish their goals. 自定义注解在实际的使用中，其实没有得到足够的重视。 In this article, we will cover the basics of annotations, including what annotations are, how they are useful in large-than-academic examples, and how to process them. In order to demonstrate how annotations work in practice, we will create a Javascript Object Notation (JSON) serializer that processes annotated objects and produces a JSON string representing each object. Along the way, we will cover many of the common stumbling blocks of annotations, including the quirks of the Java reflection framework and visibility concerns for annotation consumers. The interested reader can find the source code for the completed JSON serializer on GitHub. 这篇文章以一个 JSON 序列化器的注解为例来介绍。 What Are Annotations? 注解是什么 Annotations are decorators that are applied to Java constructs, such as classes, methods, or fields, that associate metadata with the construct. These decorators are benign and do not execute any code in-and-of-themselves, but can be used by runtime frameworks or the compiler to perform certain actions. Stated more formally, the Java Language Specification (JLS), Section 9.7, provides the following definition: An annotation is a marker which associates information with a program construct, but has no effect at run time. It is important to note the last clause in this definition: Annotations have no effect on a program at runtime. This is not to say that a framework may not change its behavior based on the presence of an annotation at runtime, but that the inclusion of an annotation does not itself change the runtime behavior of a program. While this may appear to be a nuanced distinction, it is a very important one that must be understood in order to grasp the usefulness of annotations. For example, adding the @Autowired annotation to an instance field does not in-and-of-itself change the runtime behavior of a program: The compiler simply includes the annotation at runtime, but the annotation does not execute any code or inject any logic that alters the normal behavior of the program (the behavior expected when the annotation is omitted). Once we introduce the Spring framework at runtime, we are able to gain powerful Dependency Injection (DI) functionality when our program is parsed. By including the annotation, we have instructed the Spring framework to inject an appropriate dependency into our field. We will see shortly (when we create our JSON serializer) that the annotation itself does not accomplish this, but rather, the annotation acts as a marker, informing the Spring framework that we desire a dependency to be injected into the annotated field. 注解是一种装饰器，那么参考装饰器设计模式的概念，它其实是给被装饰者增加功能用的。 Retention and Target 保存和目标 Creating an annotation requires two pieces of information: (1) a retention policy and (2) a target. A retention policy specifies how long, in terms of the program lifecycle, the annotation should be retained for. For example, annotations may be retained during compile-time or runtime, depending on the retention policy associated with the annotation. As of Java 9, there are three standard retention policies, as summarized below: 创建一个注解，需要两方面的信息： 一个保留策略 一个目标 保留策略指定了多长时间，用术语来说就是程序的生命周期，这个注解可以被保留。参考下面： POLICY DESCRIPTION Source Annotations are discarded by the compiler Class Annotations are recorded in the class file generated by the compiler but are not required to be retained by the Java Virtual Machine (JVM) that processes the class file at runtime Runtime Annotations are recorded in the class file by the compiler and retained at runtime by the JVM As we will see shortly, the runtime option for annotation retention is one of the most common, as it allows for Java programs to reflectively access the annotation and execute code based on the presence of an annotation, as well as access the data associated with an annotation. Note that an annotation has exactly one associated retention policy. The target of an annotation specifies which Java constructs an annotation can be applied to. For example, some annotations may be valid for methods only, while others may be valid for both classes and fields. As of Java 9, there are eleven standard annotation targets, as summarized in the following table: 注解的 target 指定了一个注解将会被用于哪一个 Java 构造器，下面表格进行了总结： TARGET DESCRIPTION Annotation Type Annotates another annotation Constructor Annotates a constructor Field Annotates a field, such as an instance variable of a class or an enum constant Local variable Annotates a local variable Method Annotates a method of a class Module Annotates a module (new in Java 9) Package Annotates a package Parameter Annotates a parameter to a method or constructor Type Annotates a type, such as a class, interfaces, annotation types, or enum declarations Type Parameter Annotates a type parameter, such as those used as formal generic parameters Type Use Annotates the use of a type, such as when an object of a type is created using the newkeyword, when an object is cast to a specified type, when a class implements an interface, or when the type of a throwable object is declared using the throws keyword (for more information, see the [Type Annotations and Pluggable Type Systems Oracle tutorial] For more information on these targets, see Section 9.7.4 of the JLS. It is important to note that one or more targets may be associated with an annotation. For example, if the field and constructor targets are associated with an annotation, then the annotation may be used on either fields or constructors. If on the other hand, an annotation only has an associated target of method, then applying the annotation to any construct other than a method results in an error during compilation. 一个注解可以被关联上一个或者多个 target。 Annotation Parameters 注解参数 Annotations may also have associated parameters. These parameters may be a primitive (such as int or double), String, class, enum, annotation, or an array of any of the five preceding types (see Section 9.6.1 of the JLS). Associating parameters with an annotation allows for an annotation to provide contextual information or can parameterize a processor of an annotation. For example, in our JSON serializer implementation, we will allow for an optional annotation parameter that specifies the name of a field when it is serialized (or use the variable name of the field by default if no name is specified). 注解可以有关联的参数，这些参数可以是一个原语（例如 int 或者 double），这里原语的意思是第一级的类型定义，String, class, enum, annotation，或者前面这五种类型的数据。把参数和一个注解关联起来，这样可以允许一个注解提供上下文的一些信息或者可以参数化一个注解的一个处理器。举个例子，在我们的 JSON 序列化器的实现中，我们将会允许一个作为可选项的注解参数来指定将要被序列化的字段的名字（或者，如果没有名称被指定，那么使用这个字段的默认名字）。 How Are Annotations Created? 注解是怎么创建的 For our JSON serializer, we will create a field annotation that allows a developer to mark a field to be included when serializing an object. For example, if we create a car class, we can annotate the fields of the car (such as make and model) with our annotation. When we serialize a car object, the resulting JSON will include make and model keys, where the values represent the value of the make and model fields, respectively. For the sake of simplicity, we will assume that this annotation will be used only for fields of type String, ensuring that the value of the field can be directly serialized as a string. 下面要开始举例说明如何创建一个注解。 To create such a field annotation, we declare a new annotation using the @interface keyword: @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.FIELD) public @interface JsonField { public String value() default &quot;&quot;; } 这个地方以前没有看仔细，这里是@interface，不是 interface。 The core of our declaration is the public @interface JsonField, which declares an annotation type with a public modifier, allowing our annotation to be used in any package (assuming the package is properly imported if in another module). The body of the annotation declares a single String parameter, named value, that has a type of String and a default value of an empty string. 上面这段代码声明的核心是 public @interface JsonField，这声明了一个带有 public 修饰符的注解，允许我们的注解在任意的包被使用（假设这个包在任何模块中正确地被引入—-这句话有点看不懂）。声明的 body 部分声明了一个 String 的参数，名字是 value ，类型是 String，默认值是空的 String。 Note that the variable name value has a special meaning: It defines a Single-Element Annotation (Section 9.7.3. of the JLS) and allows users of our annotation to supply a single parameter to the annotation without specifying the name of the parameter. For example, a user can annotate a field using @JsonField(&quot;someFieldName&quot;) and is not required to declare the annotation as @JsonField(value = &quot;someFieldName&quot;), although the latter may still be used (but it is not required). The inclusion of a default value of empty string allows for the value to be omitted, resulting in value holding an empty string if no value is explicitly specified. For example, if a user declares the above annotation using the form @JsonField, then the value parameter is set to an empty string. 注意这个 value 有一个特殊的意义，它定义了一个单元素的注解，允许注解的使用者给一个注解提供一个单一的参数，而不用指定参数的名字。例如用户可以@JsonField(&quot;someFieldName&quot;) 这样去对一个字段进行注解，而不需要@JsonField(value = &quot;someFieldName&quot;) 这样去声明这个注解，当然后面这种方式还是可以被使用的。默认值的包含允许这个 value 值被忽略，而没有明确的 value 被指定的时候，就会使用这个空字符串作为默认值，例如，一个用户是用@JsonField 来声明的，那么这个 value 就会被设置成空串。 The retention policy and target of the annotation declaration are specified using the @Retention and @Targetannotations, respectively. The retention policy is specified using the [java.lang.annotation.RetentionPolicy](https://docs.oracle.com/javase/9/docs/api/java/lang/annotation/RetentionPolicy.html)enum and includes constants for each of the three standard retention policies. Likewise, the target is specified using the [java.lang.annotation.ElementType](https://docs.oracle.com/javase/9/docs/api/java/lang/annotation/ElementType.html)enum, which includes constants for each of the eleven standard target types. In summary, we created a public, single-element annotation named JsonField, which is retained by the JVM during runtime and may only be applied to fields. This annotation has a single parameter, value, of type String with a default value of an empty string. With our annotation created, we can now annotate fields to be serialized. How Are Annotations Used? 注解是怎么被使用的 Using an annotation requires only that the annotation is placed before an appropriate construct (any valid target for the annotation). For example, we can create a Carclass using the following class declaration: 下面结合一个例子来讲，注解是如何被使用的。 public class Car { @JsonField(&quot;manufacturer&quot;) private final String make; @JsonField private final String model; private final String year; public Car(String make, String model, String year) { this.make = make; this.model = model; this.year = year; } public String getMake() { return make; } public String getModel() { return model; } public String getYear() { return year; } @Override public String toString() { return year + &quot; &quot; + make + &quot; &quot; + model; } } This class exercises the two major uses of the @JsonField annotation: (1) with an explicit value and (2) with a default value. We could have also annotated a field using the form @JsonField(value = &quot;someName&quot;), but this style is overly verbose and does not aid in the readability of our code. Therefore, unless the inclusion of an annotation parameter name in a single-element annotation adds to the readability of code, it should be omitted. For annotations with more than one parameter, the name of each parameter is required to differentiate between parameters (unless only one argument is provided, in which case, the argument is mapped to the value parameter if no name is explicitly provided). 上面的例子实践了注解的两个用法，一个是带有明确的值，一个是带有一个默认值。对于单一参数的注解，可以忽略掉参数名；反之，则不行。 Given the above uses of the @JsonField annotation, we would expect that a Car ject is serialized into a JSON string of the form {&quot;manufacturer&quot;:&quot;someMake&quot;, &quot;model&quot;:&quot;someModel&quot;} (note, as we will see later, we will disregard the order of the keys–manufacturer and model–in this JSON string). Before we proceed, it is important to note that adding the @JsonField annotations does not change the runtime behavior of the Carclass. If we compile this class, the inclusion of @JsonField annotations does not enhance the behavior of the Car class anymore than had we omitted the annotations. These annotations are simply recorded, along with the value of the value parameter, in the class file for the Car class. Altering the runtime behavior of our system requires that we process these annotations. 基于上面的 @JsonField 的注解使用，我们可以期望一个 Car 被序列化成一个 JSON 字符串。后面这段是讲，这个注解并没有改变 Car 的运行时的行为。 How are Annotations Processed? 注解是怎么被处理的 Processing annotations is accomplished through the Java Reflection Application Programming Interface (API). Sidelining the technical nature of the reflection API for a moment, the reflection API allows us to write code that will inspect the class, methods, fields, etc. of an object. For example, if we create a method that accepts a Car object, we can inspect the class of this object (namely, Car) and discover that this class has three fields: (1) make, (2) model, and (3) year. Furthermore, we can inspect these fields to discover if each is annotated with a specific annotation. 注解的处理是由反射的 API 来完成的，反射的 API 允许我们写一些代码去检视一个对象的类、方法、域等。 Using this capability, we can iterate through each field of the class associated with the object passed to our method and discover which of these fields are annotated with the @JsonField annotation. If the field is annotated with the @JsonField annotation, we record the name of the field and its value. Once all the fields have been processed, then we can create the JSON string using these field names and values. 使用这个能力，我们可以遍历传递给我们的方法的对象的类的每一个字段，并且去发现那些字段是被 @JsonField 来注解的。前半句有些拗口，“传递给我们的方法的对象”，这里是指什么呢？如果一个域被用 @JsonField 注解，我们记录下来它的域的名称和它的值。 Determining the name of the field requires more complex logic than determining the value. If the @JsonFieldincludes a provided value for the value parameter (such as &quot;manufacturer&quot; in the previous @JsonField(&quot;manufacturer&quot;) use), we will use this provided field name. If the value of the value parameter is an empty string, we know that no field name was explicitly provided (since this is the default value for the value parameter), or else, an empty string was explicitly provided. In either case, we will use the variable name of the field as the field name (for example, model in the private final String model declaration). 确定域的名字比确定值，逻辑要更加复杂。如果 @JsonField 包含一个给 value 参数提供一个值（例如之前的 @JsonField(&quot;manufacturer&quot;) 里面的&quot;manufacturer&quot; ），我们将那个作为域的值。如果 value 参数的值是一个空串，那么我们知道域的名称没有被明确地提供（因为这是 value 参数的默认值），或者提供了一个空串。这种情况下，我们将使用这个域的变量名作为域的名（例如，在 private final String model 声明里面的 model）。 这个注解其实是为了确认这个域的名称，这里的 value 是这个域的名称的 value，不是这个域的值，这里容易混淆，之前就没搞明白这里。 Combining this logic into a JsonSerializer class, we can create the following class declaration: public class JsonSerializer { public String serialize(Object object) throws JsonSerializeException { try { Class&lt;?&gt; objectClass = requireNonNull(object).getClass(); Map&lt;String, String&gt; jsonElements = new HashMap&lt;&gt;(); for (Field field: objectClass.getDeclaredFields()) { field.setAccessible(true); if (field.isAnnotationPresent(JsonField.class)) { jsonElements.put(getSerializedKey(field), (String) field.get(object)); } } System.out.println(toJsonString(jsonElements)); return toJsonString(jsonElements); } catch (IllegalAccessException e) { throw new JsonSerializeException(e.getMessage()); } } private String toJsonString(Map&lt;String, String&gt; jsonMap) { String elementsString = jsonMap.entrySet() .stream() .map(entry -&gt; &quot;\&quot;&quot; + entry.getKey() + &quot;\&quot;:\&quot;&quot; + entry.getValue() + &quot;\&quot;&quot;) .collect(Collectors.joining(&quot;,&quot;)); return &quot;{&quot; + elementsString + &quot;}&quot;; } private static String getSerializedKey(Field field) { String annotationValue = field.getAnnotation(JsonField.class).value(); if (annotationValue.isEmpty()) { return field.getName(); } else { return annotationValue; } } } Note that multiple responsibilities have been combined into this class for the sake of brevity. For a refactored version of this serializer class, see this branch in the codebase repository. We also create an exception that will be used to denote if an error has occurred while processing the object supplied to our serialize method: public class JsonSerializeException extends Exception { private static final long serialVersionUID = -8845242379503538623L; public JsonSerializeException(String message) { super(message); } } Although the JsonSerializer class appears complex, it consists of three main tasks: (1) finding all fields of the supplied class annotated with the @JsonField annotation, (2) recording the field name (or the explicitly provided field name) and value for all fields that include the @JsonField annotation, and (3) converting the recorded field name and value pairs into a JSON string. 上面这个 JsonSerializer 类有一些复杂，它包含了3个主要任务： 1\. 发现所有被@JsonField注解的域。 2\. 记录所有包含在@JsonField注解的域的名字，或者明确提供域的名字和所有域的值。 3\. 转换这些域的名和值到一个JSON字符串。 The line requireNonNull(object).getClass() simply checks that the supplied object is not null (and throws a NullPointerException if it is) and obtains the [Class](https://docs.oracle.com/javase/9/docs/api/java/lang/Class.html) object associated with the supplied object. We will use this Class object shortly to obtain the fields associated with the class. Next, we create a Map of Strings to Strings, which will be used store the field name and value pairs. 这个代码里面涉及到了一些新的 Java 的功能，我以前没有接触过，例如 requireNonNull(object).getClass() 可以检查一个对象是否是 null， 如果不是，则去获取这个对象的 [Class] 信息。 With our data structures established, we next iterate through each field declared in the class of the supplied object. For each field, we configure the field to suppress Java language access checking when accessing the field. This is a very important step since the fields we annotated are private. In the standard case, we would be unable to access these fields, and attempting to obtain the value of the private field would result in an IllegalAccessException being thrown. In order to access these private fields, we must instruct the reflection API to suppress the standard Java access checking for this field using the setAccessible method. The setAccessible(boolean) documentation defines the meaning of the supplied boolean flag as follows: 这里涉及到了一个比较关键的地方，即权限问题，本来 Java 的这些私有域都是不能被访问的，为了访问这些私有域，我们必须通知反射的 API 去抑制住标准的 Java 访问检查，即使用 setAccessible 这个方法。 A value of true indicates that the reflected object should suppress Java language access checking when it is used. A value of false indicates that the reflected object should enforce Java language access checks. Note that with the introduction of modules in Java 9, using the setAccessible method requires that the package containing the class whose private fields will be accessed should be declared open in its module definition. For more information, see this explanation by Michał Szewczyk and Accessing Private State of Java 9 Modules by Gunnar Morling. 这里会有一个访问权限的问题，因为注解关联的域是私有域，那么想要访问，需要修改反射时的访问权限。 After gaining access to the field, we check if the field is annotated with the @JsonField. If it is, we determine the name of the field (either through an explicit name provided in the @JsonField annotation or the default name, which equals the variable name of the field) and record the name and field value in our previously constructed map. Once all fields have been processed, we then convert the map of field names to field values (jsonElements) into a JSON string. We accomplish by converting the map into a stream of entries (key-value pairs for each entry in the map), mapping each entry to a string of the form &quot;&lt;fieldName&gt;&quot;:&quot;&lt;fieldValue&gt;&quot;, where &lt;fieldName&gt; is the key for the entry and &lt;fieldValue&gt; is the value for the entry. Once all entries have been processed, we combine all of these entry strings with a comma. This results in a string of the form &quot;&lt;fieldName1&gt;&quot;:&quot;&lt;fieldValue1&gt;&quot;,&quot;&lt;fieldName2&gt;&quot;:&quot;&lt;fieldValue2&gt;&quot;,.... Once this terminal string has been joined, we surround it with curly braces, creating a valid JSON string. In order to test this serializer, we can execute the following code: Car car = new Car(&quot;Ford&quot;, &quot;F150&quot;, &quot;2018&quot;); JsonSerializer serializer = new JsonSerializer(); serializer.serialize(car); This results in the following output: {&quot;model&quot;:&quot;F150&quot;,&quot;manufacturer&quot;:&quot;Ford&quot;} As expected, the maker and model fields of the Car object have been serialized, using the name of the field (or the explicitly supplied name in the case of the maker field) as the key and the value of the field as the value. Note that the order of JSON elements may be reversed from the output seen above. This occurs because there is no definite ordering for the array of declared fields for a class, as stated in the getDeclaredFieldsdocumentation: The elements in the returned array are not sorted and are not in any particular order. Due to this limitation, the order of the elements in the JSON string may vary. To make the order of the elements deterministic, we would have to impose ordering ourselves (such as by sorting the map of field names to field values). Since a JSON object is defined as an unordered set of name-value pairs, as per the JSON standard, imposing ordering is unneeded. Note, however, a test case for the serialize method should pass for either {&quot;model&quot;:&quot;F150&quot;,&quot;manufacturer&quot;:&quot;Ford&quot;} or {&quot;manufacturer&quot;:&quot;Ford&quot;,&quot;model&quot;:&quot;F150&quot;}. 上面输出的 JSON 并没有排序，这并没有什么问题，规则中并没有要求有顺序。 Conclusion 结论 Java annotations are a very powerful feature in the Java language, but most often, we are the users of standard annotations (such as @Override) or common framework annotations (such as @Autowired), rather than their developers. While annotations should not be used in place of interfaces or other language constructs that properly accomplish a task in an object-oriented manner, they can greatly simplify repetitive logic. For example, rather than creating a toJsonStringmethod within an interface and having all classes that can be serialized implement this interface, we can annotate each serializable field. This takes the repetitive logic of the serialization process (mapping field names to fields values) and places it into a single serializer class. It also decouples the serialization logic from the domain logic, removing the clutter of manual serialization from the conciseness of the domain logic. While custom annotations are not frequently used in most Java applications, knowledge of this feature is a requirement for any intermediate or advanced user of the Java language. Not only will knowledge of this feature enhance the toolbox of a developer, which is just as important, but it will also aid in the understanding of the common annotations in the most popular Java frameworks. 总结本文以一个实际的例子，描述了怎么去写一个 Java 的注解，反复理解这个过程，对于理解注解，有很好的帮助。 其实通过一些特定的类，可以反向获取到很多 Java 对象类的信息，例如域，例如注解，这样来给原本的对象类增加了一些功能。 这里面涉及到的类、方法、用法：Class 这个类可以获取对象类的基本信息，从 JDK 1.0 就开始存在了，以前研究反射应该是用过，但是没有仔细琢磨过这个类。requireNonNull 这个是 Objects 的一个静态方法，1.7 之后引入的，这个类可以理解为一个工具类，它的注释里面也写着 utilities。Field 用来动态访问类或者接口的类@Test JUnit 的注解NullPointerException静态 import 参考底下的链接，1.5 版本引入的，不再需要类名，直接可以使用静态方法，不过不推荐使用 参考https://dzone.com/articles/creating-custom-annotations-in-javahttps://www.geeksforgeeks.org/static-import-java/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树学习笔记（未完待续）]]></title>
    <url>%2Fdata-structure%2FBinary-tree-note.html</url>
    <content type="text"><![CDATA[摘要二叉树学习笔记（未完待续）。 博客IT老兵驿站。 前言昨天（2019-11-07）复习红黑树，发现红黑树和二叉树密不可分，所以这里再复习一下二叉树。 在大学的时候，这块我很认真地学习了一遍。大学毕业后，因为找工作的缘故，我又多次对这块进行过认真的学习，对于这块，心里还是比较清楚的。 现在这个笔记呢，既复习一下知识和概念，也回顾总结一下很多经历过的事情。 正文定义参考维基百科 二叉查找树（英语：Binary Search Tree），也称为二叉搜索树、有序二叉树（ordered binary tree）或排序二叉树（sorted binary tree），是指一棵空树或者具有下列性质的二叉树：若任意节点的左子树不空，则左子树上所有节点的值均小于它的根节点的值；若任意节点的右子树不空，则右子树上所有节点的值均大于它的根节点的值；任意节点的左、右子树也分别为二叉查找树；没有键值相等的节点。 对于定义，这里面会隐含一些理解上需要打通的问题，例如树的高度和叶子的关系，这也是经常出现在面试题里面的一个问题。今天事情有点多，先不做这块的总结了。 遍历二叉树的遍历分为前序遍历、中序遍历、后序遍历，这个前中后是以输出根结点关键字的顺序来区别的。 note：二叉树的遍历涉及到了一个很关键的计算机算法，递归算法，对这个算法，原本一直有点畏惧，一直没有搞清楚。在05年去OpenTV面试的时候，被这个算法给难住了，痛定思痛，那次面试失利之后，对这个算法进行了深度的学习和研究，后来在实际工作中，在合理的时机，也尝试使用了这个算法，现在算是基本掌握了，所以很多事情，躲是躲不开的。 每个结点的内容（摘抄自《算法导论》，这里用的是“结点”）： 其中每个结点就是一个对象。除了 key 和卫星数据之外，每个结点还包含属性 left、right 和 p，它们分别指向结点的左孩子、右孩子和双亲。如果某个孩子结点和父结点不存在，则相应属性的值为 NIL。 发现《算法导论》这里居然印错了，不知道家里面那个纸版的，是不是也是这样，这是第三版了，还会有这样的错误，不应该。 总结今天暂时写到这里，未完待续…… 参考https://zh.wikipedia.org/wiki/%E4%BA%8C%E5%85%83%E6%90%9C%E5%B0%8B%E6%A8%B9]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红黑树学习笔记]]></title>
    <url>%2Fdata-structure%2FB-tree-note-1.html</url>
    <content type="text"><![CDATA[摘要红黑树学习笔记。 博客IT老兵驿站。 前言在08、09年的时候，那个时候因为工作的需求，需要研究文件系统，然后就遇到了红黑树，也就研究了一下红黑树，不过时至今日，感觉已经记不太清楚了，感觉当时可能也没有研究的很透彻。 最近的工作中，又遇到了红黑树，就捡起来复习复习。孔子说，“温故而知新，可以为师矣”，我为不了师，不过发现，温故确实是可以知新的。 正文定义参考维基百科： 节点是红色或黑色。根是黑色。所有叶子都是黑色（叶子是NIL节点）。每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。）从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。 想要充分理解红黑树，有一些前提条件，需要充分理解二叉树和平衡二叉树，否则，对于红黑树，是很难充分理解的，所以，红黑树可能先看到这里，先去仔细看看平衡二叉树。 参考《算法导论》https://zh.wikipedia.org/wiki/%E7%BA%A2%E9%BB%91%E6%A0%91]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL的聚集索引]]></title>
    <url>%2Fmysql%2Fclustered-index-mysql.html</url>
    <content type="text"><![CDATA[概要MySQL 的聚集索引，有的地方又叫聚簇索引，其实英文单词是 cluster。 博客原帖收藏于IT老兵驿站。 正文什么是聚集索引？参考这里 A clustered index, on the other hand, is actually the table. It is an index that enforces the ordering on the rows of the table physically. 聚集索引就是说索引和数据是在一起的。一般的索引是单独的一个数据结构，而数据是一个数据结构；而聚集索引是说这两块内容是在一起的，并且是有顺序的。 参考官网： 14.6.2.1 Clustered and Secondary IndexesEvery InnoDB table has a special index called the clustered index where the data for the rows is stored. Typically, the clustered index is synonymous with the primary key. To get the best performance from queries, inserts, and other database operations, you must understand how InnoDB uses the clustered index to optimize the most common lookup and DML operations for each table.每一个 InnoDB 表都会有一个特别的索引，被称作聚簇索引，存储了行的数据。聚簇索引是主键的同义词。When you define a PRIMARY KEY on your table, InnoDB uses it as the clustered index. Define a primary key for each table that you create. If there is no logical unique and non-null column or set of columns, add a new auto-increment column, whose values are filled in automatically.If you do not define a PRIMARY KEY for your table, MySQL locates the first UNIQUE index where all the key columns are NOT NULL and InnoDB uses it as the clustered index.MySQL 会自己选择一个唯一索引来作为主键，如果你没有定义主键的话。If the table has no PRIMARY KEY or suitable UNIQUE index, InnoDB internally generates a hidden clustered index named GEN_CLUST_INDEX on a synthetic column containing row ID values. The rows are ordered by the ID that InnoDB assigns to the rows in such a table. The row ID is a 6-byte field that increases monotonically as new rows are inserted. Thus, the rows ordered by the row ID are physically in insertion order. 上文的摘抄讲了 MySQL 是如何使用聚集索引的。 光看上文，还是有些没有搞明白，参考《高可用 MySQL》，找到这张图：从这张图就可以看得比较明白了，叶子节点包含所有数据，而非叶子节点里面存在的是索引，这样其实数据和索引在一个结构里面，这就是聚簇索引。 参考http://www.mysqltutorial.org/mysql-index/mysql-clustered-index/https://dev.mysql.com/doc/refman/5.7/en/innodb-index-types.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>聚集索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B树、B+树学习之一]]></title>
    <url>%2Fmysql%2FB-tree-note-1.html</url>
    <content type="text"><![CDATA[前言这篇帖子原本写于2015年，写于厦门，时至今日（2019年11月5日）来看，感觉当时记录笔记，缺乏一个完整的思路，记录的不好，不方便以后的阅读，今天刚好又遇到了B树的学习，那么把它修改一下。 其实在11年刚进360的时候，学习 MySQL，那个时候对B树做过一次很深的研究，自己实践过一遍 B 树的数据结构，不过那个时候没有云笔记，自己也没有做好笔记管理，现在找不到了，对于这些，用一句香港连续剧里面的话可以很准确的表达心情，无限唏嘘。 原本的帖子不是用 MD 写的，不利于编辑，只好删除掉，重新用 MD 格式来编辑。 正文个人理解，现在，B 树主要的一个应用是数据库存储引擎（InnoDB），用B树的原因，是为了更少地访问磁盘，更快地检索到数据的位置。（关于这一点，《算法导论》在介绍 B 树时，也是这样地描写的） 所以，这里要对磁盘的很多概念有所了解。 磁盘RPM磁盘参数 RPM 是Revolutions Per Minute的缩写 revolutionn. 革命；旋转；运行；循环这么说，旋转就是革命了。 《算法导论》第三版第279页： 对存储在磁盘上的一颗大的B树，通常看到分支因子在50~2000之间，具体取决于一个关键字相对于一页的大小。一个大的分支因子可以大大地降低树的高度以及查找任何一个关键字所需的磁盘存取次数。 块的概念参考：http://oss.org.cn/kernel-book/ch09/9.1.htm，很可惜，这个网站已经不能访问了，只能参考《深入理解Linux内核》了。 9.1 基本概念在上一章中，我们把Ext2、Minix、Ext等实际可使用的文件系统称为具体文件系统。具体文件系统管理的是一个逻辑空间，这个逻辑空间就象一个大的数组，数组的每个元素就是文件系统操作的基本单位——逻辑块，逻辑块是从0开始编号的，而且，逻辑块是连续的。与逻辑块相对的是物理块，物理块是数据在磁盘上的存取单位，也就是每进行一次I/O操作，最小传输的数据大小。我们知道数据是存储在磁盘的扇区中的，那么扇区是不是物理块呢？或者物理块是多大呢？这涉及到文件系统效率的问题。 参考维基百科 In computing (specifically data transmission and data storage), a block, sometimes called a physical record, is a sequence of bytes or bits, usually containing some whole number of records, having a maximum length, a block size.[1] Data thus structured are said to be blocked. The process of putting data into blocks is called blocking, while deblocking is the process of extracting data from blocks. Blocked data is normally stored in a data buffer and read or written a whole block at a time. In the 1970s IBM introduced the Direct Access Storage Device (DASD) with fixed-block architecture using sizes of 512, 1024, 2048, or 4096 bytes. Cray Research had an 819 disk controller in 1975 that transferred 512 64-bit words (4096 bytes) per sector. Later,[specify] hard disk drives supporting 1,024-byte sectors began to be integrated into consumer electronics devices such as portable media players and digital video cameras.[citation needed] However, by far the majority of hard drives shipped up to the start of the 2010s still used the traditional 512-byte sector size. 从这里感觉，sector 和 block 是一个概念。 参考这里，看到：​​​​? 树的阶数如何翻译，应该用 orderhttps://en.wikipedia.org/wiki/Tree_(data_structure)。 block 的概念https://en.wikipedia.org/wiki/Block_(data_storage)。 B 树的定义维基百科的定义 根据 Knuth 的定义，一个 m 阶的B树是一个有以下属性的树：每一个节点最多有 m 个子节点每一个非叶子节点（除根节点）最少有 ⌈m/2⌉ 个子节点如果根节点不是叶子节点，那么它至少有两个子节点有 k 个子节点的非叶子节点拥有 k − 1 个键所有的叶子节点都在同一层 这是维基上 B 树的定义。（各个地方的定义不尽相同，可能背后的概念是一样的，我觉得不一致，可能是因为我理解的还不够透彻。） 这个地方没有强调关键字是有序的。 数据结构教程手录，摘自《数据结构（C语言版） 严蔚敏 吴伟民 清华大学出版社》。 B树的概念：一颗m阶的B-树，或为空树，或为满足下列特征的m叉树：1.树中每个结点至多有m棵子树。2.若根结点不是叶子结点，则至少有两棵子树。3.除根之外的所有非终端结点至少有[m/2]棵子树。4.所有的非终端结点中包含下列信息数据(n, A0, K1, A1, K2, A2, …, Kn, An)其中：Ki(i=1, …, n)为关键字，且Ki &lt; Ki+1(i=1, …, n-1); Ai(i=0, …, n)为指向子树根结点的指针，且指针Ai-1所指向子树中所有结点的关键字均小于Ki(i=1, …, n), An所指子树中所有结点的关键字均大于Kn， n([m/2]-1&lt;=n&lt;=m-1)为关键字的个数（或n+1为子树个数）。5.所有的叶子结点都出现在同一层次上，并且不带信息（可以看作是外部结点或查找失败的结点，实际上这些结点不存在，指向这些结点的指针为空）。 （插曲1：《数据结构（C语言版） 严蔚敏 吴伟民》这本书，我原本有两本，毕业之后看了好几遍，在搬家时终于决定卖掉来减轻负担，结果最近发现有些内容，原本以为理解清楚了，实际上还是有更深层次的内容没有理解，最近又打算买回来，只怪当时无知啊。 插曲2：我一直感觉这本书写的不太好，写的中间过程和思路不够清晰，不如《数据结构与算法分析—C语言描述 （美）Mark Allen Weiss 著 冯舜玺 译 机械工业出版社 》写的清晰，这会儿突然想到清华的学习方法，感觉稍稍明白了一些其中的味道，也许这本书不是一本很好的用来自学的书，但是却是一本很好的教材，这里面的感想总结在这里。 11年研究B树，研究了一段时间，还写了写算法，这次想捡起来，发现都忘记了，这次又花了一段时间研究，发现所有的根本都要围绕定义去思考，突然理解了高中数学中的定理和定理的属性的意思，定理定义是约定、规定一个东西，然后这个东西会表现出一些特有的性质来，这样你可以根据其他东西是否满足这个性质来进行一定地判断。） 这里面要搞清楚树的高度、树的度、树和子树的关系，子树和关键字之间的关系，这些关系搞清楚，这个 B 树就搞明白了。 《高可用 MySQL》这里所说的 B+ 树的概念和清华严蔚敏说的《数据结构》里面讲的不一样，《数据结构》说的是 B+ 树是说数据都只在叶子节点，这里说的是 B+ 树是叶子节点指向下一个叶子节点。 总结今天先整理到这里，明天继续完善，今天还没有涉及到 B 树的主要概念，只是涉及到了磁盘的一些概念。 参考http://blog.csdn.net/liuaigui/article/details/6168186。http://stackoverflow.com/questions/12345804/difference-between-blocks-and-sectorshttps://en.wikipedia.org/wiki/Disk_sectorhttp://baike.baidu.com/view/110462.htm 这里是我原来所学的http://oss.org.cn/kernel-book/ch09/9.1.htm 物理块的概念https://en.wikipedia.org/wiki/B-treehttp://blog.csdn.net/v_july_v/article/details/6530142 这个帖子介绍的比较详细]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>B 树</tag>
        <tag>B+ 树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux的top和free命令里面的buffer和cache]]></title>
    <url>%2Flinux%2Flinux-top-buffer-cache.html</url>
    <content type="text"><![CDATA[概要Linux的top和free命令里面的buffer和cache 博客原帖收藏于IT老兵博客 前言linux里面top命令和free命令都会有一个buff/cache，从08年就一直遇到这个，感觉一直没有搞的太明白，这次好好地做一个总结。这个总结不见得能一步到位，到不了位，就分步来走，聚沙成塔。 正文参考这里， buffersMemory used by kernel buffers (Buffers in /proc/meminfo)cacheMemory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)buff/cacheSum of buffers and cacheavailableEstimation of how much memory is available for starting new applications, without swapping. Unlike the data provided by the cache or free fields, this field takes into account page cache and also that not all reclaimable memory slabs will be reclaimed due to items being in use (MemAvailable in /proc/meminfo, available on kernels 3.14, emulated on kernels 2.6.27+, otherwise the same as free) 这里说，buffer是内核所使用的buffer，而cache是page cache和slabs所使用的。 内核所使用的buffer，暂时不明白。而page cache，我大概知道，是文件系统所使用的缓存。slabs机制是内存分配的一个机制，预分配一些大块的内存，在使用的时候，再从中分配，易于回收，这样是为了规避碎片化，slab本身的意思就是大块的厚板。 上面的帖子还参考了这里，感觉讲解的更加细致。 1. What is the difference between “buffer”, and the other type of cache?Buffers reports the amount of page cache used for block devices. The kernel has to deliberately subtract this amount from the rest of the page cache when it reports Cached. 这里说，buffer也是指的是page cache，是用于块设备的page cache。 Early UNIX had a “buffer cache” of disk blocks, and did not have mmap(). Apparently when mmap() was first added, they simply bolted the page cache on top of the buffer cache. This is as messy as it sounds. Eventually, UNIX-based OS’s got rid of the buffer cache. So now all file cache is in units of pages. Pages are looked up by (file, offset), not by location on disk. This was called “unified buffer cache”, perhaps because people were more familiar with “buffer cache”.[3] 看到这段，我有点印象了，在08年的时候，那会还是2.6版本的Linux，而我正在看《情景》，《情景》是根据2.4来写的，所以，那会buffer是说用于块设备，就是这是一个数据结构，单独用作了块设备，而看上文，后来对这里进行了统一，都用统一的数据结构page来处理。 4. Why might we expect Buffers in particular to be larger or smaller?In this case it turns out the ext4 journal size for my filesystem is 128M. So this explains why 1) my buffer cache can stabilize at slightly over 128M; 2) buffer cache does not scale proportionally with the larger amount of RAM on my laptop. 由上文可以看到，Buffer一般基本是固定的，不随着RAM的变大而变大。 For some other possible causes, see What is the buffers column in the output from free? Note that “buffers” reported by free is actually a combination of Buffers and reclaimable slab memory. 这里说buffer是包含着Buffers和slab，这个地方就和上面第一篇文章矛盾了，那篇文章说的是cache包含着slab。 ……未完待续……（2019-10-19） 总结参考https://unix.stackexchange.com/questions/390518/what-do-the-buff-cache-and-avail-mem-fields-in-top-meanhttp://man7.org/linux/man-pages/man1/free.1.htmlhttps://unix.stackexchange.com/questions/261247/how-can-i-get-the-amount-of-available-memory-portably-across-distributions]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>top</tag>
        <tag>cache</tag>
        <tag>buffer</tag>
        <tag>free</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的VO、DO、DTO和Java Bean学习]]></title>
    <url>%2Fjava%2Fjava-vo-dto-pojo.html</url>
    <content type="text"><![CDATA[概要Java的VO、DO、DTO和Java Bean学习。 前言通过Stack Overflow的一篇文章，来学习一下Java的这几个概念。 博客原帖收藏于IT老兵博客。 正文JavaBeansA JavaBean is a class that follows the JavaBeans conventions as defined by Sun. Wikipedia has a pretty good summary of what JavaBeans are: JavaBeans are reusable software components for Java that can be manipulated visually in a builder tool. JavaBean是可重用的软件组件，使用可以被构建工具去可视化地操作。 Practically, they are classes written in the Java programming language conforming to a particular convention. They are used to encapsulate many objects into a single object (the bean), so that they can be passed around as a single bean object instead of as multiple individual objects. A JavaBean is a Java Object that is serializable, has a nullary constructor, and allows access to properties using getter and setter methods. 一个Java Bean 是一个序列化的Java对象，有一个非空的构造器，允许getter和setter方法去访问属性。 In order to function as a JavaBean class, an object class must obey certain conventions about method naming, construction, and behavior. These conventions make it possible to have tools that can use, reuse, replace, and connect JavaBeans. 需要遵循一些关于方法、命名、构造器和表现的约定，这样才好去被使用、重用、取代和连接其他的JavaBean。 The required conventions are: 要求是： The class must have a public default constructor. This allows easy instantiation within editing and activation frameworks. 类必须有一个共有的默认的构造方法。 The class properties must be accessible using get, set, and other methods (so-called accessor methods and mutator methods), following a standard naming convention. This allows easy automated inspection and updating of bean state within frameworks, many of which include custom editors for various types of properties. 类的属性必须可以被get、set和其他被称作访问器和操作器来访问。（对这里的accessor和mutator有些疑问） The class should be serializable. This allows applications and frameworks to reliably save, store, and restore the bean&apos;s state in a fashion that is independent of the VM and platform. 可以被序列化。 Because these requirements are largely expressed as conventions rather than by implementing interfaces, some developers view JavaBeans as Plain Old Java Objects that follow specific naming conventions. POJOA Plain Old Java Object or POJO is a term initially introduced to designate a simple lightweight Java object, not implementing any javax.ejb interface, as opposed to heavyweight EJB 2.x (especially Entity Beans, Stateless Session Beans are not that bad IMO). Today, the term is used for any simple object with no extra stuff. Again, Wikipedia does a good job at defining POJO: 这个是针对EJB方案来的，不需要遵循EJB规范的原生的Java类。 POJO is an acronym for Plain Old Java Object. The name is used to emphasize that the object in question is an ordinary Java Object, not a special object, and in particular not an Enterprise JavaBean (especially before EJB 3). The term was coined by Martin Fowler, Rebecca Parsons and Josh MacKenzie in September 2000: &quot;We wondered why people were so against using regular objects in their systems and concluded that it was because simple objects lacked a fancy name. So we gave them one, and it&apos;s caught on very nicely.&quot; The term continues the pattern of older terms for technologies that do not use fancy new features, such as POTS (Plain Old Telephone Service) in telephony, and PODS (Plain Old Data Structures) that are defined in C++ but use only C language features, and POD (Plain Old Documentation) in Perl. The term has most likely gained widespread acceptance because of the need for a common and easily understood term that contrasts with complicated object frameworks. A JavaBean is a POJO that is serializable, has a no-argument constructor, and allows access to properties using getter and setter methods. An Enterprise JavaBean is not a single class but an entire component model (again, EJB 3 reduces the complexity of Enterprise JavaBeans). As designs using POJOs have become more commonly-used, systems have arisen that give POJOs some of the functionality used in frameworks and more choice about which areas of functionality are actually needed. Hibernate and Spring are examples. Value ObjectA Value Object or VO is an object such as java.lang.Integer that hold values (hence value objects). For a more formal definition, I often refer to Martin Fowler&apos;s description of Value Object: In Patterns of Enterprise Application Architecture I described Value Object as a small object such as a Money or date range object. Their key property is that they follow value semantics rather than reference semantics. VO是一个小的对象，就像Money或者数据范围对象。它们的关键属性是，它们遵循value的语法而不是reference的语法。 You can usually tell them because their notion of equality isn&apos;t based on identity, instead two value objects are equal if all their fields are equal. Although all fields are equal, you don&apos;t need to compare all fields if a subset is unique - for example currency codes for currency objects are enough to test equality. A general heuristic is that value objects should be entirely immutable. If you want to change a value object you should replace the object with a new one and not be allowed to update the values of the value object itself - updatable value objects lead to aliasing problems. Early J2EE literature used the term value object to describe a different notion, what I call a Data Transfer Object. They have since changed their usage and use the term Transfer Object instead. You can find some more good material on value objects on the wiki and by Dirk Riehle. Data Transfer ObjectData Transfer Object or DTO is a (anti) pattern introduced with EJB. Instead of performing many remote calls on EJBs, the idea was to encapsulate data in a value object that could be transfered over the network: a Data Transfer Object. Wikipedia has a decent definition of Data Transfer Object: DTO是用来进行网络传输的封装对象。 Data transfer object (DTO), formerly known as value objects or VO, is a design pattern used to transfer data between software application subsystems. DTOs are often used in conjunction with data access objects to retrieve data from a database. DTO也是一个VO，经常被用于去连接从数据库获取数据的DAO。 The difference between data transfer objects and business objects or data access objects is that a DTO does not have any behaviour except for storage and retrieval of its own data (accessors and mutators). DTO和BO或者DAO的区别是，DTO出了存储和获取它自己的数据之外没有其他行为。 In a traditional EJB architecture, DTOs serve dual purposes: first, they work around the problem that entity beans are not serializable; second, they implicitly define an assembly phase where all data to be used by the view is fetched and marshalled into the DTOs before returning control to the presentation tier. 总结其实看了一下，这些概念原本都是EJB规范中的，现在EJB因为过于厚重，慢慢大家开始使用更轻量级的Java框架，但是这些概念被继承了下来。 参考https://stackoverflow.com/questions/1612334/difference-between-dto-vo-pojo-javabeanshttps://www.programering.com/a/MDM2kjNwATc.html 这个帖子也讲了很多，有待继续研究]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>VO</tag>
        <tag>DTO</tag>
        <tag>POJO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[make 和 cmake 的区别学习]]></title>
    <url>%2Flinux%2Fmake-vs-cmake.html</url>
    <content type="text"><![CDATA[概要make 和 cmake 的区别学习 博客博客地址：IT老兵驿站 正文 “The build process has one step if you use a Makefile, namely typing “make” at the command line. For CMake, there are two steps: First, you need to setup your build environment (either by typing cmake &lt;source_dir&gt; in your build directory or by running some GUI client). This creates a makefile or something equivalent, depending on the build system of your choice (e.g. Make on *nix, VC++ or MinGW on Windows, etc). The build system can be passed to CMake as a parameter. However, CMake makes reasonable default choices depending on your system configuration. Second, you perform the actual build in the selected build system.” 参考上文所说，cmake 是两步，通过执行cmake &lt;source_dir&gt;（根据CMakeLists.txt文件的配置）， 先根据环境生成 Makefile 文件，然后正常使用 make 去进行编译。 传统的make，我这里的理解是通过configure去扫描软件的依赖，然后根据Makefile.in(往往是通过automake等工具所生成的)，生成Makefile。 cmake更多是帮助进行跨平台的编译。 初步理解是这样，等待以后来完善。 参考https://www.quora.com/What-is-the-difference-between-CMake-and-makehttps://prateekvjoshi.com/2014/02/01/cmake-vs-make/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>make</tag>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-rev-parse 命令学习]]></title>
    <url>%2Fgit%2Fgit-rev-parse.html</url>
    <content type="text"><![CDATA[概要Git：git-rev-parse 命令学习 博客原帖收藏于IT老兵博客 前言在研究Jenkins的时候，遇到了git rev-parse这个命令，这里学习一下这个命令。 正文 git-rev-parse - Pick out and massage parameters 这是这个命令的概要解释，理解起来有一点障碍，挑选出来并且“按摩”参数，这个有点意思啊，对参数进行一下按摩。（这里这个massage是不是应该按照调整来理解呢？） Many Git porcelainish commands take mixture of flags (i.e. parameters that begin with a dash -) and parameters meant for the underlying git rev-list command they use internally and flags and parameters for the other commands they use downstream of git rev-list. This command is used to distinguish between them. 许多Git porcelainish命令（这个地方不知道怎么理解）会混合使用标志（即以短划线开头的参数-）以及参数，这些参数用于git rev-list它们在内部使用的基础命令，以及标志和参数用于其（git rev-list）下游使用的其他命令。这个命令用于区分它们。 看到这里，感觉这个命令好像只是为了鉴别这些参数是否用于git rev-list的，底下参考的stackoverflow的帖子说，这是一个辅助的探测（管道）工具。 git rev-parse is an ancillary plumbing command primarily used for manipulation.–verify to verify that the specified object is a valid git object. 例如一些常见场景：显示指定提交号的SHA112$ git rev-parse 5a382c95a382c95394410bb716d92ee7418c8e3a17eb8c3 验证指定的对象是一个有效的git对象12$ git rev-parse --verify HEADa1498cc2435b5df53653279395dac42e14e1d393 –verifyVerify that exactly one parameter is provided, and that it can be turned into a raw 20-byte SHA-1 that can be used to access the object database. If so, emit it to the standard output; otherwise, error out. 这里会去通过HEAD去获取a1498cc2435b5df53653279395dac42e14e1d393，然后通过这个去访问对象的数据库。 显示git默认目录12$ git rev-parse --git-dir.git 以下还有几项，不具体翻译了。 Checking if you’re currently within a repository using –is-inside-git-dir or within a work-tree using –is-inside-work-tree Checking if the repo is a bare using –is-bare-repository Printing SHA1 hashes of branches (–branches), tags (–tags) and the refs can also be filtered based on the remote (using –remote) –parse-opt to normalize arguments in a script (kind of similar to getopt) and print an output string that can be used with eval Massage just implies that it is possible to convert the info from one form into another i.e. a transformation command. These are some quick examples I can think of: 这里解释了Massage，Massage在这里暗示着有可能会把一个信息的格式转换成另外一种。 a branch or tag name into the commit’s SHA1 it is pointing to so that it can be passed to a plumbing command which only accepts SHA1 values for the commit. 把一个分支名或者tag名转换成了一个提交的SHA1，要是从这个意义来理解，这个plumbing还真是管道的意思。 a revision range A..B for git log or git diff into the equivalent arguments for the underlying plumbing command as B ^A 总结这个命令有点绕，第一天完全没看懂，第二天才初步搞明白一点。 参考https://git-scm.com/docs/git-rev-parse 官网https://cloud.tencent.com/developer/section/1138781 这里有点意思，是对上面官网的直接翻译https://stackoverflow.com/questions/15798862/what-does-git-rev-parse-do 这里讲了一些常见的用法]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git rev-parse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java：Shiro+SpringMVC的集成实践]]></title>
    <url>%2Fjava%2Fjava-shiro-springmvc-integration.html</url>
    <content type="text"><![CDATA[摘要Java：Shiro+SpringMVC的集成实践。 博客原帖位于IT老兵博客。 前言个人感觉，Shiro的官网有一个问题，讲的不够清楚，尽管看上去好像讲的挺明白，但是我总是感觉很多地方不够清楚，事实上，在阅读了很多帖子之后，发现很多人都对这一点存在疑问，那就不是我一个人的问题了。 Shiro的官网缺乏完整的例子，而且我所处理的项目是Spring的项目，如何清楚地集成在一起，似乎还没有看到，很多地方都需要摸索，看了张开涛的博客，下面一样有很多人存有疑问。 之前研究这个，花了几天的时间研究理论，感觉自己已经明白了（这个感觉在另外一篇帖子《Java：Shiro的架构学习笔记》里面有提到），实际上是，纸上得来终觉浅，绝知此事要躬行。 这篇文章结合着自己的例子，把所理解到的东西做一个总结，以备日后查看，也给需要的朋友们一个参考。 正文项目用的是XML配置，至于注解如何配置，暂时还没有时间去研究。 项目中定义一个spring-shiro.xml文件，配置在classpath里面，可以被系统读取到，这块涉及Spring读取配置文件的功能，官网是写在了applicationContext.xml文件里面，然后在web.xml里面定义filter，现在做项目，似乎已经很少用到这个web.xml文件，基本都是定义在spring-mvc.xml这个文件里面，这里给shiro单独定义了一个配置文件，原理是一样的。 先定义filter： &lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt; &lt;/bean&gt; 这个将会构造一个shiroFilter，参数是securityManager。 定义securityManager： &lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt; &lt;property name=&quot;realm&quot; ref=&quot;tokenRealm&quot; /&gt; &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot;&gt;&lt;/property&gt; &lt;property name=&quot;sessionManager&quot; ref=&quot;sessionManager&quot; /&gt; &lt;property name=&quot;subjectFactory&quot; ref=&quot;subjectFactory&quot;/&gt; &lt;property name=&quot;subjectDAO.sessionStorageEvaluator.sessionStorageEnabled&quot; value=&quot;true&quot;/&gt; &lt;!-- By default the servlet container sessions will be used. Uncomment this line to use shiro&apos;s native sessions (see the JavaDoc for more): --&gt; &lt;property name=&quot;sessionMode&quot; value=&quot;http&quot;/&gt; &lt;/bean&gt; 这里构造了securityManager，并且传递了6个参数给它，每个参数可以是自己写的继承类，也可以是默认的类，这里涉及一些业务隐私的问题，不能都贴出来了。 第一个tokenRealm是用于进行认证的组件。 &lt;bean id=&quot;tokenRealm&quot; class=&quot;xx.xx.xx.TokenRealm&quot;&gt; &lt;property name=&quot;credentialsMatcher&quot; ref=&quot;credentialsMatcher&quot;/&gt; &lt;/bean&gt; 参数是自定义的一个凭证匹配器。这里需要覆写两个方法：protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authcToken) throws AuthenticationException 返回认证信息。和protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) 返回授权信息。这个地方之前一直没有搞明白，是最让我困惑的地方，doGetAuthenticationInfo的第一个参数就是login方法送过来的token，一般这个token带有username和password，这里根据这个用户名去把数据库把用户的密码取出来，然后构造一个SimpleAuthenticationInfo simpleAuthenticationInfo = new SimpleAuthenticationInfo(username, password, getName());返回，然后会交由匹配器去匹配，匹配器主要匹配第二个参数（原型是：SimpleAuthenticationInfo(Object principal, Object credentials, String realmName)），即凭证是否相等。 而自定义的匹配器大体是下面这样，覆写匹配的函数（增加了缓存来保存尝试次数）：123456789101112131415161718192021222324252627282930313233@Override public boolean doCredentialsMatch(AuthenticationToken token, AuthenticationInfo info) &#123; String username = (String) token.getPrincipal(); AtomicInteger retryCount = loginRetryCache.get(username); System.out.println(&quot;重试次数：&quot; + retryCount); if (retryCount != null &amp;&amp; retryCount.intValue() &gt;= maxRetryCount) &#123; throw new ExcessiveAttemptsException(&quot;username: &quot; + username + &quot; tried to login more than 5 times in period&quot;); &#125; boolean matches = super.doCredentialsMatch(token, info); if (matches) &#123; //clear retry data System.out.println(&quot;清除重试次数缓存&quot;); if (retryCount != null) &#123; loginRetryCache.remove(username); &#125; return true; &#125; else &#123; if (null == retryCount) &#123; retryCount = new AtomicInteger(1); loginRetryCache.put(username, retryCount); System.out.println(&quot;插入缓存，失败次数：&quot; + retryCount); &#125; else if (retryCount.incrementAndGet() &gt;= maxRetryCount) &#123; log.warn(&quot;username: &quot; + username + &quot; tried to login more than 5 times in period&quot;); throw new ExcessiveAttemptsException(&quot;username: &quot; + username + &quot; tried to login more than 5 times in period&quot;); &#125; retryCount = loginRetryCache.get(username); System.out.println(&quot;认证失败，失败次数：&quot; + retryCount); return false; &#125; &#125; 在login完成后，Shiro其实会返回给客户端一个JSESSIONID，并且会在缓存中保存关于这个会话的一些信息，这些会话信息会被定期清理（由调度任务15分钟或者是下一次访问时判断是否过期）或者是由logout方法主动注销掉。 总结初步总结了一下Shiro的用法，实践了一天，总结了一天，终于感觉搞明白了，使用Shiro的难度主要在于牵扯的类比较多，而且文档说的不是太清楚，需要自己反复地实践。这也可能说明它设计得很灵活，一般设计得很灵活的东西，都是不容易掌握，但是，一旦掌握了，就非常得方便。这篇帖子还会不断更新，直到把这个地方的概念全部梳理清楚。 参考https://shiro.apache.org/10-minute-tutorial.htmlhttps://shiro.apache.org/static/1.3.0/apidocs/org/apache/shiro/authc/SimpleAuthenticationInfo.htmlhttps://shiro.apache.org/architecture.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Shiro</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS 设计主题（迟来的总结）]]></title>
    <url>%2Fios%2Fios-themes.html</url>
    <content type="text"><![CDATA[摘要iOS 设计主题（迟来的总结） 博客原帖收藏于IT老兵博客。 前言尽管之前写过iOS的程序，但是没有像当年（注1）学习安卓那样，去学习一下iOS的设计理念，等了这么久，今天在这里补充一下对设计理念的学习。 这篇文章感觉比较重要，我尽量逐字翻译一下，感觉这样学习得就会更加深刻一些。 注1： 这里是指2013年，记得那会学习安卓的设计理念，有一句话让我印象很深，让便捷流于手纸，而安卓是偷师于苹果的设计，这里更需要好好研究一下苹果的设计了。 正文iOS Design ThemesAs an app designer, you have the opportunity to deliver an extraordinary product that rises to the top of the App Store charts. To do so, you’ll need to meet high expectations for quality and functionality. 作为一个 app 的设计者，你是拥有机会去交付一个非常特别的产品，可以跃升到 App Store 图表的顶部的（这里应该是指 App Store 的排行吧）。为了做到这个，你需要去满足对于质量和功能性的很高的期望。 Three primary themes differentiate iOS from other platforms: iOS相较于其他平台的三个主要的不同的主题： Clarity. Throughout the system, text is legible at every size, icons are precise and lucid, adornments are subtle and appropriate, and a sharpened focus on functionality motivates the design. Negative space, color, fonts, graphics, and interface elements subtly highlight important content and convey interactivity. 清晰。遍历整个系统，text 在每一个尺寸上是清晰的，icons 是精确和清晰的，adornments 是微妙和合适的，一个轮廓很清晰在功能上的聚焦激励着设计。负空间（怎么理解？），颜色，字体，图形，和界面元素微妙地使重要内容高亮并且传递着互动性。这里的几个单词需要单独学习一下：legiblelucid Deference. Fluid motion and a crisp, beautiful interface help people understand and interact with content while never competing with it. Content typically fills the entire screen, while translucency and blurring often hint at more. Minimal use of bezels, gradients, and drop shadows keep the interface light and airy, while ensuring that content is paramount. 顺从。流动性的动作和一个美观的界面会帮助人们理解并且和内容进行互动，而永远不用和它竞争（这里怎么理解？就是这个设计让用户很排斥？）。内容很典型地填充了整个屏幕，而半透明和模糊经常给予更多的提示。最小程度地使用 bezels（怎么理解？），gradients， 丢弃掉 shadows 来让界面变得 light 和 airy，保证内容是最重要的。deference Depth. Distinct visual layers and realistic motion convey hierarchy, impart vitality, and facilitate understanding. Touch and discoverability heighten delight and enable access to functionality and additional content without losing context. Transitions provide a sense of depth as you navigate through content. 深度。明确的可视化的层级和现实化的动作传递着层级，给予活力，和便于理解。触摸和可发现性提高了乐趣，使得可以对于功能的和额外的内容进行访问，而不丢失上下文。过渡提供了深度的感觉，当你在内容间浏览的时候。 总结总体感觉，这篇文章全文翻译起来有些难度，还有一些术语没有搞清楚，还需要继续补充。 参考https://developer.apple.com/design/human-interface-guidelines/ios/overview/themes/]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>主题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac系统查看端口占用的进程]]></title>
    <url>%2Flinux%2Fmac-find-port-occupy-process.html</url>
    <content type="text"><![CDATA[概要Mac系统查看端口占用的进程 博客原帖收藏于IT老兵博客 前言在Linux下，查看占用端口的进程可以使用netstat -antpl | grep port，但是在Mac下，这个netstat的功能缩水了（很多命令在Mac下都缩水了），所以怎么查看占用端口的进程呢？ 正文搜索了一遍，感觉可以使用lsof -i tcp:{port} 这个命令来实现，{port} 表示端口号。 1234-i [i] selects the listing of files any of whose Internet address matches the address specified in i. If no address is specified, this option selects the listing of all Internet and x.25 (HP-UX) network files. 查看一下参考中的手册，-i 的参数是用于检索Internet相关的信息，例如地址、协议等，上面那个用法就是检索tcp协议，端口号为指定端口号的句柄。 参考http://man7.org/linux/man-pages/man8/lsof.8.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>端口占用</tag>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式：职责链模式]]></title>
    <url>%2Fdesign-pattern%2Fchain-of-responsibility.html</url>
    <content type="text"><![CDATA[概要设计模式：职责链模式 博客原帖收藏于IT老兵博客。 前言职责链，chain of responsibility。这篇笔记第一版记录于2015年，那个时候在研究安卓代码的时候，安卓对于事件的传递使用到了职责链。现在研究到了Spring的filter chain。上面两个都是职责链和外观两种设计模式结合在一起使用。这篇笔记主要是结合四人帮的《设计模式》来学习，这本书写的有点深奥了，难懂，需要慢慢来消化。 正文1.意图使多个对象都有机会处理请求，从而避免请求的发送者和接受者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。处理对象需要连成一个链，不确定由谁来处理，不处理需要往下传，处理了也可以继续往下传（安卓的事件传递是处理了就不传了）。请求需要被传递，这个请求可能是原封不动地被传递，也有可能略作修改地去传递，所以，这里处理对象的接口最好也是一致的，继承自一个公共的父类（这个时候可能虚基类就比较合理了）。3.适用性在以下条件下使用Responsibisibility链：• 有多个的对象可以处理一个请求，哪个对象处理该请求运行时刻自动确定。 • 你想在不明确指定接收者的情况下，向多个对象中的一个提交一个请求。• 可处理一个请求的对象集合应被动态指定。 可处理一个请求的对象集合应被动态指定。 以上摘录自《设计模式》，奇怪的是《PHP设计模式介绍》这本书没有讲职责链，这个是不是和PHP本身面向对象的能力不足有关呢？这个问题留待以后去验证。我的理解：1.这个请求要可以被传递，那么这些对象最好是有相同的可以接收这样规格请求的接口，例如安卓的事件传递（安卓的事件传递就是一个职责链，哪一层的view消耗了该事件，就返回true，然后这个返回值就向回传递。在职责链前面往往会使用外观模式，对于外部用户，他并不知道后面的职责链，这个是有外观实现者来处理的）。如果接口不一致的话，恐怕就会有些问题，应该每个对象都可以单独接收这个请求，只是要处于某种特定的情况下。2.消耗了请求，请求就不再继续向下传递，而是往回返回了；但其实，个人感觉，没有消耗请求的对象也可以针对请求有所响应。3.每个对象都可以响应请求，也就是请求需要满足一定的条件，所以，对象的顺序是要合理安排的。上面引用的这段，是15年的笔记，记录了当时研究安卓代码时，对这个职责链的理解。 《设计模式》里面这个样例有些绕，需要整理一下思路。 参考http://blog.csdn.net/zhang_xinxiu/article/details/9963305， 职责链模式，这篇文章写的很好。http://blog.csdn.net/hguisu/article/details/7533759， 外观模式，这篇文章写的也很好。]]></content>
      <categories>
        <category>Design Pattern</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>职责链模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的数组和容器（1）]]></title>
    <url>%2Fjava%2Fjava-array-and-container-note.html</url>
    <content type="text"><![CDATA[Java的数组和容器（1） 博客原帖位于IT老兵博客 前言本文记录一下对Java的数组和容器的学习。 正文数组参考《Java编程思想（第4版）》来进行学习。章节 $5.8int [] a1;int a1[];这个是数组变量的定义，但是并没有分配空间。 初始化的方式是：int [] a1 = { 1, 2, 3, 4, 5 }; // 这样是直接初始化，应该是在栈中int [] a1 = new int[10]; // 这样初始化，就在堆中了 数组是一等公民，是语言的基本单位。 容器章节 $11 $11.1List、Set、Queue、Map，这些都属于容器类，这些都是以类的形态存在的，是语言的二等公民。原则上，能用容器就尽量使用容器，因为容器会提供更多的功能，例如空间的自动增长。 ArrayListArrayList是一个很常用的容器，继承于List。参考这里，ArrayList就类似于C++的vector。 ArrayList的构建，是需要使用实例化泛型来约束的。 章节 $11.2容器分为了Collection和Map两个大类。Collection代表独立元素的序列。Map代表“键值对”对象。 参考https://www.geeksforgeeks.org/arraylist-in-java/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数组</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式：委托和代理]]></title>
    <url>%2Fdesign-pattern%2Fdelegate-and-proxy.html</url>
    <content type="text"><![CDATA[概要设计模式：委托和代理 博客原帖收藏于IT老兵博客。 正文在iOS开发中，会遇到委托这种设计模式，而之前阅读四人帮的《设计模式》，只提到有代理（proxy），感觉上这两者非常接近，非常像，是不是就是一回事呢？ 参考这里，感觉说的又不是一回事。 直接面对客户的是代理，它并不见得真的具有需要执行的功能，它来受理客户的委托，当真有委托到来了，它在委托给具体执行的人去执行。 我觉得这么解释了，就和现实生活结合在一起了，代理是负责受理这个事情的机构，他不见得自己能处理这个事情。 参考https://coderanch.com/t/637844/engineering/Difference-Proxy-Delegate]]></content>
      <categories>
        <category>Design Pattern</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>委托</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka学习笔记--存储内部（MD版）]]></title>
    <url>%2FKafka%2Fkafka-storage-principle.html</url>
    <content type="text"><![CDATA[概要Kafka学习笔记–存储内部（MD版）。 博客原帖收藏于IT老兵驿站 前言研究一下 Kafka 的存储内部的原理，这里转发一篇文章，个人感觉这篇文章讲的非常好，所以推荐出来，同时做一下学习笔记。 当然，基于不鼓励懒人，启发式记录的原则，我只是对一些重点进行笔记记录，这样无论是将来自己看，还是现在别人看，都需要参考笔记去阅读一下原文—-这也才是笔记的意义。 正文 A Practical Introduction to Kafka Storage Internals Kafka is everywhere these days. With the advent of Microservices and distributed computing, Kafka has become a regular occurrence in every product architecture. In this article, I’ll try to explain how Kafka’s internal storage mechanism works. Since this is going to be a deep dive into Kafka’s internals, I would expect you to have some understanding about it. Although I’ve tried to keep the entry level for this article pretty low, you might not be able to understand everything if you’re not familiar with the general workings of Kafka. Proceed further with that in mind. Kafka is usually referred to as a Distributed, Replicated Messaging Queue, which is technically true but might lead to some confusion, depending on your definition of what a messaging queue is. Instead, I prefer the definition Distributed, Replicated Commit Log. This I think clearly represents what Kafka does as all of us understand how logs are written to disk. And in this case, it is the messages pushed into Kafka that are stored to disk. 这里是一些总体的概要介绍。 Kafka 通常被引用为一个分布的，有备份的消息队列，这在技术上是正确的，但是这个会带来一些困扰，取决于你对于消息队列的定义。取而代之的，作者更倾向于这样的定义，Kafka 是一个分布的，备份的提交日志（commit log，这个是一个有语义的术语）。 Kafka 自己定位自己是一个分布式的流化平台，而不是上面所写的分布式，复制的消息队列，或者分布式的，复制的提交日志。（这里这个流化平台的概念不是太容易理解。） With reference to storage in Kafka, you’ll always hear two terms, Partition and Topic. Partitions are the unit of storage in Kafka for data messages. And Topic can be thought of as being a container in which these partitions lie. With the basic stuff out of our way, let’s understand these concepts better by working with Kafka. I am going to start by creating a topic in Kafka with three partitions defined. If you want to follow along, the command looks like this for a local Kafka setup on windows. 而关于 Kafka 的存储的介绍，我们会经常听到两个术语，Partition 和 Topic。Partitions 是 Kafka用来存储数据消息的基本单元，一个 Topic 可以被认为是这些 partition 的容器。 下面，作者会带着大家来实践一把。（下面的这些命令是运行在 windows 平台上的，而想要运行在其他的平台，可以参考《快速开始》那一章节。） kafka-topics.bat --create --topic freblogg --partitions 3 --replication-factor 1 --zookeeper localhost:2181 按照上面的命令，启动一下 Kafka。 If I go to Kafka’s log directory, I see three directories created as follows. $ tree freblogg* freblogg-0 |-- 00000000000000000000.index |-- 00000000000000000000.log |-- 00000000000000000000.timeindex `-- leader-epoch-checkpoint freblogg-1 |-- 00000000000000000000.index |-- 00000000000000000000.log |-- 00000000000000000000.timeindex `-- leader-epoch-checkpoint freblogg-2 |-- 00000000000000000000.index |-- 00000000000000000000.log |-- 00000000000000000000.timeindex `-- leader-epoch-checkpoint 上面的例子创建了3个 partition，当你查看目录，会看到上文这样的目录和文件结构。 We have three directories created because we’ve given three partitions for our topic, which means that each partition gets a directory on the file system. You also see some files like index, log etc. We’ll get to them shortly. 一个 partition 对应着一个实际的目录，有几种文件，index，log 等等。别着急，下面会有讲解。这句是关键，一个 partition 对应着一个目录。 One more thing that you should be able to see from here is that in Kafka, the topic is more of a logical grouping than anything else and that the Partition is the actual unit of storage in Kafka. Let’s understand partitions in some more detail.PartitionsA partition, in theory, can be described as an immutable collection (or sequence) of messages. We can only append messages to a partition but cannot delete from it. And by “We”, I am talking about the Kafka consumer. A consumer can’t delete the messages in the topic.Now we’ll send some messages into the topic. But before that, I want you to see the sizes of files in our partition folders. 这里你可以看到，一个 topic 其实是一个逻辑上的组，而 partition 则是 Kafka 中实际的存储单元。 一个 partition，在理论上，被描述为一个不可修改的消息集合（或者序列）。我们仅仅可以给一个 partition 追加消息而不能从它中间删除。这里的“我们”，我是说作为一个Kafka消费者（consumer）。 在发送消息给这个主题之前，我们先看一下这些文件的大小。 $ ls -lh freblogg-0 total 20M - freblogg 197121 10M Aug 5 08:26 00000000000000000000.index - freblogg 197121 0 Aug 5 08:26 00000000000000000000.log - freblogg 197121 10M Aug 5 08:26 00000000000000000000.timeindex - freblogg 197121 0 Aug 5 08:26 leader-epoch-checkpoint You see the index files combined are about 20M in size while the log file is completely empty. This is the same case with freblogg-1 and freblogg-2folders. Now let us send a couple of messages and see what happens. To send the messages I’m using the console producer as follows: 2个 index 文件大概 20M，log 文件是空的，3个目录是一样的。 现在，我们发送一些消息去看看会发生什么。 1kafka-console-producer.bat --topic freblogg --broker-list localhost:9092 I have sent two messages, first a customary “hello world” and then I pressed the Enter key, which becomes the second message. Now if I print the sizes again: 笔者发送了两个消息，第一个是“hello world”，第二个是一个空消息。 $ ls -lh freblogg* freblogg-0: total 20M - freblogg 197121 10M Aug 5 08:26 00000000000000000000.index - freblogg 197121 0 Aug 5 08:26 00000000000000000000.log - freblogg 197121 10M Aug 5 08:26 00000000000000000000.timeindex - freblogg 197121 0 Aug 5 08:26 leader-epoch-checkpoint freblogg-1: total 21M - freblogg 197121 10M Aug 5 08:26 00000000000000000000.index - freblogg 197121 68 Aug 5 10:15 00000000000000000000.log - freblogg 197121 10M Aug 5 08:26 00000000000000000000.timeindex - freblogg 197121 11 Aug 5 10:15 leader-epoch-checkpoint freblogg-2: total 21M - freblogg 197121 10M Aug 5 08:26 00000000000000000000.index - freblogg 197121 79 Aug 5 09:59 00000000000000000000.log - freblogg 197121 10M Aug 5 08:26 00000000000000000000.timeindex - freblogg 197121 11 Aug 5 09:59 leader-epoch-checkpoint You will see that the log files have a non zero size now. This is because the messages in the partition are stored in the ‘xxxx.log’ file. To confirm that the messages are indeed stored in the log file, we can just see what’s inside that log file. 可以看到两个文件发生了变化，都是 log 文件，这是因为partition的消息是存储在 log 文件里面的，我们来再确认一下。 $ cat freblogg-2/*.log @^@^BÂ°Â£Ã¦Ãƒ^@^K^XÃ¿Ã¿Ã¿Ã¿Ã¿Ã¿^@^@^@^A&quot;^@^@^A^VHello World^@ The file format of the ‘log’ file is not one that is conducive for textual representation but nevertheless, you should see the ‘Hello World’ at the end indicating that this file got updated when we have sent the message into the topic.Notice that the first message we sent, went into the third partition (freblogg-2) and the second message went into the second partition. This is because Kafka arbitrarily picks the partition for the first message and then distributes the messages to partitions in a round robin fashion. If a third message comes now, it would go into freblogg-0 and this order of partition continues for any new message that comes in. We can also make Kafka always choose the partition for our messages by adding a key to the message. Kafka stores all the messages with the same key into a single partition.Each new message in the partition gets an ID which is one more than the previous Id number. This Id number is also called as the Offset. So, the first message is at ‘offset’ 0, the second message is at offset 1 and so on. These offset Id’s are always incremented from the previous value. 我们可以看到这个 log 文件的尾部有 “Hello World”，这说明这个消息被发送到了这个 topic 中。 注意，第一个消息发到了第三个 partition，第二个消息发送到了第二个 partition。这是因为Kafka 是随机地选取了一个 partition 来发送第一个消息，然后使用一个 round robin 的方式选择第二个 partition 来发送第二个消息，如果有第三个消息，它就会发送到第一个 partition 里面。我们也可以选择 partition 来发送，通过增加一个 key 给消息，Kafka 的一个 partition 是使用同一个 key 的。 这里讲的在每个 partition 里面，有一个 sequence Id，也就是 offset，每个消息一个，按1来步进增加。 &lt;Quick detour&gt;We can understand those random characters in the log file, using a Kafka tool. Those extra characters might not seem useful to us, but they are useful for Kafka as they are the metadata for each message in the queue. If I run, 1kafka-run-class.bat kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files logs\freblogg-2\00000000000000000000.log This gives the output Dumping logs\freblogg-2\00000000000000000000.log Starting offset: 0 offset: 0 position: 0 CreateTime: 1533443377944 isvalid: true keysize: -1 valuesize: 11 producerId: -1 headerKeys: [] payload: Hello World offset: 1 position: 79 CreateTime: 1533462689974 isvalid: true keysize: -1 valuesize: 6 producerId: -1 headerKeys: [] payload: amazon (I’ve removed a couple of things from this output which are not necessary for this discussion.) 我们用 Kafka 的工具来输出一下log里面的内容，这些是一些元数据。 You can see that it stores information of the offset, time of creation, key and value sizes etc along with the actual message payload in the log file.It is also important to note that a partition is tied to a broker. In other words, If we have three brokers and if the folder freblogg-0 exists on broker-1, you can be sure that it will not appear in any of the other brokers. Partitions of a topic can be spread out to multiple brokers but a partition is always present on one single Kafka broker (When the replication factor has its default value, which is 1. Replication is mentioned further below). partition 是和 broker 绑定在一起的，一个 partition 只会在一个 broker 上出现一次。这里的broker 指的是宿主服务器。 但是如何备份呢？按照之前几篇文章的介绍，每个 partition 会在每个 broker 上被备份一份。 SegmentsWe’ll finally talk about those index and log files we’ve seen in the partition directory. Partition might be the standard unit of storage in Kafka, but it is not the lowest level of abstraction provided. Each partition is sub-divided into segments. 这里提到了一个新的概念，segments，尽管 partition 被作为一个标准的存储单元，但并不是被提供的最低级别的抽象。每一个 partition 又被划分为 segments。 A segment is simply a collection of messages of a partition. Instead of storing all the messages of a partition in a single file (think of the log file analogy again), Kafka splits them into chunks called segments. Doing this provides several advantages. Divide and Conquer FTW! 一个 segment是在一个 partition 里面的一个简单的消息集合。取代把一个 partition 的所有消息都放在一个单一文件里面的方案是，Kafka 把它们分成被称为 segment 的大块。这是一种分而治之的方法。 Most importantly, it makes purging data easy. As previously introduced partition is immutable from a consumer perspective. But Kafka can still remove the messages based on the “Retention policy” of the topic. Deleting segments is much simpler than deleting things from a single file, especially when a producer might be pushing data into it. 很重要的是，这使得清理数据非常容易。Kafka 根据 topic 上的 “Retention policy”（保留策略）来移除消息。这样，删除一个 segment 会比从一个文件中删除东西要容易的多，尤其在一个producer 可能要往里面放数据的时候。 123456$ ls -lh freblogg-0total 20M- freblogg 197121 10M Aug 5 08:26 00000000000000000000.index- freblogg 197121 0 Aug 5 08:26 00000000000000000000.log- freblogg 197121 10M Aug 5 08:26 00000000000000000000.timeindex- freblogg 197121 0 Aug 5 08:26 leader-epoch-checkpoint The 00000000000000000000 in front of the log and the index files in each partition folder, is the name of our segment. Each segment file has segment.log, segment.index and segment.timeindex files. 在每个 index 文件前面的00000000000000000000，是 segment 的名字，每个 segment 由segment.log，segment.index 和 segment.timeindex 构成。 Kafka always writes the messages into these segment files under a partition. There is always an active segment to which Kafka writes to. Once the segment’s size limit is reached, a new segment file is created and that becomes the newly active segment. 会有一个活跃的 segment 来让Kafka去写入。一旦segment的大小限制达到了，一个新的segment文件会被创建，并且成为新的活跃的 segment。 Each segment file is created with the offset of the first message as its file name. So, In the above picture, segment 0 has messages from offset 0 to offset 2, segment 3 has messages from offset 3 to 5 and so on. Segment 6 which is the last segment is the active segment. 每个 segment 被使用第一个消息的 offset 来作为它的文件名来创建，（可以看上面的图，我第一次看到这个图，还对这个segment的名字感到奇怪）。 123456789101112131415161718192021$ ls -lh freblogg*freblogg-0:total 20M- freblogg 197121 10M Aug 5 08:26 00000000000000000000.index- freblogg 197121 0 Aug 5 08:26 00000000000000000000.log- freblogg 197121 10M Aug 5 08:26 00000000000000000000.timeindex- freblogg 197121 0 Aug 5 08:26 leader-epoch-checkpointfreblogg-1:total 21M- freblogg 197121 10M Aug 5 08:26 00000000000000000000.index- freblogg 197121 68 Aug 5 10:15 00000000000000000000.log- freblogg 197121 10M Aug 5 08:26 00000000000000000000.timeindex- freblogg 197121 11 Aug 5 10:15 leader-epoch-checkpointfreblogg-2:total 21M- freblogg 197121 10M Aug 5 08:26 00000000000000000000.index- freblogg 197121 79 Aug 5 09:59 00000000000000000000.log- freblogg 197121 10M Aug 5 08:26 00000000000000000000.timeindex- freblogg 197121 11 Aug 5 09:59 leader-epoch-checkpoint In our case, we only had one segment in each of our partitions which is 00000000000000000000. Since we don’t see another segment file present, it means that 00000000000000000000 is the active segment in each of those partitions.The default value for segment size is a high value (1 GB) but let’s say we’ve tweaked Kafka configuration so that each segment can hold only three messages. Let’s see how that would play out. 每个 segment 的默认大小是1GB，但是让我们调整一下 Kafka 的配置，让每个 segment 只能保有3条消息。 Say this is the current state of the freblogg-2 partition. We’ve three messages pushed into it.Since ‘three messages’ is the limit we’ve set, If a new message comes into this partition, Kafka will automatically close the current segment, create a new segment, make that the active segment and store that new message in the new segment’s log file. 因为我们设置了3条消息的显示，所以如果有新的消息到来，Kafka 将会自动关闭当前的segment，创建一个新的 segment，让它成为活跃的 segment，并且在新的 segment 里面存储新的消息。 (I’m not showing the preceding zeroes to make it easy on the eyes) 12345678freblogg-2|-- 00.index|-- 00.log|-- 00.timeindex|-- 03.index|-- 03.log|-- 03.timeindex`-- You should’ve noted that the name of the newer segment is not 01. Instead, you see 03.index, 03.log. So, what is going on? 看到了吧，新的 segment 出现了，名字是03。 This is because Kafka makes the lowest offset in the segment as its name. Since the new message that came into the partition has the offset 3, that is the name Kafka gives for the new segment. It also means that since we have 00 and 03 as our segments, we can be sure that the messages with offsets 0,1 and 2 are indeed present in the 00 segment. New messages coming into freblogg-2 partition with offsets 3,4 and 5 will be stored in the segment 03.One of the common operations in Kafka is to read the message at a particular offset. For this, if it has to go to the log file to find the offset, it becomes an expensive task especially because the log file can grow to huge sizes (Default — 1G). This is where the .index file becomes useful. Index file stores the offsets and physical position of the message in the log file. 因为 segment 可以达到1G，那么查找起来，有一个索引会快很多，Index 文件存储着 offsets 和消息在 log 文件上的物理位置。 An index file for the log file I’ve showed in the ‘Quick detour’ above would look something like this: 而 index 文件则会显示类似上面的内容，在1G的空间内，有一个索引来指定位置，这样查找起来会非常的块。 If you need to read the message at offset 1, you first search for it in the index file and figure out that the message is in position 79. Then you directly go to position 79 in the log file and start reading. This makes it quite effective as you can use binary search to quickly get to the correct offset in the already sorted index file.Parallelism with PartitionsTo guarantee the order of reading messages from a partition, Kafka restricts to having only consumer (from a consumer group) per partition. So, if a partition gets messages a,f and k, the consumer will also read them in the order a,f and k. This is an important thing to make a note of as order of message consumption is not guaranteed at a topic level when you have multiple partitions. 消息的顺序仅在 partition 层面被保证，在 topic 层面是不被保证的。 Just increasing the number of consumers won’t increase the parallelism. You need to scale your partitions accordingly. To read data from a topic in parallel with two consumers, you create two partitions so that each consumer can read from its own partition. Also since partitions of a topic can be on different brokers, two consumers of a topic can read the data from two different brokers. 仅仅增加 consumer 的数量并不能增加并行能力，增加 partition 才可以，不同的 partition 有可能位于不同的 broker上，这样增加了数据的读的能力。 TopicsWe’ve finally come to what a topic is. We’ve covered a lot of things about topics already. The most important thing to know is that a Topic is merely a logical grouping of several partitions. 一个 Topic 只不过是一些 partition 的一个逻辑上的组。 A topic can be distributed across multiple brokers. This is done using the partitions. But a partition still needs to be on a single broker. Each topic will have its unique name and the partitions will be named from that.ReplicationLet’s talk about replication. Whenever we’re creating a topic in Kafka, we need to specify what the replication factor we need for that topic. Let’s say we’ve two brokers and so we’ve given the replication-factor as 2. What this means is that Kafka will try to always ensure that each partition of this topic has a backup/replica. The way Kafka distributes the partitions is quite similar to how HDFS distributes its data blocks across nodes.Say for the freblogg topic that we’ve been using so far, we’ve given the replication factor as 2. The resulting distribution of its three partitions will look something like this. Even when you have a replicated partition on a different broker, Kafka wouldn’t let you read from it because in each replicated set of partitions, there is a LEADER and the rest of them are just mere FOLLOWERS serving as backup. The followers keep on syncing the data from the leader partition periodically, waiting for their chance to shine. When the leader goes down, one of the in-sync follower partitions is chosen as the new leader and now you can consume data from this partition. 你永远是从作为 LEADER 的 partition 中去读数据，其余的 FOLLOWERS 仅仅作为备份。”The followers keep on syncing the data from the leader partition periodically, waiting for their chance to shine. “这句话挺有意思，这些 follower 周期性地保持着对 leader 的 partition 的数据同步，等待着它们能够大放光芒的一天。 A Leader and a Follower of a single partition are never in a single broker. It should be quite obvious why that is so.Finally, this long article ends. Congratulations on making this far. You now know most of what there is to know about Kafka’s data storage. To ensure that you retain this information let’s do a quick recap.RecapData in Kafka is stored in topicsTopics are partitionedEach partition is further divided into segmentsEach segment has a log file to store the actual message and an index file to store the position of the messages in the log fileVarious partitions of a topic can be on different brokers but a partition is always tied to a single brokerReplicated partitions are passive. You can consume messages from them only when the leader is downThat ought to cover everything we’ve talked about. Thanks for reading. See you again in the next one.Attribution:Kafka image — https://kafka.apache.org/images/kafka_diagram.png 总结这篇文章写得很好，非常清楚地总体地讲明白了Kafka的存储，我初步翻译了一下，总体英文难度不高，其余的内容很容易看懂。 补充一点，在partition上面的目录结构，参考这里： 参考https://medium.com/@durgaswaroop/a-practical-introduction-to-kafka-storage-internals-d5b544f6925f https://stackoverflow.com/questions/27731558/where-kafka-stores-partitions-for-the-topics 修改历史2019-11-13，修改了一下格式，每个英文单词前后加了空格，移除了一些原本的疑问，修正了几处错误，之前的图片链接都失效了，只好重新下载，再上传上去，学习就是辛苦的，苦中作乐吧。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 的工作流总结]]></title>
    <url>%2Fgit%2Fgit-gitlab-flow.html</url>
    <content type="text"><![CDATA[概要Git 的工作流总结。 博客博客地址：IT老兵驿站。 前言原本这篇笔记的命名有问题，起成了GitLab工作流总结，其实现在仔细想，应该是Git工作流总结。 这里参考了阮一峰的文章，也参考了GitLab的介绍，阮一峰的文章其实是后面这篇的一个简化版。 本篇笔记主要针对这两篇文章进行学习和总结。 正文Git工作流：特点：主要分支有：develop分支：开发主分支。master分支：线上分支。feature分支：功能开发分支，开发完需要合并回develop分支。release分支：用于测试的发布分支。hotfix分支：对线上问题进行热修复的分支。 优缺点：优点：各个分支很清楚，每个分支的功用都划分的非常清楚。缺点：分支太多，维护起来难度就会比较大。 GitHub工作流：特点：主要分支有：master分支：feature分支：每次开发新开辟一个feature分支，开发完merge会master分支。 优缺点：分支很简单，不过有的场景不适合。例如开发完，不能立刻发布的一些场景，那么这个时候，代码合并回master，master对应的就不再是已经部署的线上版本了。 GitLab工作流：特点：根据不同情况，分成了两种情况。 一种是有三个分支，master、preproduction、production，从上游到下游进行合并。一种是不同的版本发布环境下，各开一个分支，也是由上游向下游合并。这里面有一个原则，就是永远是从上游向下游去merge，或者去cherry-pick。 这个地方，我是这里去理解。Git的标准流程分支太多，merge的方向也太多，GitLab约束了方向，只从上游向下游merge。主开发分支可以领先很多，不断持续化发布出来备选发布的版本，可以发布到生产环境。 参考https://docs.gitlab.com/ee/workflow/gitlab_flow.htmlhttp://www.ruanyifeng.com/blog/2015/12/git-workflow.html]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>工作流</tag>
        <tag>Workflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 的 RequestBody 和 ResponseBody 注解]]></title>
    <url>%2Fjava%2Fjava-annotation-request-response.html</url>
    <content type="text"><![CDATA[概要Spring 的 RequestBody 和 ResponseBody 注解。 博客博客地址：IT老兵驿站。 正文关于这个问题，这里的解释非常清楚。 再参考一下代码里面的注释：123456789101112131415/** * Annotation indicating a method parameter should be bound to the body of the web request. * The body of the request is passed through an &#123;@link HttpMessageConverter&#125; to resolve the * method argument depending on the content type of the request. Optionally, automatic * validation can be applied by annotating the argument with &#123;@code @Valid&#125;. * * &lt;p&gt;Supported for annotated handler methods in Servlet environments. * * @author Arjen Poutsma * @since 3.0 * @see RequestHeader * @see ResponseBody * @see org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter * @see org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter */ 概括一下：@RequestBody是把一个HttpRequest对象转换成一个DTO或者一个DO，这个反序列化地过程是自动完成的，而这个注解则是这个动作的开关。现在更多的是把一个json字符串转换成一个定义好映射关系的对象。 这里会有一个默认的HttpMessageConverter，来进行转换的工作。 @ResponseBody则是反方向进行操作，把一个自定义的对象序列化成一个json字符串。 参考https://www.baeldung.com/spring-request-response-bodyhttps://stackoverflow.com/questions/40247556/spring-boot-automatic-json-to-object-at-controller]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>RequestBody</tag>
        <tag>ResponseBody</tag>
        <tag>注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python函数的参数]]></title>
    <url>%2Fpython%2Fpython-args-kwargs.html</url>
    <content type="text"><![CDATA[概要Git：Python函数的参数。 博客博客地址：IT老兵驿站。 前言工作中遇到一个问题，python的函数是如何接收参数的呢？会判断参数个数吗？会判断参数的顺序吗？ 正文阅读了Python的官网对于args和kwargs的介绍，得到了下面的结论： python的函数不会预先判断参数的个数，你送几个，它接手几个。 如果没有用kwargs（字典）的方式来送，那么python就按照顺序来接收，没有关键字。 如果使用了kwargs（字典）的方式来送，那么就按照字典的方式来取。 参考http://book.pythontips.com/en/latest/args_and_kwargs.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>参数</tag>
        <tag>args</tag>
        <tag>kwargs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的值传递]]></title>
    <url>%2Fjava%2Fjava-value-pass.html</url>
    <content type="text"><![CDATA[概要Java的值传递。 博客原帖收藏于IT老兵博客。 正文复习《C++ Primer》第三版第89页，复习了一遍C++的引用。 找到了以前总结的笔记： C++的引用是怎么样的？ “引用是C++的一个特征,它就像能自动被编译器逆向引用的常量型指针一样。 ” “使用引用时有一定的规则: 1) 当引用被创建时,它必须被初始化。(指针则可以在任何时候被初始化。) 2) 一旦一个引用被初始化为指向一个对象,它就不能被改变为对另一个对象的引用。(指 针则可以在任何时候指向另一个对象。) 3) 不可能有NULL引用。必须确保引用是和一块合法的存储单元关连。” --《C++编程思想》 再来理解Java的引用，在《Java编程思想》的第二章，有涉及到这个地方的知识，这里说Java对于对象的传递是通过引用。 https://stackoverflow.com/questions/4712798/how-to-use-references-in-java， 这里讲了原始类型和对象的传递方式。 总结其实Java都是通过值传递，对于原始的类型，是值传递；对于对象，是把引用的值通过值传递传给新的变量，这个变量里面保存的是原来引用所指向的对象的地址，这个时候修改了这个值，对原本的引用不会产生影响。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>值传递</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git index的概念]]></title>
    <url>%2Fgit%2Fgit-index.html</url>
    <content type="text"><![CDATA[博客博客地址：IT老兵驿站。 概要Git：git index的概念。 正文 摘来的这张图描述的非常清楚，就无需再赘述了。 参看参考里面，这个哥们讲的非常清楚，还有链接，介绍他所使用的工作流。 这是一个非常重要的概念，也梳理了好几次，总是没有记清楚，这个不应该。为什么呢？记忆力有些衰退了？ 参考https://stackoverflow.com/questions/3689838/whats-the-difference-between-head-working-tree-and-index-in-git]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git index</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring的@configuration和@Bean]]></title>
    <url>%2Fjava%2Fjava-spring-configuration-bean.html</url>
    <content type="text"><![CDATA[概要Spring的@configuration和@Bean。 博客原帖位于IT老兵博客。 正文总结整理一下spring的@configuration和@Bean。 参考这里，这个网站有不少挺好的对于技术的解释性文章。 Annotating a class with the @Configuration indicates that the class can be used by the Spring IoC container as a source of bean definitions. @Configuration意味着这个被注解的类可以被Spring IoC container（Spring IoC 容器）作为一个bean定义的来源，就是在启动时，让容器去扫描这里，加载底下的对象。 The @Bean annotation tells Spring that a method annotated with @Bean will return an object that should be registered as a bean in the Spring application context. @Bean意味着被它注解的方法将会返回一个对象，可以被作为bean注册在Spring application的context中。 参考https://www.tutorialspoint.com/spring/spring_java_based_configuration.htm]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>SprintMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解一下@RestControlle的作用]]></title>
    <url>%2Fjava%2Fjava-spring-restcontroller-note.html</url>
    <content type="text"><![CDATA[理解一下@RestControlle的作用。 原文 文章收藏于IT老兵博客。 正文 理解一下@RestControlle的作用。 This code uses Spring 4’s new @RestController annotation, which marks the class as a controller where every method returns a domain object instead of a view. It’s shorthand for @Controller and @ResponseBody rolled together. 上文摘自官网。 由上面可见，@RestController=@Controller+@ResponseBody，下一步，理解@Controller。 Classic controllers can be annotated with the @Controller annotation. This is simply a specialization of the @Component class and allows implementation classes to be autodetected through the classpath scanning. @Controller is typically used in combination with a @RequestMapping annotation used on request handling methods. 摘自这里，可以看出以下几点： @Controller 是一种特殊化的@Component 类。 @Controller 习惯于和@RequestMapping绑定来使用，后者是用来指定路由映射的。 下一步，理解@ResponseBody。 The request handling method is annotated with @ResponseBody. This annotation enables automatic serialization of the return object into the HttpResponse. 摘自同样的地方，这里可以看出： @ResponseBody 是用来把返回对象自动序列化成HttpResponse的。 再参考这里： 3. @ResponseBody The @ResponseBody annotation tells a controller that the object returned is automatically serialized into JSON and passed back into the HttpResponse object. Suppose we have a custom Response object: 1 2 3 4 5 public class ResponseTransfer { private String text; // standard getters/setters } Next, the associated controller can be implemented: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Controller @RequestMapping("/post") public class ExamplePostController { @Autowired ExampleService exampleService; @PostMapping("/response") @ResponseBody public ResponseTransfer postResponseController( @RequestBody LoginForm loginForm) { return new ResponseTransfer("Thanks For Posting!!!"); } } In the developer console of our browser or using a tool like Postman, we can see the following response: 1 {"text":"Thanks For Posting!!!"} Remember, we don’t need to annotate the @RestController-annotated controllers with the @ResponseBody annotation since it’s done by default here. 从上面可以看出： @ResponseBody告诉控制器返回对象会被自动序列化成JSON，并且传回HttpResponse这个对象。 再补充一下@RequestBody： Simply put, the @RequestBody annotation maps the HttpRequest body to a transfer or domain object, enabling automatic deserialization of the inbound HttpRequest body onto a Java object. First, let’s have a look at a Spring controller method: 1 2 3 4 5 6 7 @PostMapping("/request") public ResponseEntity postController( @RequestBody LoginForm loginForm) { exampleService.fakeAuthenticate(loginForm); return ResponseEntity.ok(HttpStatus.OK); } Spring automatically deserializes the JSON into a Java type assuming an appropriate one is specified. By default, the type we annotate with the @RequestBody annotation must correspond to the JSON sent from our client-side controller: 1 2 3 4 5 public class LoginForm { private String username; private String password; // ... } Here, the object we use to represent the HttpRequest body maps to our LoginForm object. Let’s test this using CURL: 1 2 3 4 5 curl -i \ -H "Accept: application/json" \ -H "Content-Type:application/json" \ -X POST --data '{"username": "johnny", "password": "password"}' "https://localhost:8080/.../request" This is all that is needed for a Spring REST API and an Angular client using the @RequestBody annotation! @RequestBody把HttpRequest body映射成一个 transfer or domain object（DTO或者DO），把一个入境（inbound）的HttpRequest的body反序列化成一个Java对象。 参考 https://spring.io/guides/gs/rest-service/ https://www.baeldung.com/spring-controller-vs-restcontroller https://www.baeldung.com/spring-request-response-body]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>RestControlle</tag>
        <tag>annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的进程查看工具jps学习笔记]]></title>
    <url>%2Fjava%2Fjava-jps-note.html</url>
    <content type="text"><![CDATA[Java：Java的进程查看工具jps学习笔记。 收藏原帖位于IT老兵博客。 前言研究Java已经有两年多，居然不知道还有一个命令叫JPS，可以查看Java的进程，真是惭愧，从官网找到讲解，开始研究。 正文jps - Java Virtual Machine Process Status Tooljps - Java虚拟机进程状态工具 Synopsis Parameters Description Options Host Identifier Output Format Examples See Also SYNOPSIS**jps** [ *options* ] [ *hostid* ] PARAMETERSoptions Command-line options. hostid The host identifier of the host for which the process report should be generated. The hostid may include optional components that indicate the communications protocol, port number, and other implementation specific data. 这两个参数下面有详细的介绍，在这个地方，我第一遍也没有看懂。 DESCRIPTIONThe jps tool lists the instrumented HotSpot Java Virtual Machines (JVMs) on the target system. The tool is limited to reporting information on JVMs for which it has the access permissions. 这句话，谷歌是这样翻译的： jps 工具列出了目标系统上的已检测HotSpot Java虚拟机（JVM）。这里我对instrumented这个词在这里怎么理解有些疑惑，已经装配的？ If jps is run without specifying a hostid, it will look for instrumented JVMs on the local host. If started with a hostid, it will look for JVMs on the indicated host, using the specified protocol and port. A jstatd process is assumed to be running on the target host. 如果指定了hostid，它将会在指明的host上寻找JVM，使用指定的协议和端口。一个jstatd 被假定运行在目标host之上–这个是前提。 The jps command will report the local VM identifier, or lvmid, for each instrumented JVM found on the target system. The lvmid is typically, but not necessarily, the operating system’s process identifier for the JVM process. With no options, jps will list each Java application’s lvmid followed by the short form of the application’s class name or jar file name. The short form of the class name or JAR file name omits the class’s package information or the JAR files path information. lvmid是操作系统上的JVM进程标识符。如果不输入别的options，jps只会列出这个lvmid和一个Java应用程序的类名或者jar文件名的简短的形式（short form）。 The jps command uses the java launcher to find the class name and arguments passed to the main method. If the target JVM is started with a custom launcher, the class name (or JAR file name) and the arguments to the main method will not be available. In this case, the jps command will output the string Unknown for the class name or JAR file name and for the arguments to the main method. jps命令会使用java launcher（启动器，怎么理解？）去寻找传递给main方法的类名和参数。如果目标JVM是使用自定义的launcher，那么就得不到。那样的话，会输出一个Unknown 。 The list of JVMs produced by the jps command may be limited by the permissions granted to the principal running the command. The command will only list the JVMs for which the principle has access rights as determined by operating system specific access control mechanisms. NOTE: This utility is unsupported and may not be available in future versions of the JDK. It is not currently available on Windows 98 and Windows ME platforms. OPTIONSThe jps command supports a number of options that modify the output of the command. These options are subject to change or removal in the future. -qSuppress the output of the class name, JAR file name, and arguments passed to the main method, producing only a list of local VM identifiers.抑制类名，JAR文件名和传递给main方法的参数。 -mOutput the arguments passed to the main method. The output may be null for embedded JVMs.输出传递给main方法的参数。 -lOutput the full package name for the application’s main class or the full path name to the application’s JAR file. -vOutput the arguments passed to the JVM. -VOutput the arguments passed to the JVM through the flags file (the .hotspotrc file or the file specified by the -XX:Flags=&lt;filename&gt; argument). -J optionPass option to the java launcher called by jps. For example, -J-Xms48m sets the startup memory to 48 megabytes. It is a common convention for -J to pass options to the underlying VM executing applications written in Java. HOST IDENTIFIERThe host identifier, or hostid is a string that indicates the target system. The syntax of the hostid string largely corresponds to the syntax of a URI: [*protocol*:][[//]*hostname*][:*port*][/*servername*] protocolThe communications protocol. If the protocol is omitted and a hostname is not specified, the default protocol is a platform specific, optimized, local protocol. If the protocol is omitted and a hostname is specified, then the default protocol is rmi. hostnameA hostname or IP address indicating the target host. If hostname is omitted, then the target host is the local host. portThe default port for communicating with the remote server. If the hostname is omitted or the protocol specifies an optimized, local protocol, then port is ignored. Otherwise, treatment of the port parameter is implementation specific. For the default rmi protocol the port indicates the port number for the rmiregistry on the remote host. If port is omitted, and protocol indicates rmi, then the default rmiregistry port (1099) is used. servernameThe treatment of this parameter depends on the implementation. For the optimized, local protocol, this field is ignored. For the rmi protocol, this parameter is a string representing the name of the RMI remote object on the remote host. See the -n option for the jstatd command. OUTPUT FORMATThe output of the jps command follows the following pattern: *lvmid* [ [ *classname* | *JARfilename* | &quot;Unknown&quot;] [ *arg** ] [ *jvmarg** ] ] Where all output tokens are separated by white space. An arg that includes embedded white space will introduce ambiguity when attempting to map arguments to their actual positional parameters. NOTE: You are advised not to write scripts to parse jps output since the format may change in future releases. If you choose to write scripts that parse jps output, expect to modify them for future releases of this tool. EXAMPLESThis section provides examples of the jps command. Listing the instrumented JVMs on the local host: **jps** 18027 Java2Demo.JAR 18032 jps 18005 jstat Listing the instrumented JVMs on a remote host: This example assumes that the jstat server and either the its internal RMI registry or a separate external rmiregistry process are running on the remote host on the default port (port 1099). It also assumes that the local host has appropriate permissions to access the remote host. This example also includes the -l option to output the long form of the class names or JAR file names. **jps -l remote.domain** 3002 /opt/jdk1.7.0/demo/jfc/Java2D/Java2Demo.JAR 2857 sun.tools.jstatd.jstatd Listing the instrumented JVMs on a remote host with a non-default port for the RMI registry This example assumes that the jstatd server, with an internal RMI registry bound to port 2002, is running on the remote host. This example also uses the -m option to include the arguments passed to the main method of each of the listed Java applications. **jps -m remote.domain:2002** 3002 /opt/jdk1.7.0/demo/jfc/Java2D/Java2Demo.JAR 3102 sun.tools.jstatd.jstatd -p 2002 SEE ALSO java - the Java Application Launcher jstat - the Java virtual machine Statistics Monitoring Tool jstatd - the jstat daemon rmiregistry - the Java Remote Object Registry 总结“眼过千遍，不如手写一遍”。花费几十分钟，记录一下，顿时感觉基本都搞明白了。 参考https://docs.oracle.com/javase/7/docs/technotes/tools/share/jps.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>进程</tag>
        <tag>jps</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的POJO和Beans]]></title>
    <url>%2Fjava%2Fjava-pojo-bean-note.html</url>
    <content type="text"><![CDATA[Java：学习Java的POJO和Beans。 收藏原帖位于IT老兵博客，沉淀着一个IT老兵对于这个行业的认知。 前言想总结一下POJO和Beans，发现这个工作有人已经做了，认真地阅读，然后转帖下来，记录笔记。 正文POJO vs Java BeansPOJO classes POJO stands for Plain Old Java Object. It is an ordinary Java object, not bound by any special restriction other than those forced by the Java Language Specification and not requiring any class path. POJOs are used for increasing the readability and re-usability of a program. POJOs have gained most acceptance because they are easy to write and understand. They were introduced in EJB 3.0 by Sun microsystems. POJO代表着Plain Old Java Object，简单的旧的Java对象。它是一个常规的Java对象，除了被Java语言规范之外不被任何指定的限制所约束，并且不需要任何class path。POJOs被用于增加一个程序的可读性和可重用性。 A POJO should not: Extend prespecified classes, Ex: public class GFG extends javax.servlet.http.HttpServlet { … } is not a POJO class. Implement prespecified interfaces, Ex: public class Bar implements javax.ejb.EntityBean { … } is not a POJO class. Contain prespecified annotations, Ex: @javax.persistence.Entity public class Baz { … } is not a POJO class. 一个POJO不应该： 继承别的预先指定的类。 实现预先指定的接口。 包含预先指定的注解。 POJOs basically defines an entity. Like in you program, if you want a Employee class then you can create a POJO as follows: // Employee POJO class to represent entity Employee public class Employee { // default field String name; // public field public String id; // private salary private double salary; //arg-constructor to initialize fields public Employee(String name, String id, double salary) { this.name = name; this.id = id; this.salary = salary; } // getter method for name public String getName() { return name; } // getter method for id public String getId() { return id; } // getter method for salary public Double getSalary() { return salary; } } The above example is a well defined example of POJO class. As you can see, there is no restriction on access-modifier of fields. They can be private, default, protected or public. It is also not necessary to include any constructor in it. 对于域的访问和修改没有限制。 POJO is an object which encapsulates Business Logic. Following image shows a working example of POJO class. Controllers get interact with your business logic which in turn interact with POJO to access the database. In this example a database entity is represented by POJO. This POJO has the same members as database entity. POJO是一个封装了业务逻辑的对象。控制器和你的商业逻辑打交道，然后要通过和POJO打交道去访问数据库。在这个例子里面，一个数据库实例被一个POJO锁代表。这个POJO和数据库实体拥有同样的成员。这里其实就是ORM的概念了。 Java Beans Beans are special type of Pojos. There are some restrictions on POJO to be a bean. All JavaBeans are POJOs but not all POJOs are JavaBeans. Serializable i.e. they should implement Serializable interface. Still some POJOs who don’t implement Serializable interface are called POJOs beacause Serializable is a marker interface and therefore not of much burden. Fields should be private. This is to provide the complete control on fields. Fields should have getters or setters or both. A no-arg constructor should be there in a bean. Fields are accessed only by constructor or getter setters. Bean是一种特殊类型的POJO。要成为一个bean，有一些对于POJO的限制。 所有JavaBeans都是POJO，但不是所有的POJO是JavaBeans。 有些POJOs不实现Serializable接口，它们仍然被称作POJO，这里和上面的POJO的特征有点冲突啊。这里说Serializable interface是一个marker的interface，什么意思，标记型的interface？ 域可以是私有的。 域需要提供getter或者setter方法，或者都要。 需要一个无参数的构造函数。 域只能被构造函数或者getter和setter方法访问。 Getters and Setters have some special names depending on field name. For example, if field name is someProperty then its getter preferably will be: public void getSomeProperty() { return someProperty; } and setter will be public void setSomePRoperty(someProperty) { this.someProperty=someProperty; } Visibility of getters and setters in generally public. Getters and setters provide the complete restriction on fields. e.g. consider below property, Integer age; If you set visibility of age to public, then any object can use this. Suppose you want that age can’t be 0. In that case you can’t have control. Any object can set it 0. But by using setter method, you have control. You can have a condition in your setter method. Similarly, for getter method if you want that if your age is 0 then it should return null, you can achieve this by using getter method as in following example: // Java program to illustrate JavaBeans class Bean { // private field property private Integer property; Bean() { // No-arg constructor } // setter method for property public void setProperty(Integer property) { if (property == 0) { // if property is 0 return return; } this.property=property; } // getter method for property public int getProperty() { if (property == 0) { // if property is 0 return null return null; } return property; } } // Class to test above bean public class GFG { public static void main(String[] args) { Bean bean = new Bean(); bean.setProperty(0); System.out.println(&quot;After setting to 0: &quot; + bean.getProperty()); bean.setProperty(5); System.out.println(&quot;After setting to valid&quot; + &quot; value: &quot; + bean.getProperty()); } } Output: After setting to 0: null After setting to valid value: 5 POJO vs Java Bean POJOJAVA BEANIt doesn’t have special restrictions other than those forced by Java language.It is a special POJO which have some restrictions.It doesn’t provide much control on members.It provides complete control on members.It can implement Serializable interface.It should implement serializable interface.Fields can be accessed by their names.Fields are accessed only by getters and setters.Fields can have any visibility.Fields have only private visibility.There can be a no-arg constructor.It must have a no-arg constructor.It is used when you don’t want to give restriction on your members and give user complete access of your entityIt is used when you want to provide user your entity but only some part of your entity. 总结看过了这篇文章，基本搞明白了POJO和Bean的概念。 参考https://www.geeksforgeeks.org/pojo-vs-java-beans/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>POJO</tag>
        <tag>Beans</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka的学习笔记--介绍（总体概念）]]></title>
    <url>%2FKafka%2Fkafka-introduction-note.html</url>
    <content type="text"><![CDATA[前言最近项目要用到 Kafka，要研究一下这个，先得总体对它有个了解，而个人经验，想要获得总体了解，最好的办法就是阅读维基百科，或者阅读官网的介绍，这次我来阅读官网的介绍。 我觉得，反复研读，充分理解这个介绍，可以提纲挈领，之后再配合对各个模块具体的研究，可以很快理解和掌握。 下面摘抄了正文，配合上总结的一定的段落大意，便于快速抓住要旨。 博客原帖收藏于IT老兵驿站。 正文IntroductionApache Kafka® is a distributed streaming platform. What exactly does that mean?kafka 是一个分布式的流化平台。怎么理解这个意思呢？ A streaming platform has three key capabilities: Publish and subscribe to streams of records, similar to a -message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. 一个流化平台需要有三个关键的能力： 对记录流的发布和订阅，类似一个消息队列或者企业级消息系统。 采用一种持久容错的方式来存储记录流。 当记录流产生的时候处理它。 Kafka is generally used for two broad classes of applications: Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data Kafka通常被用于两种广泛的应用类型： 建立实时流化数据管道，在系统或者应用程序中可靠地获取数据 建立实时流化应用程序，对流化数据来转换或者响应 To understand how Kafka does these things, let’s dive in and explore Kafka’s capabilities from the bottom up.First a few concepts:Kafka is run as a cluster on one or more servers that can span multiple datacenters.The Kafka cluster stores streams of records in categories called topics.Each record consists of a key, a value, and a timestamp. 一些新的概念：Kafka 作为一个集群来运行，在一台或者多台服务器上，可以跨越多个数据中心。（后面这句怎么理解？是不是是指每一个服务器上都储存着一份完整的数据）Kafka 集群存储着流化记录在被称为 topics 的分类中。每一个记录包含着一个 key，一个 value，和一个 timestamp。 Kafka has four core APIs:The Producer API allows an application to publish a stream of records to one or more Kafka topics.The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table. 4个核心API： Producer API 允许一个应用程序去发布一个记录流到一个或多个 Kafka topics 中。Consumer API 允许一个应用程序去订阅一个或多个 topics，并且处理产生给它们（前面的topics）的记录流。Streams API 允许一个应用程序去扮演一个stream processor（流化处理器），从一个或多个topics中消费一个输入流，并且产生一个输出流到一个或多个输出的 topics 中，有效转换输入流到输出流。Connector API 允许建造和运行一个可重用的生产者或者消费者来连接 Kafka topics 到一个existing 的应用程序或者数据系统。例如，一个连接器到一个关系型数据库可以捕获对一个表的每一次变化。 In Kafka the communication between the clients and the servers is done with a simple, high-performance, language agnostic TCP protocol. This protocol is versioned and maintains backwards compatibility with older version. We provide a Java client for Kafka, but clients are available in many languages. Kafka 协议是用 TCP 协议构建，与语言无关，版本化的，兼容过去老的版本。 Topics and LogsLet’s first dive into the core abstraction Kafka provides for a stream of records—the topic.A topic is a category or feed name to which records are published. Topics in Kafka are always multi-subscriber; that is, a topic can have zero, one, or many consumers that subscribe to the data written to it. 一个 topic 是一个类别或者 feed name（不知道怎么解释，应该是类似 RSS 的概念，一个可以被订阅的东西的名字），用来发布记录的。Topics 通常是多订户的，就是说，一个 topic 可以有零个，一个，或者很多消费者来订阅写在它里面的数据的。 For each topic, the Kafka cluster maintains a partitioned log that looks like this:Each partition is an ordered, immutable sequence of records that is continually appended to—a structured commit log. The records in the partitions are each assigned a sequential id number called the offset that uniquely identifies each record within the partition. 每一个 partition 是一个顺序的，不可变顺序的记录，可以被连续追加的结构化的 commit log（提交日志？）。每条在 partition 中的记录被分配一个 sequential id number， 被称为offset，是在 partition 中的每条记录唯一的标识符。 这里留有了几个问题，如何划分 partition，如何对 partition 进行读写？consumer 是针对partition来消费的？ partition 是由 Kafka 来划分的，它来使用 Round-Robin（单循环的方式）来进行写。（奇怪，这句话是哪里找来的，不是原文中所提到的，这个地方是可以，而不是一定采用 Round-Robin，还可以设置一个key，这样去做分配） The Kafka cluster durably persists all published records—whether or not they have been consumed—using a configurable retention period. For example, if the retention policy is set to two days, then for the two days after a record is published, it is available for consumption, after which it will be discarded to free up space. Kafka’s performance is effectively constant with respect to data size so storing data for a long time is not a problem. 无论记录是否被消费，Kafka 集群都会一个可配置的保持周期留存所有发布的记录。 In fact, the only metadata retained on a per-consumer basis is the offset or position of that consumer in the log. This offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads records, but, in fact, since the position is controlled by the consumer it can consume records in any order it likes. For example a consumer can reset to an older offset to reprocess data from the past or skip ahead to the most recent record and start consuming from “now”. 这段比较关键，事实上，唯一保留在每一个消费者端的元数据就是这个 offset，偏移量，这个偏移量是用户端的，即每个用户读到哪里了。 This combination of features means that Kafka consumers are very cheap—they can come and go without much impact on the cluster or on other consumers. For example, you can use our command line tools to “tail” the contents of any topic without changing what is consumed by any existing consumers. consumer 是非常便宜的–这里的意思是 consumer 可以很容易的加入或者离开，并没有对其他consumer 产生太多的影响。 The partitions in the log serve several purposes. First, they allow the log to scale beyond a size that will fit on a single server. Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data. Second they act as the unit of parallelism—more on that in a bit. partitons 有很多用途。可以使日志分布在多台服务器上，这样一个 topic 可以处理更多的数据–这里的意思是一个 topic 可以通过 partition 放在多台 server 上，从而实现 topic 数据的横向扩展？那每个 partition 又被 ZooKeeper 进行备份，那这里是一个多对多的关系了。其次，它们扮演者并行单位的角色–在某种程度上，更多要依靠这一点。 DistributionThe partitions of the log are distributed over the servers in the Kafka cluster with each server handling data and requests for a share of the partitions. Each partition is replicated across a configurable number of servers for fault tolerance.Each partition has one server which acts as the “leader” and zero or more servers which act as “followers”. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster. 这段讲了 sever 和 partition 的关系，这里说 partitions of the log，（日志的partition，这是说Kafka其实是基于日志的？是的，commit log）每一个 server 处理着数据和请求。每一个partition 会在一个可配置数目的 server 中间被复制。（那么一个 partition 会被分在多个server上吗？不会，每个partition在每个server上是完整的）server在交替扮演者 leader（领导者）和follower（跟随者）的角色。leader 为 partition 处理所有读写请求，followers 被动地复制leader。 这里存在了几个问题： 这里的 server 和 cluster 是不是指的是一回事？应该是一个 cluster 包含多个 server。 这里 partition 的一致性维护其实是由 ZooKeeper 提供的吗？ISR 的持久性是通过 ZooKeeper维护的，但是算法又没有使用 ZAB 算法。 Geo-ReplicationKafka MirrorMaker provides geo-replication support for your clusters. With MirrorMaker, messages are replicated across multiple datacenters or cloud regions. You can use this in active/passive scenarios for backup and recovery; or in active/active scenarios to place data closer to your users, or support data locality requirements. ProducersProducers publish data to the topics of their choice. The producer is responsible for choosing which record to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the record). More on the use of partitioning in a second! Producers 负责在 topic 中选择哪一条记录分配给哪一个 partition。这可以采用 round-robin（循环）方式或者其它方式。 ConsumersConsumers label themselves with a consumer group name, and each record published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines. Consumers 用一个 consumer group 的名字来表示自己，每一条发布到一个 topic 的记录被传递到（不是拖的方式吗？这里这么说感觉像是推的方式了，可能方式本身并不重要，也可能是两种方式混合在一起，为了达到目的）每一个订阅的 consumer group 的一个 consumer 实例。Consumer instances可以在单独的进程或者单独的机器上。 这里 consumer instance 和 consumer group 都分别指代什么呢？一个 group 中会有多个instance。consumer group 对应着 partition。 If all the consumer instances have the same consumer group, then the records will effectively be load balanced over the consumer instances.If all the consumer instances have different consumer groups, then each record will be broadcast to all the consumer processes. 上面两句有些含糊，consumer instance 和 consumer group 的关系，和 consumer processes的关系。 A two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.More commonly, however, we have found that topics have a small number of consumer groups, one for each “logical subscriber”. Each group is composed of many consumer instances for scalability and fault tolerance. This is nothing more than publish-subscribe semantics where the subscriber is a cluster of consumers instead of a single process. 然而，更为通用的是，我们已经发现 topics 有很小数目的 consumer groups，每一个是“logical subscriber”，逻辑上的订阅者。每一个组由很多 consumer instances 组成，为了可扩展性和容错。除了订阅者是一个 consumer 的 cluster 代替了一个单独的进程之外，这和 publish-subscribe 语法没有任何区别。 The way consumption is implemented in Kafka is by dividing up the partitions in the log over the consumer instances so that each instance is the exclusive consumer of a “fair share” of partitions at any point in time. This process of maintaining membership in the group is handled by the Kafka protocol dynamically. If new instances join the group they will take over some partitions from other members of the group; if an instance dies, its partitions will be distributed to the remaining instances. Kafka 所实现的消费，是把 log 中的 partition 分散到 consumer instance 上去，以便每一个instance 在任何一个时间点上都是一个排他的，对于 partition 的“fair share”（公平共享）的consumer。维持这个 group 内关系的过程是由Kafka协议动态控制。如果一些新的 instance 加入了 group，它们就会从 group 其它成员那里接管一些 partitions；如果一个 instance 挂掉，它的 partitions 就会分配给剩余的 instance（这里的这个概念，应该就是 rebalance）。 （这说明一个实例对应着一个 partition。） 参考这里： Having consumers as part of the same consumer group means providing the “competing consumers” pattern with whom the messages from topic partitions are spread across the members of the group. Each consumer receives messages from one or more partitions (“automatically” assigned to it) and the same messages won’t be received by the other consumers (assigned to different partitions). In this way, we can scale the number of the consumers up to the number of the partitions (having one consumer reading only one partition); in this case, a new consumer joining the group will be in an idle state without being assigned to any partition.Having consumers as part of different consumer groups means providing the “publish/subscribe” pattern where the messages from topic partitions are sent to all the consumers across the different groups. It means that inside the same consumer group, we’ll have the rules explained above, but across different groups, the consumers will receive the same messages. It’s useful when the messages inside a topic are of interest for different applications that will process them in different ways. We want all the interested applications to receive all the same messages from the topic. 一个 group 内的 consumer 互相是竞争的，一个消息被一个 consumer 消费了，group 内的别的 consumer 就不会再收到这个消息了。而 consumer 在不同的 group 中，则每个 group 都会收到这个消息，但是在一个 group 内，还是同样的原则。这样说来，group 对应着 topic。 Kafka only provides a total order over records within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. However, if you require a total order over records this can be achieved with a topic that has only one partition, though this will mean only one consumer process per consumer group. Kafka 仅在 partition 中对记录提供总的顺序，而不是在一个 topic 下不同的 partition 之间。对于大多数应用程序而言，按分区排序与按键分区数据的能力相结合就足够了。然后，如果你需要对记录进行总排序，那只能是一个 topic 只有一个 partition，这意味着一个 consumer group只有一个 consumer 进程。 Multi-tenancyYou can deploy Kafka as a multi-tenant solution. Multi-tenancy is enabled by configuring which topics can produce or consume data. There is also operations support for quotas. Administrators can define and enforce quotas on requests to control the broker resources that are used by clients. For more information, see the security documentation. 多租期，这段还有待理解。 GuaranteesAt a high-level Kafka gives the following guarantees:Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a record M1 is sent by the same producer as a record M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.A consumer instance sees records in the order they are stored in the log.For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any records committed to the log.More details on these guarantees are given in the design section of the documentation. 保证，消息是具有先后性的，最小服务数量，这是说容灾能力，如果复制因子是 N，那么至多可以瘫痪 N - 1 台server。 Kafka as a Messaging SystemHow does Kafka’s notion of streams compare to a traditional enterprise messaging system?Messaging traditionally has two models: queuing and publish-subscribe. In a queue, a pool of consumers may read from a server and each record goes to one of them; in publish-subscribe the record is broadcast to all consumers. Each of these two models has a strength and a weakness. The strength of queuing is that it allows you to divide up the processing of data over multiple consumer instances, which lets you scale your processing. Unfortunately, queues aren’t multi-subscriber—once one process reads the data it’s gone. Publish-subscribe allows you broadcast data to multiple processes, but has no way of scaling processing since every message goes to every subscriber. 传统的消息系统有两种模型：队列和发布者-订阅者，前者消息是一对一的，后者消息是一对多的，二者互有优劣。 The consumer group concept in Kafka generalizes these two concepts. As with a queue the consumer group allows you to divide up processing over a collection of processes (the members of the consumer group). As with publish-subscribe, Kafka allows you to broadcast messages to multiple consumer groups.The advantage of Kafka’s model is that every topic has both these properties—it can scale processing and is also multi-subscriber—there is no need to choose one or the other.Kafka has stronger ordering guarantees than a traditional messaging system, too.A traditional queue retains records in-order on the server, and if multiple consumers consume from the queue then the server hands out records in the order they are stored. However, although the server hands out records in order, the records are delivered asynchronously to consumers, so they may arrive out of order on different consumers. This effectively means the ordering of the records is lost in the presence of parallel consumption. Messaging systems often work around this by having a notion of “exclusive consumer” that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.Kafka does it better. By having a notion of parallelism—the partition—within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances in a consumer group than partitions. Kafka 采取了两家之长，一个消息可以支持多订户，使用了 partition，增加了并发。 Kafka as a Storage SystemAny message queue that allows publishing messages decoupled from consuming them is effectively acting as a storage system for the in-flight messages. What is different about Kafka is that it is a very good storage system.Data written to Kafka is written to disk and replicated for fault-tolerance. Kafka allows producers to wait on acknowledgement so that a write isn’t considered complete until it is fully replicated and guaranteed to persist even if the server written to fails. 这段比较重要，Kafka 允许生产者等待确认，以便在完全复制之前写入不被认为是完整的，并且即使写入的服务器失败也保证持久性–这可能是通过复制到了别的服务器上来实现的。 The disk structures Kafka uses scale well—Kafka will perform the same whether you have 50 KB or 50 TB of persistent data on the server.As a result of taking storage seriously and allowing the clients to control their read position, you can think of Kafka as a kind of special purpose distributed filesystem dedicated to high-performance, low-latency commit log storage, replication, and propagation.For details about the Kafka’s commit log storage and replication design, please read this page. Kafka for Stream ProcessingIt isn’t enough to just read, write, and store streams of data, the purpose is to enable real-time processing of streams.In Kafka a stream processor is anything that takes continual streams of data from input topics, performs some processing on this input, and produces continual streams of data to output topics.For example, a retail application might take in input streams of sales and shipments, and output a stream of reorders and price adjustments computed off this data.It is possible to do simple processing directly using the producer and consumer APIs. However for more complex transformations Kafka provides a fully integrated Streams API. This allows building applications that do non-trivial processing that compute aggregations off of streams or join streams together.This facility helps solve the hard problems this type of application faces: handling out-of-order data, reprocessing input as code changes, performing stateful computations, etc.The streams API builds on the core primitives Kafka provides: it uses the producer and consumer APIs for input, uses Kafka for stateful storage, and uses the same group mechanism for fault tolerance among the stream processor instances. 流化处理，把输入流进行处理，转成想要的输出流，这块需要实践一下，才好理解。 Putting the Pieces TogetherThis combination of messaging, storage, and stream processing may seem unusual but it is essential to Kafka’s role as a streaming platform.A distributed file system like HDFS allows storing static files for batch processing. Effectively a system like this allows storing and processing historical data from the past.A traditional enterprise messaging system allows processing future messages that will arrive after you subscribe. Applications built in this way process future data as it arrives.Kafka combines both of these capabilities, and the combination is critical both for Kafka usage as a platform for streaming applications as well as for streaming data pipelines.By combining storage and low-latency subscriptions, streaming applications can treat both past and future data the same way. That is a single application can process historical, stored data but rather than ending when it reaches the last record it can keep processing as future data arrives. This is a generalized notion of stream processing that subsumes batch processing as well as message-driven applications.Likewise for streaming data pipelines the combination of subscription to real-time events make it possible to use Kafka for very low-latency pipelines; but the ability to store data reliably make it possible to use it for critical data where the delivery of data must be guaranteed or for integration with offline systems that load data only periodically or may go down for extended periods of time for maintenance. The stream processing facilities make it possible to transform data as it arrives.For more information on the guarantees, APIs, and capabilities Kafka provides see the rest of the documentation. 总结一下，夸一夸Kafka，就不赘述了。 总结通过阅读这一篇文档，对Kafka有了一个总体的印象。 参考http://kafka.apache.org/intro]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>介绍</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java访问MongoDB：安装驱动]]></title>
    <url>%2Fmongodb%2Fmongodb-java-driver.html</url>
    <content type="text"><![CDATA[Java访问MongoDB：安装驱动。 引用原帖收藏于IT老兵驿站。 前言Spring项目中可能需要用到MongoDB，要了解一下Java项目如何连接MongoDB，查了查google，对比了一下，发现官网的讲解非常清楚，这样的话，直接阅读官网，效率是最高的。 官网的位置：http://mongodb.github.io/mongo-java-driver/3.9/driver/getting-started/quick-start/。 本文捡着最主要的地方做一下笔记，方便自己学习，也输出一下，这样输出的速度也快，也可以方便他人借鉴，但不再全文翻译，那样输出太慢，而且个人感觉，只是方便了懒人，学习计算机，不提高英文，永远是短一条腿走路，很难达到很高的水平。 InstallationThe recommended way to get started using one of the drivers in your project is with a dependency management system.There are two Maven artifacts available in the release. The preferred artifact for new applications is mongodb-driver-sync however, we still publish the legacy mongo-java-driver uber-jar as well as the mongodb-driver jar introduced in 3.0. Java的MongoDB驱动当前一共有4个库，先需要搞清楚这4个库的关系，优先推荐mongodb-driver-sync。 MongoDB Driver SyncThe MongoDB Driver mongodb-driver-sync is the synchronous Java driver containing only the generic MongoCollection interface that complies with a new cross-driver CRUD specification. It does not include the legacy API (e.g. DBCollection). 这个库只有MongoCollection接口，并且不包含废弃的API。 IMPORTANTThis is a Java 9-compliant module with an Automatic-Module-Name of org.mongodb.driver.sync.client.The mongodb-driver-sync artifact is a valid OSGi bundle whose symbolic name is org.mongodb.driver-sync. 这块有点没明白，似乎是分成了两个包，一个是兼容Java 9的org.mongodb.driver.sync.client和遵循OSGi的org.mongodb.driver-sync。 下面是Maven的配置，原帖中还有Gradle的配置方式。1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver-sync&lt;/artifactId&gt; &lt;version&gt;3.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Note: You can also download the mongodb-driver-sync jar directly from sonatype.If downloading mongodb-driver-sync manually, you must also download its dependencies: bson and mongodb-driver-core 当然，你可以可以直接从sonatype下载jar包，如果那样的话，你需要还下载一下依赖。 MongoDB Driver LegacyThe MongoDB Legacy driver mongodb-driver-legacy is the legacy synchronous Java driver whose entry point is com.mongodb.MongoClient and central classes include com.mongodb.DB, com.mongodb.DBCollection, and com.mongodb.DBCursor. 这个mongodb-driver-legacy驱动带有com.mongodb.MongoClient这个入口，核心类是com.mongodb.DB, com.mongodb.DBCollection和com.mongodb.DBCursor，很多网上的样例用的是这两个类。 IMPORTANTWhile not deprecated, we recommend that new applications depend on the mongodb-driver-syncmodule.1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver-legacy&lt;/artifactId&gt; &lt;version&gt;3.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Note: You can also download the mongodb-driver-legacy jar directly from sonatype.If downloading mongodb-driver-legacy manually, you must also download its dependencies: bson andmongodb-driver-core MongoDB DriverThe MongoDB Driver mongodb-driver is the updated synchronous Java driver that includes the legacy API as well as a new generic MongoCollection interface that complies with a new cross-driver CRUD specification. mongodb-driver是更新的同步化的Java驱动，包含旧的API，也包含新的MongoCollection接口。（这个应该是为了过渡考虑） IMPORTANTmongodb-driver is not an OSGi bundle: both mongodb-driver and mongodb-driver-core, a dependency of mongodb-driver, include classes from the com.mongodb package.For OSGi-based applications, use the mongodb-driver-sync or the mongo-java-driver uber jar instead.It is also not a Java 9 module.This module is deprecated and will no longer be published in the next major release of the driver (4.0).1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver&lt;/artifactId&gt; &lt;version&gt;3.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Note: You can also download the mongodb-driver jar directly from sonatype.If downloading mongodb-driver manually, you must also download its dependencies: bson and mongodb-driver-coreUber Jar (Legacy)For new applications, the preferred artifact is mongodb-driver-sync; however, the legacy mongo-java-driver uber jar is still available. The uber jar contains: the BSON library, the core library, and the mongodb-driver. uber jar这个包是很老的包，但是还是可以用。 NOTEThis is a Java 9-compliant module with an Automatic-Module-Name of org.mongodb.driver.sync.client.The mongo-java-driver artifact is a valid OSGi bundle whose symbolic name is org.mongodb.mongo-java-driver.IMPORTANTThis module is deprecated and will no longer be published in the next major release of the driver (4.0).1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Note: You can also download the mongo-java-driver jar directly from sonatype. 参考http://mongodb.github.io/mongo-java-driver/3.9/driver/getting-started/installation/]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MongoDB</tag>
        <tag>driver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS的font和font-size]]></title>
    <url>%2Fcss%2Fcss-font-note.html</url>
    <content type="text"><![CDATA[CSS的font和font-size。 引用原帖收藏于IT老兵驿站。 前言今天遇到一个问题，没有搞明白font和font-size的关系，研究了https://www.w3schools.com/css/css_font.asp这个帖子，终于搞明白，不过搞不懂为什么，这个网站为什么也需要翻墙，所以就诞生了中文的http://www.w3school.com.cn/这样的网站。但是“取法乎上，才能得其中”，想得到最好的学习效果，还是应该去看原版的东西。 这篇文章原本以富文本编辑发表，但是富文本编辑器的引用功能失效，导致格式很乱，只好再编辑成Markdown的格式。 正文先来破题，一言以蔽之，font相当于多个font相关属性的组合属性。 这篇笔记会把部分重要的原文摘录在下面，底下配上一定的翻译和笔记，这篇文章感觉英文相对都比较简单，没有太多需要翻译的地方。原文中有去往在线操练场的链接，也都是不翻墙而不能用。 The CSS font properties define the font family, boldness, size, and the style of a text. CSS的font属性定义了font family、boldness,、size和一个文本的style。这是CSS的四个属性。 CSS Font FamiliesIn CSS, there are two types of font family names:generic family - a group of font families with a similar look (like “Serif” or “Monospace”)font family - a specific font family (like “Times New Roman” or “Arial”) Font FamilyThe font family of a text is set with the font-family property.The font-family property should hold several font names as a “fallback” system. If the browser does not support the first font, it tries the next font, and so on. font-family属性准备了一些字体作为“后备”系统。如果浏览器不支持前面的字体，它就尝试下一个，以此类推。 Start with the font you want, and end with a generic family, to let the browser pick a similar font in the generic family, if no other fonts are available. 用你想要的字体开始，用一个generic family（普通的字体家族）结束，如果没有其它字体可用，让浏览器从generic family选择一个相似的字体。 Note: If the name of a font family is more than one word, it must be in quotation marks, like: “Times New Roman”.More than one font family is specified in a comma-separated list: Example123p &#123; font-family: &quot;Times New Roman&quot;, Times, serif;&#125; For commonly used font combinations, look at our Web Safe Font Combinations. Font Style字体样式 The font-style property is mostly used to specify italic text. 字体样式属性更多用来指定italic（斜体）文本。 This property has three values:normal - The text is shown normallyitalic - The text is shown in italicsoblique - The text is “leaning” (oblique is very similar to italic, but less supported) 这个属性和italic有些像，不过缺乏一些浏览器的支持123456789101112Examplep.normal &#123; font-style: normal;&#125;p.italic &#123; font-style: italic;&#125;p.oblique &#123; font-style: oblique;&#125; Font Size字体大小 The font-size property sets the size of the text. font-size属性设置文本的大小。 Being able to manage the text size is important in web design. However, you should not use font size adjustments to make paragraphs look like headings, or headings look like paragraphs. 在web设计中能够管理文本的大小是很重要的。然而，你不应该使用这个属性去修改标题，让它像段落，反之亦然。 Always use the proper HTML tags, like&lt;h1&gt; - &lt;h6&gt;for headings and &lt;p&gt;for paragraphs. 能使用HTML标签的地方，尽量去使用，例如&lt;h1&gt; - &lt;h6&gt;，去用作标题，和&lt;p&gt;用作段落。 The font-size value can be an absolute, or relative size. 字体大小可以是一个绝对大小，或者是一个相对大小。 Absolute size:Sets the text to a specified sizeDoes not allow a user to change the text size in all browsers (bad for accessibility reasons)Absolute size is useful when the physical size of the output is knownRelative size:Sets the size relative to surrounding elementsAllows a user to change the text size in browsersNote: If you do not specify a font size, the default size for normal text, like paragraphs, is 16px (16px=1em). Set Font Size With Pixels用pixels来设置字体大小 Setting the text size with pixels gives you full control over the text size:123456789101112Exampleh1 &#123; font-size: 40px;&#125;h2 &#123; font-size: 30px;&#125;p &#123; font-size: 14px;&#125; Tip: If you use pixels, you can still use the zoom tool to resize the entire page. Set Font Size With Em用Em设置字体大小 To allow users to resize the text (in the browser menu), many developers use em instead of pixels.The em size unit is recommended by the W3C. em字体单位被W3C所推荐。 1em is equal to the current font size. The default text size in browsers is 16px. So, the default size of 1em is 16px. 1em相当于当前的字体大小，例如当前浏览器的字体大小是16px，那么1em的默认大小就是16px。 The size can be calculated from pixels to em using this formula: pixels/16=em pixel和em的换算公式：pixels/16=em123456789101112Exampleh1 &#123; font-size: 2.5em; /* 40px/16=2.5em */&#125;h2 &#123; font-size: 1.875em; /* 30px/16=1.875em */&#125;p &#123; font-size: 0.875em; /* 14px/16=0.875em */&#125; In the example above, the text size in em is the same as the previous example in pixels. However, with the em size, it is possible to adjust the text size in all browsers.Unfortunately, there is still a problem with older versions of IE. The text becomes larger than it should when made larger, and smaller than it should when made smaller. Use a Combination of Percent and Em组合使用百分比和Em The solution that works in all browsers, is to set a default font-size in percent for the element:12345678910111213141516Examplebody &#123; font-size: 100%;&#125;h1 &#123; font-size: 2.5em;&#125;h2 &#123; font-size: 1.875em;&#125;p &#123; font-size: 0.875em;&#125; Our code now works great! It shows the same text size in all browsers, and allows all browsers to zoom or resize the text! Font Weight字体权重（姑且把weight翻译成权重） The font-weight property specifies the weight of a font:12345678Examplep.normal &#123; font-weight: normal;&#125;p.thick &#123; font-weight: bold;&#125; Responsive Font Size响应式字体大小 The text size can be set with a vw unit, which means the “viewport width”. 使用vm单位当做字体大小的单位，vm意味着“viewport width”，视点宽度。 That way the text size will follow the size of the browser window:Hello WorldResize the browser window to see how the font size scales.12Example&lt;h1 style=&quot;font-size:10vw&quot;&gt;Hello World&lt;/h1&gt; Viewport is the browser window size. 1vw = 1% of viewport width. If the viewport is 50cm wide, 1vw is 0.5cm. Viewport是浏览器窗体大小，1vw=1%的viewport的宽度，如果viewport是50cm宽，那么1vw是0.5cm宽。 Font Variant字体变种 The font-variant property specifies whether or not a text should be displayed in a small-caps font. font-variant这个属性指定了一个文本是否会用small-caps字体所显示。 In a small-caps font, all lowercase letters are converted to uppercase letters. However, the converted uppercase letters appears in a smaller font size than the original uppercase letters in the text. 在small-caps字体中，所有小写字母会被转换成大写字母。然后，转换后的大写字母用一种比正常的大写字母小的字体来显示。这个最好要实践一下才能理解。12345678Examplep.normal &#123; font-variant: normal;&#125;p.small &#123; font-variant: small-caps;&#125; 总结研究了font，搞明白了一直没有搞明白的这些关系。现在感觉学的都有些碎片化，欠缺的是如何去设计CSS呢？要找一篇对于这个的介绍来学习学习。 参考https://www.w3schools.com/css/css_font.asp]]></content>
      <categories>
        <category>CSS</category>
      </categories>
      <tags>
        <tag>CSS</tag>
        <tag>font</tag>
        <tag>font-size</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java：学习@Autowired]]></title>
    <url>%2Fjava%2Fjava-spring-autowired.html</url>
    <content type="text"><![CDATA[概要本文记录一下对于Java的@Autowired注解的学习。 博客原贴收藏在IT老兵驿站。 前言使用SpringMVC，现在不可避免要接触注解，会遇到@Autowired，查询了google，找到这篇文章，摘录下来，做一些笔记。为什么这样呢？因为至少来说，以当前的理解，自己来写，不太可能超越这篇文章，我也不想像很多人那样搞所谓“二次原创”，其实不还是抄袭，只是针对了判定抄袭的规则来做文章，不耍这样的小聪明，没有意义。 本篇选取主要的部分翻译一下，具体的过程可以参考代码，本文的英文难度不高，可以尝试着读一读。 正文 OverviewStarting with Spring 2.5, the framework introduced a new style of Dependency Injection driven by @Autowired Annotations. This annotation allows Spring to resolve and inject collaborating beans into your bean.In this tutorial, we will look at how to enable autowiring, various ways to wire in beans, making beans optional, resolving bean conflicts using @Qualifier annotation along with potential exception scenarios. Spring2.5之后，可以使用@Autowired 注解来实现DI（依赖注入），这个词本身的英文意思就是自动装配。 Enabling @Autowired AnnotationsIf you are using Java based configuration in your application you can enable annotation-driven injection by using AnnotationConfigApplicationContext to load your spring configuration as below: @Configuration @ComponentScan(&quot;com.baeldung.autowire.sample&quot;) public class AppConfig {} As an alternative, in Spring XML, it can be enabled by declaring it inSpring XML files like so: context:annotation-config/ 想使用注解，现需要配置Spring可以支持注解，有两种方式，一个是在代码中，一个是在XML中，这个涉及到另外一些知识点，作者这里是假设读者是知道这些的，不明白的话，需要去查一查。 Using @Autowired Once annotation injection is enabled, autowiring can be used on properties, setters, and constructors. 可以用在属性、setter方法和构造器上。 3.1. @Autowired on Properties The annotation can be used directly on properties, therefore eliminating the need for getters and setters: @Component(&quot;fooFormatter&quot;) public class FooFormatter { public String format() { return &quot;foo&quot;; } } @Component public class FooService { @Autowired private FooFormatter fooFormatter; } In the above example, Spring looks for and injects fooFormatter when FooService is created. 上面的例子介绍了@Autowired如何用在属性上。 3.2. @Autowired on SettersThe @Autowired annotation can be used on setter methods. In the below example, when the annotation is used on the setter method, the setter method is called with the instance of FooFormatter when FooServiceis created: public class FooService { private FooFormatter fooFormatter; @Autowired public void setFooFormatter(FooFormatter fooFormatter) { this.fooFormatter = fooFormatter; } } 上面讲了如何用在方法上。 3.3. @Autowired on ConstructorsThe @Autowired annotation can also be used on constructors. In the below example, when the annotation is used on a constructor, an instance of FooFormatter is injected as an argument to the constructor when FooService is created: public class FooService { private FooFormatter fooFormatter; @Autowired public FooService(FooFormatter fooFormatter) { this.fooFormatter = fooFormatter; } } 如何用在构造器方法上。 @Autowired and Optional DependenciesSpring expects @Autowired dependencies to be available when the dependent bean is being constructed. If the framework cannot resolve a bean for wiring, it will throw the below-quoted exception and prevent the Spring container from launching successfully: Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.autowire.sample.FooDAO] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations:{@org.springframework.beans.factory.annotation.Autowired(required=true)}To avoid this from happening, a bean can optional be specified as below: public class FooService { @Autowired(required = false) private FooDAO dataAccessor; } 如果没有找到响应的bean，又不想系统停止加载，参考上面的写法。 Autowire DisambiguationBy default, Spring resolves @Autowired entries by type. If more than one beans of the same type are available in the container, the framework will throw a fatal exception indicating that more than one bean is available for autowiring. @Autowired是根据类型来进行装备的。但是会存在同一类型内有多个备选bean，这个时候，框架会抛出一个致命错误—-这种问题倒是暂时还没有遇到过，下面讲述了三种解决方案。 5.1. Autowiring by @QualifierThe @Qualifier annotation can be used to hint at and narrow down the required bean: @Component(&quot;fooFormatter&quot;) public class FooFormatter implements Formatter { public String format() { return &quot;foo&quot;; } } @Component(&quot;barFormatter&quot;) public class BarFormatter implements Formatter { public String format() { return &quot;bar&quot;; } } public class FooService { @Autowired private Formatter formatter; } Since there are two concrete implementations of Formatter available for the Spring container to inject, Spring will throw a NoUniqueBeanDefinitionException exception when constructing the FooService: Caused by: org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type [com.autowire.sample.Formatter] is defined: expected single matching bean but found 2: barFormatter,fooFormatter This can be avoided by narrowing the implementation using a @Qualifier annotation: public class FooService { @Autowired @Qualifier(&quot;fooFormatter&quot;) private Formatter formatter; } By specifying the @Qualifier with the name of the specific implementation, in this case as fooFormatter, we can avoid ambiguity when Spring finds multiple beans of the same type.Please note that the value of the @Qualifier annotation matches with the name declared in the @Component annotation of our FooFormatter implementation. 使用@Qualifier 注解来标识谁是合格的。 5.2. Autowiring by Custom QualifierSpring allows us to create our own @Qualifier annotation. To create a custom Qualifier, define an annotation and provide the @Qualifier annotation within the definition as below: @Qualifier @Target({ElementType.FIELD, ElementType.METHOD,ElementType.TYPE, ElementType.PARAMETER}) @Retention(RetentionPolicy.RUNTIME) public @interface FormatterType { String value(); } Once defined, the FormatterType can be used within various implementations to specify custom value: @FormatterType(&quot;Foo&quot;) @Component public class FooFormatter implements Formatter { public String format() { return &quot;foo&quot;; } } @FormatterType(&quot;Bar&quot;) @Component public class BarFormatter implements Formatter { public String format() { return &quot;bar&quot;; } } Once the implementations are annotated, the custom Qualifier annotation can be used as below: @Component public class FooService { @Autowired @FormatterType(&quot;Foo&quot;) private Formatter formatter; } The value specified in the @Target annotation restrict where the qualifier can be used to mark injection points.In the above code snippet, the qualifier can be used to disambiguate the point where Spring can inject the bean into a field, a method, a type, and a parameter. 5.3. Autowiring by NameAs a fallback Spring uses the bean name as a default qualifier value.So by defining the bean property name, in this case as fooFormatter, Spring matches that to the FooFormatter implementation and injects that specific implementation when FooService is constructed: public class FooService { @Autowired private Formatter fooFormatter; } 使用名字来自动装配。 ConclusionAlthough both @Qualifier and bean name fallback match can be used to narrow down to a specific bean, autowiring is really all about injection by type and this is how best to use this container feature.The source code of this tutorial can be found in the GitHub project – this is an Eclipse based project, so it should be easy to import and run as it is. 总结老外的文章讲的真清楚，他们会对读者负责，很认真地把所有问题讲清楚，不像很多国人，哪怕是很多好像挺有名的教授出的书，都是“言简意赅”，看着好费劲，给人一种高高在上的感觉。当年读清华严蔚敏出的《数据结构》，读着就非常费劲，后来看大师出的《算法导论》，反而容易理解，中国人怎么总就是这么高傲呢。 参考https://www.baeldung.com/spring-autowire https://stackoverflow.com/questions/1018797/can-you-use-autowired-with-static-fields 介绍了静态域能不能自动装配]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Autowired</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java：synchronized关键字]]></title>
    <url>%2Fjava%2Fjava-synchronized-note.html</url>
    <content type="text"><![CDATA[Java：synchronized关键字 原帖收藏于IT老兵博客。 前言这里翻译一篇对Java的synchronized关键字的讲解文章，这个关键字用于解决Java世界里面竞争条件时的访问冲突问题。 正文Java synchronized块将方法或代码块标记为已同步。 Java synchronized块可用于避免竞争条件。 Java同步关键字Java中的同步块使用synchronized关键字标记。Java中的同步块在某个对象上同步。在同一对象上的所有同步块在同一时刻只能被一个线程执行。尝试进入同步块的所有其他线程将被阻塞，直到同步块内的线程退出块。 synchronized关键字可用于标记四种不同类型的块： 实例方法 静态方法 实例方法中的代码块 静态方法中的代码块 这些块在不同对象上同步，您需要哪种类型的同步块取决于具体情况。 同步实例方法这是一个同步的实例方法：123public synchronized void add（int value）&#123; this.count + = value;&#125; 请注意在方法声明中使用synchronized关键字，这告诉Java该方法是同步的。 Java中的同步实例方法在拥有该方法的实例（对象）上同步。因此，每个实例的同步方法在不同的对象上同步：拥有它的实例。在一个同步实例方法中，只有一个线程可以执行。如果存在多个实例，则一次只能有一个线程可以在每个实例的同步实例方法内执行。每个实例一个线程。 同步静态方法静态方法被标记为synchronized，就像使用synchronized关键字的实例方法一样。这是一个Java synchronized静态方法示例：123public static synchronized void add（int value）&#123; count + = value;&#125; 此处，synchronized关键字告诉Java该方法已同步。 同步静态方法在同步静态方法所属的类的类对象上同步。由于每个类在Java VM中只存在一个类对象，因此在同一个类中的静态同步方法中只能执行一个线程。 如果静态同步方法位于不同的类中，则一个线程可以在每个类的静态同步方法内执行。每个类一个线程，无论它调用哪个静态同步方法。 实例方法中的同步块您不需要同步整个方法，有时只需要同步方法的一部分。方法中的Java同步块使这成为可能。 以下是非同步Java方法中的同步Java代码块：12345public void add（int value）&#123; synchronized（this）&#123; this.count + = value; &#125;&#125; 此示例使用Java synchronized块构造将代码块标记为已同步。此代码现在将像执行同步方法一样执行。 请注意Java synchronized块构造如何在括号中获取对象。在示例中使用“this”，这是调用add方法的实例。由synchronized构造在括号中获取的对象称为监视器对象。据说代码在监视器对象上同步。同步实例方法使用它所属的对象作为监视对象。 只有一个线程可以在同一监视器对象上同步的Java代码块内执行。 以下两个示例在调用它们的实例上同步。因此，它们在同步方面是等效的：12345678910111213public class MyClass &#123; public synchronized void log1（String msg1，String msg2）&#123; log.writeln（MSG1）; log.writeln（MSG2）; &#125; public void log2（String msg1，String msg2）&#123; synchronized（this）&#123; log.writeln（MSG1）; log.writeln（MSG2）; &#125; &#125;&#125; 因此，在该示例中，只有单个线程可以在两个同步块中的任一个内执行。 如果第二个同步块在与此不同的对象上同步，则一次一个线程就能够在每个方法内执行。 静态方法中的同步块以下是与静态方法相同的两个示例。这些方法在方法所属的类的类对象上同步：123456789101112131415public class MyClass &#123; public static synchronized void log1（String msg1，String msg2）&#123; log.writeln（MSG1）; log.writeln（MSG2）; &#125; public static void log2（String msg1，String msg2）&#123; synchronized（MyClass.class）&#123; log.writeln（MSG1）; log.writeln（MSG2）; &#125; &#125;&#125; 只有一个线程可以同时在这两个方法中的任何一个内执行。 如果第二个同步块在与MyClass.class不同的对象上同步，那么一个线程可以同时在每个方法内执行。 Java同步例子下面是一个示例，它启动2个线程并让它们在同一个Counter实例上调用add方法。 一次只有一个线程能够在同一个实例上调用add方法，因为该方法在它所属的实例上是同步的。12345678public class Counter &#123; long count = 0; public synchronized void add(long value) &#123; this.count += value; &#125;&#125; 1234567891011121314public class CounterThread extends Thread &#123; protected Counter counter = null; public CounterThread(Counter counter) &#123; this.counter = counter; &#125; public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; counter.add(i); &#125; &#125;&#125; 1234567891011public class Example &#123; public static void main(String[] args) &#123; Counter counter = new Counter(); Thread threadA = new CounterThread(counter); Thread threadB = new CounterThread(counter); threadA.start(); threadB.start(); &#125;&#125; 创建了两个线程。 相同的Counter实例在其构造函数中传递给它们。 Counter.add()方法在实例上同步，因为add方法是一个实例方法，并标记为synchronized。 因此，只有一个线程可以一次调用add()方法。 另一个线程将等到第一个线程离开add()方法，然后才能执行方法本身。 如果两个线程引用了两个单独的Counter实例，那么同时调用add()方法就没有问题。 调用将是对不同的对象，因此调用的方法也将在不同的对象（拥有该方法的对象）上同步。 因此呼叫不会阻止。 这是如何看起来：123456789101112public class Example &#123; public static void main(String[] args) &#123; Counter counterA = new Counter(); Counter counterB = new Counter(); Thread threadA = new CounterThread(counterA); Thread threadB = new CounterThread(counterB); threadA.start(); threadB.start(); &#125;&#125; 注意两个线程threadA和threadB如何不再引用相同的计数器实例。 counterA和counterB的add方法在它们的两个拥有实例上同步。 因此，在counterA上调用add()将不会阻止对counterB的add()调用。 Java并发实用程序同步机制是Java的第一种机制，用于同步对多个线程共享的对象的访问。但同步机制并不是很先进。 这就是为什么Java 5获得了一整套concurrency utility classes来帮助开发人员实现比同步所获得的更细粒度的并发控制。 总结这篇文章对synchronized关键字进行了详细的讲解，个人感觉讲解的不错，翻译出来，做个记录。 参考http://tutorials.jenkov.com/java-concurrency/synchronized.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信开发：NAT穿透]]></title>
    <url>%2Fwechat%2Fnat-penetration-ngrok.html</url>
    <content type="text"><![CDATA[原帖收藏于IT老兵博客。 微信开发：NAT穿透。 前言调试微信，遇到问题，微信无法直接通知到开发机上，而是需要一台服务器的地址上，这给调试增加了难度，上网研究了一下，如果在windows下，可以使用花生壳软件，进行NAT穿透，但是花生壳不支持mac，又找了找，发现了ngrok，这个配置很简单，解决了问题。 正文一共需要四步： Download ngrokFirst, download the ngrok client, a single binary with zero run-time dependencies.Mac OS XWindows Linux Mac (32-bit) Windows (32-bit)Linux (ARM) Linux (ARM64) Linux (32-bit)FreeBSD (64-Bit) FreeBSD (32-bit) 第一步，下载ngrok，它支持Mac、Linux、Windows等多个操作系统。 Unzip to installOn Linux or OSX you can unzip ngrok from a terminal with the following command. On Windows, just double click ngrok.zip.unzip /path/to/ngrok.zipMost people like to keep ngrok in their primary user folder or set an alias for easy command-line access. 第二步，解压缩。 Connect your accountRunning this command will add your authtoken to your ngrok.yml file. Connecting an account will list your open tunnels in the dashboard, give you longer tunnel timeouts, and more. Visit the dashboard to get your auth token.1./ngrok authtoken &lt;YOUR_AUTH_TOKEN&gt; Don’t have an account?Sign up for free to get your auth token. 第三步，注册账号，点击上面的Sign up链接，去注册，或者使用谷歌账户授权，有了授权，这个工具会提供更好的支持。 Fire it upTry it out by running it from the command line:./ngrok helpTo start a HTTP tunnel on port 80, run this next:./ngrok http 80Read the documentation to get more ideas on how to use ngrok. 第四部，开始使用，./ngrok help 可以获取帮助，./ngrok http 80 开始运行，注意80是你想要反向代理的端口，这个要根据你的需求来设置，例如这里是tomcat，端口是8080，那么上面要改成8080。 运行起来之后，它会给你分配一个动态的域名，如上图，在浏览器访问这个域名，就可以访问到你的服务了。 总结工欲善其事，必先利其器。微信开发，如果不配置好这个回调，那么每次都需要部署到服务器上才能验证，这样效率会低很多。]]></content>
      <categories>
        <category>微信</category>
      </categories>
      <tags>
        <tag>微信</tag>
        <tag>穿透</tag>
        <tag>NAT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReactiveCocoa指导一笔记]]></title>
    <url>%2Fios%2Fios-reactivecocoa-tutorial-the-definitive-introduction-part-1-2-note.html</url>
    <content type="text"><![CDATA[ReactiveCocoa指导一笔记 原帖收藏于[IT老兵博客](https://www.oxysun.cn/ios/ios-reactivecocoa-tutorial-the-definitive-introduction-part-1-2-note.html)。 前言 这周在学习RAC，找到一篇很好的文章，（摘录下来，对部分内容进行一定的翻译，感觉翻译要比简单的读，更需要字斟句酌，更容易留下较深的印象，加深理解，翻译出来，也可以帮助到别人）。原本计划是部分翻译，最后没有掌握好，还是全部翻译了，一共花费了将近十个小时，之前对这篇文章的研究也花费了将近十个小时，这样效率有点低，下一步，要看看怎么提高一下这个效率。 这篇文章跟着走一遍，对于RAC基本就能有一定的印象了，然后再针对每个细节来进行学习，就能较快掌握住RAC了，否则，只看官网，官网的文章写得感觉并不是很清楚。 原帖位置：https://www.raywenderlich.com/2493-reactivecocoa-tutorial-the-definitive-introduction-part-1-2。 ReactiveCocoa Tutorial – The Definitive Introduction: Part 1/2 Get to grips with ReactiveCocoa in this 2-part tutorial series. Put the paradigms to one-side, and understand the practical value with work-through examples. 在这两部分指导系统掌握 ReactiveCocoa。先把范式放在一边，先通过对这些例子的实践来理解实际的意思。 As an iOS developer, nearly every line of code you write is in reaction to some event; a button tap, a received network message, a property change (via Key Value Observing) or a change in user’s location via CoreLocation are all good examples. However, these events are all encoded in different ways; as actions, delegates, KVO, callbacks and others. ReactiveCocoa defines a standard interface for events, so they can be more easily chained, filtered and composed using a basic set of tools. 作为一个iOS开发者，几乎你编写的每一行代码都是一个对某种事件的响应；一个按钮的点击，一个网络消息的接收，一个属性的改变（通过Key Value Observing）或者通过 CoreLocation 的用户位置的改变都是很好的例子。然后，这些事件被用不同的方式进行的编码；被当做 action， delegates， KVO， callback 和其他。ReactiveCocoa 定义了一个标准的事件界面，这样他们可以被更容易地链化，过滤和使用一个基本的工具集来编写。 Sound confusing? Intriguing? … Mind blowing? Then read on :] 听上去有些困惑？有趣？ ... 令人兴奋？来继续阅读吧 :] ReactiveCocoa combines a couple of programming styles: ReactiveCocoa 由一组编程风格组成： Functional Programming which makes use of higher order functions, i.e. functions which take other functions as their arguments 函数式编程 这使用了很多高阶函数，例如函数使用其它函数作为参数 Reactive Programming which focuses of data-flows and change propagation 响应式编程 这聚焦于数据流和改变的传播 For this reason, you might hear ReactiveCocoa described as a Functional Reactive Programming (or FRP) framework. 因为这个原因，你可能听到过把ReactiveCocoa描述成一个函数响应式框架。 Rest assured, that is as academic as this tutorial is going to get! Programming paradigms are a fascinating subject, but the rest of this ReactiveCocoa tutorials focuses solely on the practical value, with work-through examples instead of academic theories. 请放心，这就像本教程的学术内容一样！ 编程范式是一个引人入胜的主题，但ReactiveCocoa教程的其余部分仅关注实用价值，并通过实例来代替学术理论。 The Reactive Playground 响应式游乐场 Throughout this ReactiveCocoa tutorial, you’ll be introducing reactive programming to a very simple example application, the ReactivePlayground. Download the starter project, then build and run to verify you have everything set up correctly. 通过这个指导，你将引入响应式编程到一个非常简单的样例程序中，就是这个响应式游乐场。下载这个starter project，然后编译并且运行它去校验你已经正确安装了所有的事情。 ReactivePlayground is a very simple app that presents a sign-in screen to the user. Supply the correct credentials, which are, somewhat imaginatively, user for the username, and password for the password, and you’ll be greeted by a picture of a lovely little kitten. ReactivePlayground 是一个非常简单的app代表了一个sign-in屏幕给用户。提供正确的凭证，有点想象力，用户名是user， 密码是 password， 然后你将会看到一只可爱的小猫的欢迎页。 Awww! How cute! 喔！多么可爱！ Right now it’s a good point to spend a little time looking through the code of this starter project. It is quite simple, so it shouldn’t take long. 现在是时候花费一些时间去看一下这个项目的代码。它非常简单，所以这不用花很长时间。 Open RWViewController.m and take a look around. How quickly can you identify the condition that results in the enabling of the Sign In button? What are the rules for showing / hiding the signInFailure label? In this relatively simple example, it might take only a minute or two to answer these questions. For a more complex example, you should be able to see how this same type of analysis might take quite a bit longer. 打开RWViewController.m大概看一下。 您能多快识别出产生启用Sign In按钮的条件？ 显示/隐藏signInFailure标签的规则是什么？ 在这个相对简单的例子中，回答这些问题可能只需要一两分钟。 对于更复杂的示例，您应该能够看到相同类型的分析可能需要更长的时间。 With the use of ReactiveCocoa, the underlying intent of the application will become a lot clearer. It’s time to get started! 随着ReactiveCocoa的使用，这个程序的潜在意图会更加清楚。让我们开始吧！ Adding the ReactiveCocoa Framework 增加 ReactiveCocoa 框架 The easiest way to add the ReactiveCocoa framework to your project is via CocoaPods. If you’ve never used CocoaPods before it might make sense to follow the Introduction To CocoaPods tutorial on this site, or at the very least run through the initial steps of that tutorial so you can install the prerequisites. 最容易的增加ReactiveCocoa框架到你的项目中的方法是通过CocoaPods。如果你没有使用过它，那么遵循着Introduction To CocoaPods的指导是有意义的，或者至少运行一下那个指导的初始步骤，所以你就可以安装依赖了。 Note: If for some reason you don’t want to use CocoaPods you can still use ReactiveCocoa, just follow the Importing ReactiveCocoa steps in the documentation on GitHub. 注意：如果因为什么原因，你不想使用CocoaPods， 你仍然可以使用ReactiveCocoa， 只要跟着在Github上的Importing ReactiveCocoa的文档的步骤。 If you still have the ReactivePlayground project open in Xcode, then close it now. CocoaPods will create an Xcode workspace, which you’ll want to use instead of the original project file. 如果你的ReactivePlayground项目是打开的，那么现在关上它。CocoaPods将会创建一个Xcode工作空间，你可以使用那个来替换原本的项目文件。 Open Terminal. Navigate to the folder where your project is located and type the following: 打开终端。切换到项目目录，输入以下命令： touch Podfile open -e Podfile This creates an empty file called Podfile and opens it with TextEdit. Copy and paste the following lines into the TextEdit window: 这将创建一个空的名字是Podfile的文件，用文本编辑器打开它。复制并且贴入下面的代码： platform :ios, '7.0' pod 'ReactiveCocoa', '2.1.8' This sets the platform to iOS, the minimum SDK version to 7.0, and adds the ReactiveCocoa framework as a dependency. 这个设置了iOS的平台，最低SDK版本是7.0，并且增加了ReactiveCocoa框架作为依赖。 Once you’ve saved this file, go back to the Terminal window and issue the following command: 一旦你保存了这个文件，回到终端，输入下面的命令： pod install You should see an output similar to the following: 你将会看到类似以下的输出： Analyzing dependencies Downloading dependencies Installing ReactiveCocoa (2.1.8) Generating Pods project Integrating client project [!] From now on use `RWReactivePlayground.xcworkspace`. This indicates that the ReactiveCocoa framework has been downloaded, and CocoaPods has created an Xcode workspace to integrate the framework into your existing application. 这意味着ReactiveCocoa框架已经被下载，并且CocoaPods创建了一个Xcode工作空间，集成了框架到你的程序中。 Open up the newly generated workspace, RWReactivePlayground.xcworkspace, and look at the structure CocoaPods created inside the Project Navigator: 打开新生成的工作空间，RWReactivePlayground.xcworkspace，看一下 CocoaPods 在项目浏览器中创建的结构： You should see that CocoaPods created a new workspace and added the original project, RWReactivePlayground, together with a Pods project that includes ReactiveCocoa. CocoaPods really does make managing dependencies a breeze! 你应该看到 CocoaPods 创建了一个新的工作空间，并且增加了原始的项目，RWReactivePlayground，和一个包含着ReactiveCocoa的 Pods 项目在一起。CocoaPods 确实使管理依赖变得轻而易举。 You’ll notice this project’s name is ReactivePlayground, so that must mean it’s time to play … 你将会注意到这个项目的名字是 ReactivePlayground， 所以是时候来玩了 ... Time To Play 游玩时间到 As mentioned in the introduction, ReactiveCocoa provides a standard interface for handling the disparate stream of events that occur within your application. In ReactiveCocoa terminology these are called signals, and are represented by the RACSignal class. 就像介绍里面提到，ReactiveCocoa提供了一个标准的界面，来处理不同的产生自你的应用中的事件流。在ReactiveCocoa的术语中，它们被称为signal（信号，后文都称为signal），被RACSignal类所代表。 Open the initial view controller for this app, RWViewController.m, and import the ReactiveCocoa header by adding the following to the top of the file: 打开这个应用的初始的view controller，RWViewController.m， 引入ReactiveCocoa头文件。 #import &lt;ReactiveCocoa/ReactiveCocoa.h&gt; You aren’t going to replace any of the existing code just yet, for now you’re just going to play around a bit. Add the following code to the end of the viewDidLoadmethod: 你还不能替代现有的任何代码，你可以先运行一下。在viewDidLoad方法底部加入下面的代码： [self.usernameTextField.rac_textSignal subscribeNext:^(id x) { NSLog(@"%@", x); }]; Build and run the application and type some text into the username text field. Keep an eye on the console and look for an output similar to the following: 编译并且运行这个程序，在username文本域输入一些字符。留心控制台，会看到类似下面的输出： 2013-12-24 14:48:50.359 RWReactivePlayground[9193:a0b] i 2013-12-24 14:48:50.436 RWReactivePlayground[9193:a0b] is 2013-12-24 14:48:50.541 RWReactivePlayground[9193:a0b] is 2013-12-24 14:48:50.695 RWReactivePlayground[9193:a0b] is t 2013-12-24 14:48:50.831 RWReactivePlayground[9193:a0b] is th 2013-12-24 14:48:50.878 RWReactivePlayground[9193:a0b] is thi 2013-12-24 14:48:50.901 RWReactivePlayground[9193:a0b] is this 2013-12-24 14:48:51.009 RWReactivePlayground[9193:a0b] is this 2013-12-24 14:48:51.142 RWReactivePlayground[9193:a0b] is this m 2013-12-24 14:48:51.236 RWReactivePlayground[9193:a0b] is this ma 2013-12-24 14:48:51.335 RWReactivePlayground[9193:a0b] is this mag 2013-12-24 14:48:51.439 RWReactivePlayground[9193:a0b] is this magi 2013-12-24 14:48:51.535 RWReactivePlayground[9193:a0b] is this magic 2013-12-24 14:48:51.774 RWReactivePlayground[9193:a0b] is this magic? You can see that each time you change the text within the text field, the code within the block executes. No target-action, no delegates — just signals and blocks. That’s pretty exciting! 你可以看到每一次你在这个文本域修改了文本，这个 block 中的代码会运行。没有target-action， 没有delegates ----只有signal和 block。是不是很令人兴奋！ ReactiveCocoa signals (represented by RACSignal) send a stream of events to their subscribers. There are three types of events to know: next, error and completed. A signal may send any number of next events before it terminates after an error, or it completes. In this part of the tutorial you’ll focus on the next event. Be sure to read part two when it’s available to learn about error and completed events. ReactiveCocoa signals（被 RACSignal 所代表）发送了一个事件流给它们的订阅者。有三种类型的时间需要了解：next，error 和 completed。一个signal在它在一个error或者completed结束之前可以发送任意数量的next事件。在这部分的指导中，你将会聚焦于next事件。当有条件去学习error和completed事件时，要保证去阅读part two。 RACSignal has a number of methods you can use to subscribe to these different event types. Each method takes one or more blocks, with the logic in your block executing when an event occurs. In this case, you can see that the subscribeNext:method was used to supply a block that executes on each next event. RACSignal有很多方法，你可以用来去订阅这些不同的事件类型。每一个方法接收一个或多个block，当事件发生时，block中的逻辑会被执行。在这种情况下，您可以看到subscribeNext:方法用于提供在每个next事件上执行的块。 The ReactiveCocoa framework uses categories to add signals to many of the standard UIKit controls so you can add subscriptions to their events, which is where the rac_textSignal property on the text field came from. ReactiveCocoa框架使用了categories去给很多标准的 UIKit 组件增加signals，这样你可以给它们增加订阅，这就是这个文本域的rac_textSignal域的来源。 But enough with the theory, it’s time to start making ReactiveCocoa do some work for you! 但是理论足够（这里为什么要用“但是”），是时候开始让ReactiveCocoa为你做一些工作了！ ReactiveCocoa has a large range of operators you can use to manipulate streams of events. For example, assume you’re only interested in a username if it’s more than three characters long. You can achieve this by using the filter operator. Update the code you added previously in viewDidLoad to the following: ReactiveCocoa有很多的操作符，你可以用来操纵事件流。举例来说，假设你只对3个字符以上的username感兴趣。你可以用filter 操作符来得到这个。更新之前你加入到viewDidLoad的代码： [[self.usernameTextField.rac_textSignal filter:^BOOL(id value) { NSString *text = value; return text.length &gt; 3; }] subscribeNext:^(id x) { NSLog(@"%@", x); }]; If you build and run, then type some text into the text field, you should find that it only starts logging when the text field length is greater than three characters: 如果您构建并运行，然后在文本字段中键入一些文本，您会发现它只在文本字段长度大于三个字符时才开始记录： 2013-12-26 08:17:51.335 RWReactivePlayground[9654:a0b] is t 2013-12-26 08:17:51.478 RWReactivePlayground[9654:a0b] is th 2013-12-26 08:17:51.526 RWReactivePlayground[9654:a0b] is thi 2013-12-26 08:17:51.548 RWReactivePlayground[9654:a0b] is this 2013-12-26 08:17:51.676 RWReactivePlayground[9654:a0b] is this 2013-12-26 08:17:51.798 RWReactivePlayground[9654:a0b] is this m 2013-12-26 08:17:51.926 RWReactivePlayground[9654:a0b] is this ma 2013-12-26 08:17:51.987 RWReactivePlayground[9654:a0b] is this mag 2013-12-26 08:17:52.141 RWReactivePlayground[9654:a0b] is this magi 2013-12-26 08:17:52.229 RWReactivePlayground[9654:a0b] is this magic 2013-12-26 08:17:52.486 RWReactivePlayground[9654:a0b] is this magic? What you’ve created here is a very simple pipeline. It is the very essence of Reactive Programming, where you express your application’s functionality in terms of data flows. 你在这里创建的是一个非常简单的管道。 这是Reactive Programming的本质，您可以根据数据流表达应用程序的功能。 It can help to picture these flows graphically: 用图片来表示这个流是很有帮助的： In the above diagram you can see that the rac_textSignal is the initial source of events. The data flows through a filter that only allows events to pass if they contain a string with a length that is greater than three. The final step in the pipeline is subscribeNext: where your block logs the event value. 在上面这个图中，你可以看到rac_textSignal是事件的初始来源。数据流通过一个filter，仅仅允许包含字符串长度大于三的事件通过。管道的最后一步是subscribeNext: ，那里你的 block 输出了事件的值。 At this point it’s worth noting that the output of the filter operation is also an RACSignal. You could arrange the code as follows to show the discrete pipeline steps: 在这点上，值得注意的是，filter的输出也是一个RACSignal。你也可以像下面这样组织代码，来显示离散的管道步骤： RACSignal *usernameSourceSignal = self.usernameTextField.rac_textSignal; RACSignal *filteredUsername = [usernameSourceSignal filter:^BOOL(id value) { NSString *text = value; return text.length &gt; 3; }]; [filteredUsername subscribeNext:^(id x) { NSLog(@"%@", x); }]; Because each operation on an RACSignal also returns an RACSignal it’s termed a fluent interface. This feature allows you to construct pipelines without the need to reference each step using a local variable. 因为在一个RACSignal的每一个操作都返回一个RACSignal，所以它被称为fluent interface。这个特征允许你去构建管道，而不需要使用一个本地变量去指向管道的每一步。 Note: ReactiveCocoa makes heavy use of blocks. If you’re new to blocks, you might want to read Apple’s Blocks Programming Topics. And if, like me, you’re familiar with blocks, but find the syntax a little confusing and hard to remember, you might find the amusingly titled f*****gblocksyntax.com quite useful! (We censored the word to protect the innocent, but the link is fully functional.) 注意：ReactiveCocoa大量地使用了 blocks。如果你对block不了解，你应该去阅读苹果的 Blocks Programming Topics。或者，像我一样，你对blocks比较熟悉，但是发现这个语法有一些困扰，或者难于记忆，你可以去看看 f*****gblocksyntax.com，会很有帮助。（这个网址带着一些脏字，为了照顾读者，所以这里隐去了几个字母，但是链接是完全有效的。） A Little Cast 一个小的转换 If you updated your code to split it into the various RACSignal components, now is the time to revert it back to the fluent syntax: 如果你更新你的代码，把它分解成几个RACSignal组件，现在是时候把它转换成流利的语法： [[self.usernameTextField.rac_textSignal filter:^BOOL(id value) { NSString *text = value; // implicit cast return text.length &gt; 3; }] subscribeNext:^(id x) { NSLog(@"%@", x); }]; The implicit cast from id to NSString, at the indicated location in the code above, is less than elegant. Fortunately, since the value passed to this block is always going to be an NSString, you can change the parameter type itself. Update your code as follows: 这个从id到 NSString的隐含转换，在上面代码的带有注释的哪一行，是不优雅的。幸运的是，因为传递给这个block的值永远应该是一个NSString，所以你可以修改入参类型，就像下面这样： [[self.usernameTextField.rac_textSignal filter:^BOOL(NSString *text) { return text.length &gt; 3; }] subscribeNext:^(id x) { NSLog(@"%@", x); }]; Build and run to confirm this works just as it did previously. 构建并且运行去确认它工作起来和之前是一样的。 What’s An Event? 事件是什么？ So far this tutorial has described the different event types, but hasn’t detailed the structure of these events. What’s interesting is that an event can contain absolutely anything! 到这里，这份指导已经描述了不同的事件类型，但是还没有揭示这些事件的结构细节。有趣的是，一个事件可以包含任何东西！ As an illustration of this point, you’re going to add another operation to the pipeline. Update the code you added to viewDidLoad as follows: 作为对这点的一个阐述，你可以给管道增加另外一个操作。更新你之前增加的代码： [[[self.usernameTextField.rac_textSignal map:^id(NSString *text) { return @(text.length); }] filter:^BOOL(NSNumber *length) { return [length integerValue] &gt; 3; }] subscribeNext:^(id x) { NSLog(@"%@", x); }]; If you build and run you’ll find the app now logs the length of the text instead of the contents: 如果你构建并且运行它，你回发现现在程序打印的是文本的长度，而不再是它的内容了： 2013-12-26 12:06:54.566 RWReactivePlayground[10079:a0b] 4 2013-12-26 12:06:54.725 RWReactivePlayground[10079:a0b] 5 2013-12-26 12:06:54.853 RWReactivePlayground[10079:a0b] 6 2013-12-26 12:06:55.061 RWReactivePlayground[10079:a0b] 7 2013-12-26 12:06:55.197 RWReactivePlayground[10079:a0b] 8 2013-12-26 12:06:55.300 RWReactivePlayground[10079:a0b] 9 2013-12-26 12:06:55.462 RWReactivePlayground[10079:a0b] 10 2013-12-26 12:06:55.558 RWReactivePlayground[10079:a0b] 11 2013-12-26 12:06:55.646 RWReactivePlayground[10079:a0b] 12 The newly added map operation transforms the event data using the supplied block. For each next event it receives, it runs the given block and emits the return value as a next event. In the code above, the map takes the NSString input and takes its length, which results in an NSNumber being returned. 新增加的map操作使用提供的block转换了事件的数据。对于每一个它接收到的next事件，它运行给定的block，并且把返回值作为 next事件发射出去。在上面的代码中，map接收了NSString输入，并且获取了它的长度，用一个NSNumber返回出去。 For a stunning graphic depiction of how this works, take a look at this image: 有关其工作原理的精美图像描述，请查看此图像： As you can see, all of the steps that follow the map operation now receive NSNumberinstances. You can use the map operation to transform the received data into anything you like, as long as it’s an object. 就像你可以看到的，所有跟着map的步骤现在都接收到一个NSNumber实例。你可以使用map操作来转换接收到的数据成你想要的任何东西，只要它是一个对象。 Note: In the above example the text.length property returns an NSUInteger, which is a primitive type. In order to use it as the contents of an event, it must be boxed. Fortunately the Objective-C literal syntax provides and option to do this in a rather concise manner – @(text.length). 注意：在上面的例子中，这个text.length返回一个NSUInteger， 这是一个原始类型。为了在一个事件的内容中使用它，它必须被包裹。幸运的是， Objective-C literal syntax提供了一个非常简洁的方式去处理这个 -- @(text.length)。（这里好像是写错了“provides and option”似乎不合语法） That’s enough playing! It’s time to update the ReactivePlayground app to use the concepts you’ve learned so far. You may remove all of the code you’ve added since you started this tutorial. 这就足够了！ 是时候更新ReactivePlayground应用程序以使用您迄今为止学到的概念。 您可以删除自本教程开始以来添加的所有代码。 Creating Valid State Signals 创建有效状态信号 The first thing you need to do is create a couple of signals that indicate whether the username and password text fields are valid. Add the following to the end of viewDidLoad in RWViewController.m: 你需要做的第一件事情是创建一对信号，用来指示username和password 两个文本域是否有效。在RWViewController.m 的 viewDidLoad底部增加下面的代码： RACSignal *validUsernameSignal = [self.usernameTextField.rac_textSignal map:^id(NSString *text) { return @([self isValidUsername:text]); }]; RACSignal *validPasswordSignal = [self.passwordTextField.rac_textSignal map:^id(NSString *text) { return @([self isValidPassword:text]); }]; As you can see, the above code applies a map transform to the rac_textSignalfrom each text field. The output is a boolean value boxed as a NSNumber. 就像你看到的，上面代码使用了一个 map 去给每一个文本域转换 rac_textSignal。输出是被一个NSNumber包裹的布尔值。 The next step is to transform these signals so that they provide a nice background color to the text fields. Basically, you subscribe to this signal and use the result to update the text field background color. One viable option is as follows: 下一步是去转换这些信号，使得它们可以给这些文本域提供背景色。基本上，您订阅此信号并使用结果更新文本字段背景颜色。 一个可行的选择如下： [[validPasswordSignal map:^id(NSNumber *passwordValid) { return [passwordValid boolValue] ? [UIColor clearColor] : [UIColor yellowColor]; }] subscribeNext:^(UIColor *color) { self.passwordTextField.backgroundColor = color; }]; (Please don’t add this code, there’s a much more elegant solution coming!) Conceptually you’re assigning the output of this signal to the backgroundColorproperty of the text field. However, the code above is a poor expression of this; it’s all backwards! 从概念上讲，您将此信号的输出分配给文本字段的background的Color属性。 但是，上面的代码对此表达的不好; 一切都倒退了！（不是很好的响应式的表达方式） Fortunately, ReactiveCocoa has a macro that allows you to express this with grace and elegance. Add the following code directly beneath the two signals you added to viewDidLoad: 幸运的是，ReactiveCocoa有一个宏允许你用一种优雅的方式来表达。在你加入viewDidLoad的两个信号下面直接加入下面的代码： RAC(self.passwordTextField, backgroundColor) = [validPasswordSignal map:^id(NSNumber *passwordValid) { return [passwordValid boolValue] ? [UIColor clearColor] : [UIColor yellowColor]; }]; RAC(self.usernameTextField, backgroundColor) = [validUsernameSignal map:^id(NSNumber *passwordValid) { return [passwordValid boolValue] ? [UIColor clearColor] : [UIColor yellowColor]; }]; The RAC macro allows you to assign the output of a signal to the property of an object. It takes two arguments, the first is the object that contains the property to set and the second is the property name. Each time the signal emits a next event, the value that passes is assigned to the given property. 这个RAC宏允许你分配一个signal的输出给一个对象的属性。它使用了两个参数，第一个是那个对象，包含着将要设置的属性，第二个是属性名。每一次signal发射一个next事件，被传递的值就被分配给给定的属性。 This is a very elegant solution, don’t you think? 这是一种非常优雅的解决方案，你不觉得吗？ One last thing before you build and run. Locate the updateUIState method and remove the first two lines: 你构建和运行前，最后一件事情，找到updateUIState方法，移除前面两行： self.usernameTextField.backgroundColor = self.usernameIsValid ? [UIColor clearColor] : [UIColor yellowColor]; self.passwordTextField.backgroundColor = self.passwordIsValid ? [UIColor clearColor] : [UIColor yellowColor]; That will clean up the non-reactive code. 这将清理非响应式的代码。 Build and run the application. You should find that the text fields look highlighted when invalid, and clear when valid. 构建并且运行程序。你可以发现文本域当无效时会变得高亮，有效时高亮会消失。 Visuals are nice, so here is a way to visualize the current logic. Here you can see two simple pipelines that take the text signals, map them to validity-indicating booleans, and then follow with a second mapping to a UIColor which is the part that binds to the background color of the text field. 可视化是很好的，所以这里有一个办法是把这个逻辑可视化。（用图表的方式易于理解。）这里你可以看到两个简单的管道，接收文本signal，把他们map成指示有效的布尔型，然后再map成一个 UIColor，和文本域的background的color属性绑定起来。 Are you wondering why you created separate validPasswordSignal and validUsernameSignal signals, as opposed to a single fluent pipeline for each text field? Patience dear reader, the method behind this madness will become clear shortly! 你是否会有疑问，为什么要创建validPasswordSignal和validUsernameSignal两个信号，而不是为创建一个流畅的管道？请有点耐心，这种处理背后的方法马上就会变得清楚（就会出现了）。 Combining signals 合并信号 In the current app, the Sign In button only works when both the username and password text fields have valid input. It’s time to do this reactive-style! 在当前的程序中，这个Sign In按钮只在username和password两个文本域都有有效的输入时工作。是时候去把这个做成响应式了。 The current code already has signals that emit a boolean value to indicate if the username and password fields are valid; validUsernameSignal and validPasswordSignal. Your task is to combine these two signals to determine when it is okay to enable the button. 当前的代码已经有了signal发射了一个布尔值去指示着username和password域是否有效：validUsernameSignal和validPasswordSignal。你的任务是合并这两个signal去决定什么时候可以让这个按钮开始工作。 At the end of viewDidLoad add the following: 在viewDidLoad底部增加： RACSignal *signUpActiveSignal = [RACSignal combineLatest:@[validUsernameSignal, validPasswordSignal] reduce:^id(NSNumber *usernameValid, NSNumber *passwordValid) { return @([usernameValid boolValue] &amp;&amp; [passwordValid boolValue]); }]; The above code uses the combineLatest:reduce: method to combine the latest values emitted by validUsernameSignal and validPasswordSignal into a shiny new signal. Each time either of the two source signals emits a new value, the reduce block executes, and the value it returns is sent as the next value of the combined signal. 上面的代码使用了combineLatest:reduce:方法去合并由validUsernameSignal和validPasswordSignal发射的最后的值到一个闪亮的新的signal。每一次这两个源signal中的一个发射出一个新值，这个reduce block就执行，并且它返回的值被作为combined signal的next的值。 Note: The RACSignal combine methods can combine any number of signals, and the arguments of the reduce block correspond to each of the source signals. ReactiveCocoa has a cunning little utility class, RACBlockTrampoline that handles the reduce block’s variable argument list internally. In fact, there are a lot of cunning tricks hidden within the ReactiveCocoa implementation, so it’s well worth pulling back the covers! 注意：RACSignal方法可以合并任意数量的信号，reduce block的参数对应于每个源signal。 ReactiveCocoa有一个狡猾的小实用程序类RACBlockTrampoline，它在内部处理reduce block的变量参数列表。 事实上，在ReactiveCocoa实现中隐藏了许多小技巧，（最后一句没搞明白）！ Now that you have a suitable signal, add the following to the end of viewDidLoad. This will wire it up to the enabled property on the button: 现在你有了一个合适的signal， 在viewDidLoad底部增加底下的代码。这会将其连接到按钮上的enabled属性: [signUpActiveSignal subscribeNext:^(NSNumber *signupActive) { self.signInButton.enabled = [signupActive boolValue]; }]; Before running this code, it’s time to rip out the old implementation. Remove these two properties from the top of the file: 在运行这个代码前，是时候移除旧的实现了。在文件顶部移除这两个属性： @property (nonatomic) BOOL passwordIsValid; @property (nonatomic) BOOL usernameIsValid; From near the top of viewDidLoad, remove the following: 在viewDidLoad顶部，移除底下的代码： // handle text changes for both text fields [self.usernameTextField addTarget:self action:@selector(usernameTextFieldChanged) forControlEvents:UIControlEventEditingChanged]; [self.passwordTextField addTarget:self action:@selector(passwordTextFieldChanged) forControlEvents:UIControlEventEditingChanged]; Also remove the updateUIState, usernameTextFieldChanged and passwordTextFieldChanged methods. Whew! That’s a lot of non-reactive code you just disposed of! You’ll be thankful you did. 并且移除updateUIState, usernameTextFieldChanged和passwordTextFieldChanged方法。喔！你刚刚移除了好多非响应式的代码！你将来会感谢你所做的。 Finally, make sure to remove the call to updateUIState from viewDidLoad as well. 最后，确认从viewDidLoad移除掉对updateUIState的调用。 If you build and run, check the Sign In button. It should be enabled because the username and password text fields are valid, as they were before. 如果你构建并且运行了，检查Sign In按钮。如果username和password文本域有效，它将可以使用，就像前面一样。 An update to the application logic diagram gives the following: 更新应用的逻辑图： The above illustrates a couple of important concepts that allow you to perform some pretty powerful tasks with ReactiveCocoa; 上面描绘了两个重要的概念，允许你用ReactiveCocoa去完成一些强大的任务； Splitting – signals can have multiple subscribers and serve as the source for more multiple subsequent pipeline steps. In the above diagram, note that the boolean signals that indicate password and username validity are split and used for a couple of different purposes. Combining – multiple signals may be combined to create new signals. In this case, two boolean signals were combined. However, you can combine signals that emit any value type. 分割 -- signal可以有多个订阅者，可以作为后续管道步骤的源。在上面的图中，注意那个布尔型signal指示这password和 username的有效性被分割并且被用于不同的目的。 合并 -- 多个signal可以合并去创建一个新的signal。在这个例子中，两个布尔型的signal被合并。然后，你可以合并发生任何值类型的signal。 The result of these changes is the application no longer has private properties that indicate the current valid state of the two text fields. This is one of the key differences you’ll find when you adopt a reactive style — you don’t need to use instance variables to track transient state. 这些改变的结果是应用程序不再拥有私有属性来指示这两个文本域当前的这有效性。这是一个关键的不同，你将会发现当你采纳响应式方式，你不再需要实例变量去跟踪瞬时状态。 Reactive Sign-in 响应式 Sign-in The application currently uses the reactive pipelines illustrated above to manage the state of the text fields and button. However, the button press handling still uses actions, so the next step is to replace the remaining application logic in order to make it all reactive! 当前的应用程序使用上面所描述的响应式的管道来管理文本域和按钮的状态。然后，按钮的按下处理仍旧使用了actions， 所以下一步是取代这些剩余的逻辑，来让整个程序都变成响应式。 The Touch Up Inside event on the Sign In button is wired up to the signInButtonTouched method in RWViewController.m via a storyboard action. You’re going to replace this with the reactive equivalent, so you first need to disconnect the current storyboard action. 这个在Sign In按钮上的Touch Up Inside事件被通过storyboard的action绑定在RWViewController.m的signInButtonTouched方法上。你将使用响应式的等价物来要替代这些，所以你先要断开当前的storyboard的action。 Open up Main.storyboard, locate the Sign In button, ctrl-click to bring up the outlet / action connections and click the x to remove the connection. If you feel a little lost, the diagram below kindly shows where to find the delete button: 打开Main.storyboard， 找到Sign In按钮，按下ctrl和左键来打开outlet / action连接，并且点击那个x来移除这个连接。如果你没跟上，看下图，可以找到那个delete按钮。 You’ve already seen how the ReactiveCocoa framework adds properties and methods to the standard UIKit controls. So far you’ve used rac_textSignal, which emits events when the text changes. In order to handle events you need to use another of the methods that ReactiveCocoa adds to UIKit, rac_signalForControlEvents. 你可以看到了ReactiveCocoa框架如何给标准的UIKit控件增加属性和方法了。到现在为止，你已经使用了rac_textSignal， 它会在文本改变时发射事件。为了处理这些事件，你需要使用ReactiveCocoa增加给UIKit的另外一个方法，rac_signalForControlEvents。 Returning to RWViewController.m, add the following to the end of viewDidLoad: 返回RWViewController.m， 在viewDidLoad的地步增加下面的代码： [[self.signInButton rac_signalForControlEvents:UIControlEventTouchUpInside] subscribeNext:^(id x) { NSLog(@"button clicked"); }]; The above code creates a signal from the button’s UIControlEventTouchUpInsideevent and adds a subscription to make a log entry every time this event occurs. 上面的代码从按钮的UIControlEventTouchUpInside创建了一个signal，并且增加了一个订阅，在每一次事件发生时输出一条日志。 Build and run to confirm the message actually logs. Bear in mind that the button will enable only when the username and password are valid, so be sure to type some text into both fields before tapping the button! 构建并且运行代码来确认消息实际上输出了。请记住，只有当用户名和密码有效时才会启用该按钮，因此请务必在点击按钮之前在两个字段中键入一些文本！ You should see messages in the Xcode console similar to the following: 你可以在Xcode的控制台上看到类似下面的输出。 2013-12-28 08:05:10.816 RWReactivePlayground[18203:a0b] button clicked 2013-12-28 08:05:11.675 RWReactivePlayground[18203:a0b] button clicked 2013-12-28 08:05:12.605 RWReactivePlayground[18203:a0b] button clicked 2013-12-28 08:05:12.766 RWReactivePlayground[18203:a0b] button clicked 2013-12-28 08:05:12.917 RWReactivePlayground[18203:a0b] button clicked Now that the button has a signal for the touch event, the next step is to wire this up with the sign-in process itself. This presents something of a problem — but that’s good, you don’t mind a problem, right? Open up RWDummySignInService.h and take a look at the interface: 注意这个按钮有一个touch事件的signal， 下一步就是把它绑定到sign-in过程中去。这提出了一个问题 - 但这很好，你不介意一个问题，对吧？ 打开RWDummySignInService.h并查看界面： typedef void (^RWSignInResponse)(BOOL); @interface RWDummySignInService : NSObject - (void)signInWithUsername:(NSString *)username password:(NSString *)password complete:(RWSignInResponse)completeBlock; @end This service takes a username, a password and a completion block as parameters. The given block is run when the sign-in is successful or when it fails. You could use this interface directly within the subscribeNext: block that currently logs the button touch event, but why would you? This is the kind of asynchronous, event-based behavior that ReactiveCocoa eats for breakfast! 此服务将用户名，密码和完成块作为参数。 登录成功或失败时运行给定的块。 您可以直接在当前记录按钮触摸事件的subscribeNext：块中使用此接口，但为什么会这样？ Note: A dummy service is being used in this tutorial for simplicity, so that you don’t have any dependencies on external APIs. However, you’ve now run up against a very real problem, how do you use APIs not expressed in terms of signals? 注意：为简单起见，本教程中使用了虚拟服务，因此您对外部API没有任何依赖性。 但是，您现在遇到了一个非常现实的问题，您如何使用未表达信号的API？ Creating Signals 创建信号 Fortunately, it’s rather easy to adapt existing asynchronous APIs to be expressed as a signal. First, remove the current signInButtonTouched: method from the RWViewController.m. You don’t need this logic as it will be replaced with a reactive equivalent. 幸运的是，将已经存在的异步API调整表达为一个signal是一件很容易的事情。首先，从RWViewController.m文件中移除当前的signInButtonTouched:方法。你不再需要这个逻辑，因为它将被一个响应式的等价物所替代。 Stay in RWViewController.m and add the following method: 留在RWViewController.m这个文件并且增加下面的方法： -(RACSignal *)signInSignal { return [RACSignal createSignal:^RACDisposable *(id&lt;RACSubscriber&gt; subscriber) { [self.signInService signInWithUsername:self.usernameTextField.text password:self.passwordTextField.text complete:^(BOOL success) { [subscriber sendNext:@(success)]; [subscriber sendCompleted]; }]; return nil; }]; } The above method creates a signal that signs in with the current username and password. Now for a breakdown of its component parts. 上面的方法创建了一个signal，发射于当前的用户名和密码。现在我们分解它的组成部分。 The above code uses the createSignal: method on RACSignal for signal creation. The block that describes this signal is a single argument, and is passed to this method. When this signal has a subscriber, the code within this block executes. 上面的代码使用了RACSignal的createSignal:，为了来创建signal。描述了这个signal的block是一个参数，传递给这个方法。当这个signal有一个订阅者，这个block中的代码将会被执行。block这里所指的是这一段： ^RACDisposable *(id&lt;RACSubscriber&gt; subscriber) { [self.signInService signInWithUsername:self.usernameTextField.text password:self.passwordTextField.text complete:^(BOOL success) { [subscriber sendNext:@(success)]; [subscriber sendCompleted]; }]; return nil; } 这是一个block，返回值是RACDisposable *，参数是id&lt;RACSubscriber&gt; subscriber，实现了RACSubscriber协议的subscriber对象。这个地方需要反复理解block的语法和作用，否则很容易不理解。把block当做一个匿名函数来理解，这里体现的就是函数式编程，传递进去的参数不再是一个变量，而是一个函数，接受这个参数的方法，不再是对这个变量进行赋值、传递，而是对这个函数进行调用。 The block is passed a single subscriber instance that adopts the RACSubscriberprotocol, which has methods you invoke in order to emit events; you may also send any number of next events, terminated with either an error or complete event. In this case, it sends a single next event to indicate whether the sign-in was a success, followed by a complete event. 这个block被传递了一个subscriber实例，实现了RACSubscriber协议，这个协议拥有一些方法，使得你可以调用去发射event；你也可以发射任意数量的next事件，最后终止于一个error或者complete事件。在这个案例中，它发送了一个next事件，指明了sign-in是否成功，跟随者一个complete事件。 The return type for this block is an RACDisposable object, and it allows you to perform any clean-up work that might be required when a subscription is cancelled or trashed. This signal does not have any clean-up requirements, hence nil is returned. 这个block的返回类型是一个RACDisposable对象，它允许你去执行一些清理工作--当一个subscription（订阅）被取消或者被丢弃时需要进行的。这里的这个signal没有清理的需求，所以返回了一个nil。 As you can see, it’s surprisingly simple to wrap an asynchronous API in a signal! 就像你所看到的，在一个signal中包裹一个异步的API是令人惊讶的简单容易。 Now to make use of this new signal. Update the code you added to the end of viewDidLoad in the previous section as follows: 现在来使用这个signal。更新你刚才增加在viewDidLoad底部的代码： [[[self.signInButton rac_signalForControlEvents:UIControlEventTouchUpInside] map:^id(id x) { return [self signInSignal]; }] subscribeNext:^(id x) { NSLog(@"Sign in result: %@", x); }]; The above code uses the map method used earlier to transform the button touch signal into the sign-in signal. The subscriber simply logs the result. 上面的代码使用前面使用过的map方法来把按钮接触的signal转换成一个sign-in的signal，subscriber简单地打印出结果。 If you build and run, then tap the Sign In button, and take a look at the Xcode console, you’ll see the result of the above code … 如果你构造了并且运行，然后点击Sign In按钮，看一下Xcode的控制台，你会看到上面代码的结果... … and the result isn’t quite what you might have expected! ... 并不是你想要的！ 2014-01-08 21:00:25.919 RWReactivePlayground[33818:a0b] Sign in result: &lt;RACDynamicSignal: 0xa068a00&gt; name: +createSignal: The subscribeNext: block has been passed a signal all right, but not the result of the sign-in signal! 这个subscribeNext: block被传递了一个signal，但不是sign-in signal的结果！ Time to illustrate this pipeline so you can see what’s going on: 到了该说明这个管道的时候了，这样你就可以看到发生了什么： The rac_signalForControlEvents emits a next event (with the source UIButton as its event data) when you tap the button. The map step creates and returns the sign-in signal, which means the following pipeline steps now receive a RACSignal. That is what you’re observing at the subscribeNext: step. 当你点击这个按钮的时候，这个rac_signalForControlEvents发射了一个next事件（伴随着源UIButton作为它的事件的数据）。map这一步创建并且返回这个sign-in signal，这意味着后面的管道这一步现在可以接收一个RACSignal。这就是你在 subscribeNext: 这一步所观察到的。 The situation above is sometimes called the signal of signals; in other words an outer signal that contains an inner signal. If you really wanted to, you could subscribe to the inner signal within the outer signal’s subscribeNext: block. However it would result in a nested mess! Fortunately, it’s a common problem, and ReactiveCocoa is ready for this scenario. 上面这种情况有时候被称为 signal of signals （信号的信号）；换句话说，一个外部的signal包含了一个内部的signal。如果你真的想要，你可以订阅在外部的signal的subscribeNext: 块里面的内部的signal。然后它会导致一个嵌套混乱！幸运的是，这是一个很普通的问题，ReactiveCocoa已经为这种场景做好了准备。 Signal of Signals 信号的信号 The solution to this problem is straightforward, just change the map step to a flattenMap step as shown below: 这个问题的解决方案是很直接的，仅仅修改map这一步为flattenMap，就像下面： [[[self.signInButton rac_signalForControlEvents:UIControlEventTouchUpInside] flattenMap:^id(id x) { return [self signInSignal]; }] subscribeNext:^(id x) { NSLog(@"Sign in result: %@", x); }]; This maps the button touch event to a sign-in signal as before, but also flattens it by sending the events from the inner signal to the outer signal. 这把按钮的点击事件映射成一个sign-in信号，并且把它flattens， 通过发送事件从内部的signal 到外部的signal。（flatten是扁平化的意思，这里直接翻译成扁平化，不太好理解，所以还是用原词） Build and run, and keep an eye on the console. It should now log whether the sign-in was successful or not: 编译并且运行，看一下控制台。现在它会输出sign-in是否成功： 2013-12-28 18:20:08.156 RWReactivePlayground[22993:a0b] Sign in result: 0 2013-12-28 18:25:50.927 RWReactivePlayground[22993:a0b] Sign in result: 1 Exciting stuff! 令人兴奋吧！ Now that the pipeline is doing what you want, the final step is to add the logic to the subscribeNext step to perform the required navigation upon successful sign-in. Replace the pipeline with the following: 现在，管道做了你想做的，最后一步是给subscribeNext这一步增加逻辑去执行在成功sign-in后所需要的导航。替换管道如下： [[[self.signInButton rac_signalForControlEvents:UIControlEventTouchUpInside] flattenMap:^id(id x) { return [self signInSignal]; }] subscribeNext:^(NSNumber *signedIn) { BOOL success = [signedIn boolValue]; self.signInFailureText.hidden = success; if (success) { [self performSegueWithIdentifier:@"signInSuccess" sender:self]; } }]; The subscribeNext: block takes the result from the sign-in signal, updates the visibility of the signInFailureText text field accordingly, and performs the navigation segue if required. 这个subscribeNext: 块使用了sign-in signal的结果，因此更新了signInFailureText文本域的可见性，并且执行了需要的导航。 Build and run to enjoy the kitten once more! Meow! 编译并且运行，再一次去享受这个小猫程序吧！喵！ Did you notice there is one small user experience issue with the current application? When the sign-in service validates the supplied credentials, is should disable the Sign In button. This prevents the user from repeating the same sign-in. Furthermore, if a failed sign-in attempt occurred, the error message should be hidden when the user tries to sign-in once again. 你有没有注意到，当前的程序有一个很小的用户体验问题？当sign-in service确认了所支持的凭证，是应该使Sign In按钮无效。这防止了用户重复登录。更进一步，如果一个失败的sign-in尝试发生，错误信息应该在用户尝试再次登录时隐藏。 But how should you add this logic to the current pipeline? Changing the button’s enabled state isn’t a transformation, filter or any of the other concepts you’ve encountered so far. Instead, it’s what is known as a side-effect; or logic you want to execute within a pipeline when a next event occurs, but it does not actually change the nature of the event itself. 但是你应该怎么去给当前的管道增加这个逻辑呢？修改这个按钮的enabled状态并不是一个转换，过滤或者任何你到现在为止接触到的概念。取而代之的，它是一个被称为side-effect副作用，这里没有贬义的意思）的概念。 Adding side-effects 增加副作用 Replace the current pipeline with the following: 像下面这样修改管道： [[[[self.signInButton rac_signalForControlEvents:UIControlEventTouchUpInside] doNext:^(id x) { self.signInButton.enabled = NO; self.signInFailureText.hidden = YES; }] flattenMap:^id(id x) { return [self signInSignal]; }] subscribeNext:^(NSNumber *signedIn) { self.signInButton.enabled = YES; BOOL success = [signedIn boolValue]; self.signInFailureText.hidden = success; if (success) { [self performSegueWithIdentifier:@"signInSuccess" sender:self]; } }]; You can see how the above adds a doNext: step to the pipeline immediately after button touch event creation. Notice that the doNext: block does not return a value, because it’s a side-effect; it leaves the event itself unchanged. 你可以看到上面是如何在按钮点击事件创建之后添加一个doNext:步骤给管道的。注意这个doNext:块不返回值，因为它是一个副作用；它留下了没有修改的事件。 The doNext: block above sets the button enabled property to NO, and hides the failure text. Whilst the subscribeNext: block re-enables the button, and either displays or hides the failure text based on the result of the sign-in. 这个doNext:块设置了按钮的enabled属性为NO，并且隐藏了失败的文本。同时subscribeNext:重新使按钮生效，并且根据 sign-in的结果或者显示或者隐藏失败的文本。 It’s time to update the pipeline diagram to include this side effect. Bask in all it’s glory: 是时候更新管道的图来包含这个副作用了。（Bask in all it’s glory这个意思没有翻译，还没有完全搞清楚） Build and run the application to confirm the Sign In button enables and disables as expected. 编译并且运行这个程序去确认这个Sign In button像所期待的那样有效或者无效。 And with that, your work is done – the application is now fully reactive. Woot! 并且，伴随着这个，你的工作也完成了-- 这个程序已经是完全的响应式了。 If you got lost along the way, you can download the final project (complete with dependencies), or you can obtain the code from GitHub, where there is a commit to match each build and run step in this tutorial. 如果你没有跟上文章的思路，你可以下载final project最后的完整的工程），或者你可以从GitHub上去获取代码，里面有一个提交是匹配这篇知道里面的每一个编译和运行步骤。 Note: Disabling buttons while some asynchronous activity is underway is a common problem, and once again ReactiveCocoa is all over this little snafu. The RACCommand encapsulates this concept, and has an enabled signal that allows you to wire up the enabled property of a button to a signal. You might want to give the class a try. 注意：在进行某些异步活动时禁用按钮是一个常见问题，and once again ReactiveCocoa is all over this little snafu（后一句暂不会翻译）。这个RACCommand封装了这个概念，有一个enabled signal允许你连接一个按钮的enabled属性到一个 signal。You might want to give the class a try（）。 Conclusions 结论 Hopefully this tutorial has given you a good foundation that will help you when starting to use ReactiveCocoa in your own applications. It can take a bit of practice to get used to the concepts, but like any language or program, once you get the hang of it it’s really quite simple. At the very core of ReactiveCocoa are signals, which are nothing more than streams of events. What could be simpler than that? 希望这个指导能给你一个很好的基础，帮助你开始在你的程序中使用ReactiveCocoa 。这需要你采用更多的练习去熟悉这些概念，就像任何语言或者程序一样，一旦你掌握了，它实际上很简单。ReactiveCocoa的核心是信号，它们只不过是事件流。 有什么比这更简单？ With ReactiveCocoa one of the interesting things I have found is there are numerous ways in which you can solve the same problem. You might want to experiment with this application, and adjust the signals and pipelines to change the way they split and combine. 采用ReactiveCocoa， 一个有意思的事情是，将会有很多种方法你来解决一个问题。你可以在这个程序里面去实验它，调整信号和管道的拆分和合并的方式。 It’s worth considering that the main goal of ReactiveCocoa is to make your code cleaner and easier to understand. Personally I find it easier to understand what an application does if its logic is represented as clear pipelines, using the fluent syntax. 值得思考的是，ReactiveCocoa的主要目标是让你的代码干净和容易被理解。就我个人而言，如果使用流畅的语法将其逻辑表示为清晰的管道，我会发现更容易理解应用程序的作用。 In the second part of this tutorial series you’ll learn about more advanced subjects such as error handing and how to manage code that executes on different threads. Until then, have fun experimenting! 在这篇指导系列的第二部分，你将会学到更多高级的课题，例如错误处理和如何去管理代码去在不同的线程上执行。在那之前，开心地实验吧！]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>OC</tag>
        <tag>ReactiveObjC</tag>
        <tag>ReactiveCocoa</tag>
        <tag>KVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RAC学习笔记之一：KVC（一）]]></title>
    <url>%2Fios%2Fios-oc-kvc-1.html</url>
    <content type="text"><![CDATA[摘要RAC学习笔记之一：KVC（一） 博客原帖收藏于IT老兵博客。 前言学习RAC，需要先去了解KVC和KVO，这里对KVC进行一下总结。 正文KVC参考苹果官网的介绍 You typically use accessor methods to gain access to an object’s properties. A get accessor (or getter) returns the value of a property. A set accessor (or setter) sets the value of a property. In Objective-C, you can also directly access a property’s underlying instance variable. Accessing an object property in any of these ways is straightforward, but requires calling on a property-specific method or variable name. As the list of properties grows or changes, so also must the code which accesses these properties. In contrast, a key-value coding compliant object provides a simple messaging interface that is consistent across all of its properties.Key-value coding is a fundamental concept that underlies many other Cocoa technologies, such as key-value observing, Cocoa bindings, Core Data, and AppleScript-ability. Key-value coding can also help to simplify your code in some cases. 一言以蔽之，这一段的意思是编写传统的获取和设置属性的方法（getter和setter）会随着属性值的增加而变得越来越枯燥，所以OC提供了一种直接根据属性名设置值的方法，这种方法就叫做KVC，Key-value coding。 下面摘录一段李刚的《疯狂iOS》，这段例子讲的很清楚，所以摘录在下面： 5.5.1 简单的 KVC最基本的 KVC 由 NSKeyValueCoding 协议提供支持，最基本的操作属性的两个方法如下。setValue：属性值 forKey：属性名：为指定属性设置值。valueForKey：属性名：获取指定属性的值。例如，如下程序定义了 FKUser 类的接口部分。程序清单: codes/05/5.5/FKUser. H1234567#import &lt;Foundation/ Foundation. H&gt;@interface FKUser: NSobject//使用@property 定义 3 个 property@property (nonatomic, copy) NSString* name;@property (nonatomic, copy) NSString* pass;@property (nonatomic, copy) NSDate* birth;@end 上面程序定义了 FKUser 类使用@property 定义 3 个 property，接下来应该为该类定义实现部分，实现部分使用@synthesize 为这 3 个 property 合成 setter 和 getter 方法。类实现部分代码非常简单，故此处不再给出。下面使用 KVC 来设置 FKUser 对象的属性，以及访问 FKUser 对象的属性。程序代码如下:123456789101112131415161718程序清单: codes/05/5.5/FKUserTest.m#import &quot;FKUser. H&quot;int main (int argc，char★argv [])&#123; @autoreleasepool &#123; //创建 FKUser 对象 FKUser* user = [[FKUser alloc] init]; // 使用 KVC 方式为 name 属性设置属性值 [user setValue: @“孙悟空”forKey: @&quot;name&quot;]; //使用 KVC 方式为 pass 属性设置属性值 [user setValue: @&quot;1455&quot; forKey: @&quot;pass&quot;]; //使用 KVC 方式为 birth 属性设置属性值 [user setValue: [[NSDate alloc] init] forKey: @&quot;birth&quot;]; //使用KVC 获取 FKUser 对象的属性 NSLog (@&quot;user 的 name 为：%@“，【user valueForKey: @&quot;name&quot;]); NSLog (@&quot;user 的 pass 为：%@“，【user valueForKey: @&quot;pass&quot;]); NSLog (@&quot;user 的 birth 为：%@”，【user valueForKey: @&quot;birth&quot;]); 上面程序中前 3 行粗体字代码使用 KVC 方式为属性设置属性值，后 3 行粗体字代码使用 KVC 模式获取指定属性的值。编译、运行上面的程序，看到如下输出：123user 的 name 为：孙悟空user 的 pass 为：1455user 的 birth 为：2013-04-08 16:55:49 +0000 在 KVC 编程方式中，无论调用 setValue: forKey 方法，还是调用 valueForKey：方法，都是通过 NSString 对象来指定被操作属性的，其中 forKey：标签用于传入属性名。对于 setValue：属性值 forKey@”name“；代码，底层的执行机制如下。 (1) 程序优先考虑调用“setName：属性值；”代码通过 setter 方法完成设置。 (2) 如果该类没有 setName：方法，KVC 机制会搜索该类名为_ name 的成员变量，无论该成员变量是在类接口部分定义，还是在类实现部分定义，也无论用哪个访问控制符修饰，这条 KVC 代码底层实际上就是对 name 成员变量赋值。 (3) 如果该类既没有 setName：方法，也没有定义_ name 成员变量，KVC 机制会搜索该类名为 name 的成员变量，无论该成员变量是在类接口部分定义，还是在类实现部分定义，也无论用哪个访问控制符修饰，这条 KVC 代码底层实际就是对 name 成员变量赋值。 (4) 如果上面 3 条都没有找到，系统将会执行该对象的 setValue: forUndefinedKey：方法。默认的 setValue: forUndefinedKey：方法实现就是引发一个异常，这个异常将会| 导致程序因为异常结束。对于“valueforKey@”name”；，”代码，底层的执行机制如下。 (1) 程序优先考虑调用“name；”代码来获取该 getter 方法的返回值。 (2) 如果该类没有 name 方法，KVC 机制会搜索该类名为_ name 的成员变量，无论该成员变量是在类接口部分定义，还是在类实现部分定义，也无论用哪个访问控制符修饰，这条 KVC代码底层实际就是返回_ name 成员变量的值。 (3) 如果该类既没有 name 方法，也没有定义_ name 成员变量，KVC 机制会搜索该类名为name 的成员变量，无论该成员变量是在类接口部分定义，还是在类实现部分定义，也无论用哪个访问控制符修饰，这条 KVC 代码底层实际就是返回 name 成员变量的值。 (4) 如果上面 3 条都没有找到，系统将会执行该对象的 valueforUndefinedKey：方法。 总结KVC的内容有点多，一章总结不完，这篇文章基本都是摘录了，因为李刚的文章写得相当清楚，那就通过摘录来加深印象。 参考https://developer.apple.com/documentation/foundation/object_runtime/nskeyvaluecoding?language=objchttps://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/KeyValueCoding/index.htmlhttps://www.objc.io/issues/7-foundation/key-value-coding-and-observing/]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>OC</tag>
        <tag>ReactiveObjC</tag>
        <tag>ReactiveCocoa</tag>
        <tag>KVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java：URL rewrite]]></title>
    <url>%2Fjava%2Furl-rewrite.html</url>
    <content type="text"><![CDATA[Java：URL rewrite 原帖收藏于IT老兵博客。 前言在工作中不断遇到url rewrite这个概念，这个概念和redirect到底有什么区别，什么时候适用，需要总结一下。 正文介绍很神奇的是，通过google查这个关键字，维基百科的解释，更多偏向于path info的概念，而不是我所理解的那个概念。 最早接触url rewrite是在配置apache服务器的时候，参考这里，可以对request的url进行重写，把实际需要的内容返回给客户端，并且把客户端的url改变。 以下是一些摘录： SummaryThe mod_rewrite module uses a rule-based rewriting engine, based on a PCRE regular-expression parser, to rewrite requested URLs on the fly. By default, mod_rewrite maps a URL to a filesystem path. However, it can also be used to redirect one URL to another URL, or to invoke an internal proxy fetch.mod_rewrite provides a flexible and powerful way to manipulate URLs using an unlimited number of rules. Each rule can have an unlimited number of attached rule conditions, to allow you to rewrite URL based on server variables, environment variables, HTTP headers, or time stamps.mod_rewrite operates on the full URL path, including the path-info section. A rewrite rule can be invoked in httpd.conf or in .htaccess. The path generated by a rewrite rule can include a query string, or can lead to internal sub-processing, external request redirection, or internal proxy throughput. 这里还有一些对于rewrite的作用介绍： 1，首先是满足观感的要求。对于追求完美主义的网站设计师，就算是网页的地址也希望看起来尽量简洁明快。形如http://www.123.com/news/index.asp?id=123的网页地址，自然是毫无美感可言，而用UrlRewrite技术，你可以轻松把它显示为 http://www.123.com/news/123.html。2，其次可以隐藏网站所用的编程语言，还可以提高网站的可移植性。 当网站每个页面都挂着鲜明的.asp/.aspx/.php这种开发语言的标记，别人一眼即可看出你的网站是用什么语言做的。而且在改变网站的语言的时候，你需要改动大量的链接。而且，当一个页面修改了扩展名，它的pagerank也会随之消失，从头开始。我们可以用UrlRewrite技术隐藏我们的实现细节，这样修改移植都很方便，而且完全不损失pagerank。提高安全性，可以有效的避免一些参数名、ID等完全暴露在用户面前，如果用户随便乱输的话，不符合规则的话直接会返回个404或错误页面，这比直接返回500或一大堆服务器错误信息要好的多3，最后也是最重要的作用，是有利于搜索引擎更好地抓取你网站的内容。理论上，搜索引擎更喜欢静态页面形式的网页，搜索引擎对静态页面的评分一般要高于动态页面。所以，UrlRewrite可以让我们网站的网页更容易被搜索引擎所收录。 效率参考这里，主要的一点，url rewrite比redirect更有效率，因为它没有让客户端进行二次请求，而是直接返回了需要的内容，这一点其实有一点像java的forward。 支持Apache是支持rewrite的，Nginx参考的是apache，所以也是支持的，而java的服务器一直对此没有很好的支持，可能也是源自上面的原因，这里介绍了一个包，来使java的web server可以支持这个rewrite功能。 参考http://httpd.apache.org/docs/current/mod/mod_rewrite.htmlhttps://visualstudiomagazine.com/blogs/tool-tracker/2018/06/url-rewriting.aspxhttp://tuckey.org/urlrewrite/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>url</tag>
        <tag>rewrite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS：初识SASS]]></title>
    <url>%2Fcss%2Fcss-sass-introduction.html</url>
    <content type="text"><![CDATA[CSS的文档流介绍。 原帖完整收藏于IT老兵驿站，并会不断更新。 前言在过去十几年的IT工作中，对CSS的接触不是特别多，只知道它大体是做什么的，直到这两年开发越来越靠近前端，开始越来越频繁地接触CSS，突然发现有一个叫SASS的东西，非常流行，要了解一下这个SASS是个什么东西，这篇文章就是对这个SASS整体的概念有一个了解。 正文这个应该是它的官网。 Sass is the most mature, stable, and powerful professional grade CSS extension language in the world. Sass是世界上最成熟，稳定、最强大的专业级的CSS扩展语言。 它存在的价值呢？参考这里： 什么是 ScssScss 是 CSS 的扩展， 在保证兼容性的基础上， 允许使用变量、 嵌套、 混合、 导入等特性， 在编写大量的 CSS 文件时很有帮助。特色完全兼容 CSS3在语法上扩展了变量、 嵌套以及混合等操作颜色以及其它的有用的函数高级特性， 比如针对类库的控制声明格式良好并且可控制的输出Firebug 集成 前端的知识点参考，当然少不了阮一峰的文章： 一、什么是SASS SASS是一种CSS的开发工具，提供了许多便利的写法，大大节省了设计者的时间，使得CSS的开发，变得简单和可维护。 大体说的意思差不多，印证了，SASS其实是一种CSS开发工具，或者说是“预处理器”，使用它，你可以使用一些比较方便的语法来对CSS进行编程，而不再使用CSS原始的那种配置型的语言—-那种不能算是编程—-然后通过预处理来翻译成正常的CSS。类似的工具好像还有Less，等回头有时间对这二者进行一下比较。 参考以下是收集整理的一些Sass的网站和文章https://sass-lang.com/ 官网http://www.ruanyifeng.com/blog/2012/06/sass.html 阮一峰的博客http://sass.bootcss.com/docs/sass-reference/ Sass中文的一个网站https://www.sass.hk/ Sass中文的一个网站https://beginor.github.io/2014/07/11/introduction-to-scss.htmlhttp://sass.bootcss.com/docs/sass-reference/ 一些Sass的介绍https://medium.com/@awesome_sayrah/introduction-to-sass-scss-and-less-7ff7e494e798 这篇文章还没有完全看完]]></content>
      <categories>
        <category>CSS</category>
      </categories>
      <tags>
        <tag>CSS</tag>
        <tag>SASS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[席慕蓉《一棵开花的树》读后杂感]]></title>
    <url>%2Fessay%2Fnote-ximurong-tree.html</url>
    <content type="text"><![CDATA[席慕蓉《一棵开花的树》读后杂感。 一棵开花的树如何让你遇见我在我最美丽的时刻 为这我已在佛前求了五百年求佛让我们结一段尘缘佛於是把我化做一棵树长在你必经的路旁 阳光下慎重地开满了花朵朵都是我前世的盼望 当你走近请你细听那颤抖的叶是我等待的热情 而当你终於无视地走过在你身後落了一地的朋友啊那不是花瓣那是我凋零的心 读完这篇散文，感觉眼眶都湿了，这样是什么样的语言功底，居然能够沁入你的身体，让你无比感动，眼眶发湿，引起了内心的共鸣。 今天周五，稍微有些懈怠，百无聊赖，已经有些厌倦无限的下拉《今日头条》来打发时间，突然想看看小说，看看这些文字的东西，看了一篇金庸的《倚天屠龙记》，看了一篇李敖的《传统下的独白》，然后看到这篇席慕蓉先生的散文，看完后，心潮澎湃，好像已经很久没有体会到这种通过阅读而获得的感动了，感觉被麻木、僵硬、厚重的神经包裹下的那很久没有被触达的敏感的灵魂，深深地被打动，又开始去思考人生、爱情和婚姻。 尽管一直在记录笔记，已经记录了十余年了，但是直到此刻，开始反省，似乎我更多记录的，是技术的学习和整理，是产品的分析，都是一些说明性的问题，已经很久没有用语言去表达心中的感受，表达这种激动的感觉，似乎这种能力已经快退化了，总感觉言不达意，怎么都不能把心中的那份感觉表达出来。 和爱人的相识，已经过去了二十余年，现在的她已经有了白头发，有了雀斑，有了一些鱼尾纹。 而与她初始的时候，也正是在她最美丽的时候，那时的她，就像一个含苞待放的花朵，就像娇艳欲滴的苹果–我已经不能很好地表达我的感情–让我看到了她最美丽的样子，这原来是她前世在佛前求了五百年换来的，想到这里，就觉得感动。 周星驰对柴静说，他这辈子应该没有啥机会去结婚了，很多人留言，他还不算老，怎么这么说呢。 我在想，也许是他发现，他终于知道谁是他心中最爱的人，但是，这辈子可能都不能和他在一起了，我原来以为是朱茵，但现在感觉，可能不是。 看徐峥的《催眠大师》，徐峥对莫文蔚说，“无论你多么爱你的男朋友，但是你要知道，你再也不可能见到他了”，仔细思考这句话，这会是一种怎么样的痛苦啊，我小时候，假期会去姥姥家、奶奶家过，等到开学的时候要回自己家，那个时候我会哭的稀里哗啦，因为一下子就见不到宠爱我的老人们了，但那仅仅是暂时的，再过一个假期，就又能见到了，或者说，我知道，她们在另外一个城市生活，我想见她们，是有机会见到的。但是，有的人，你再也没法见到他了，无论你付出怎么样的努力，你都无法突破生与死的这个界限，你最多只能在梦中去和他们相见，这是一种把心放在绞肉机里面，而且你还能真切体会到感受的一种痛苦。 真爱、尘缘、轮回、生与死，我可能永远也参不透，那就好好珍惜吧。]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>席慕蓉</tag>
        <tag>一棵开花的树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue：学习笔记（七）-自定义指令]]></title>
    <url>%2Fjs%2Fjs-vue-note-directive.html</url>
    <content type="text"><![CDATA[Vue：学习笔记（七）-自定义指令。 提醒原帖完整收藏于IT老兵驿站，并会不断更新。 前言前面总结到了组件，对混入也进行了研究，不过感觉没有啥需要总结的，就先总结指令吧，参考这里，记录笔记。 正文简介 全局注册12345678// 注册一个全局自定义指令 `v-focus`Vue.directive(&apos;focus&apos;, &#123; // 当被绑定的元素插入到 DOM 中时…… inserted: function (el) &#123; // 聚焦元素 el.focus() &#125;&#125;) 这里这个focus方法不太熟，查一下手册，原来是用来获取焦点的方法。 局部注册。12345678directives: &#123; focus: &#123; // 指令的定义 inserted: function (el) &#123; el.focus() &#125; &#125;&#125; 钩子函数在某个生命周期被回调，就像Vue自己的生命周期，Android的生命周期，暴露一些接口给用户。 函数简写在很多时候，你可能想在 bind 和 update 时触发相同行为，而不关心其它的钩子。比如这样写:123Vue.directive(&apos;color-swatch&apos;, function (el, binding) &#123; el.style.backgroundColor = binding.value&#125;) 这段没有看懂–明白了，上面这段是最简单的一个自定义指令的写法。 对象字面量这个字面量又不懂了，还需要参考手册。 字面量：These are fixed values, not variables, that you literally provide in your script.。这个是翻译过来的意思：字面量是由语法表达式定义的常量；或，通过由一定字词组成的语词表达式定义的常量。 初步理解，字面量是一个常量，符合了一定的语法，符合了数组的语法，就是数组字面量，符合了对象的语法，就是对象字面量，大体理解成这样，看了手册半天，感觉还是没有完全理解。 总结对于字面量的概念，还需要继续研究，反复理解，将来再补充。对于指令的理解，感觉大体都明白了。 参考https://cn.vuejs.org/v2/guide/custom-directive.htmlhttps://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/focus]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>JS</tag>
        <tag>Vue</tag>
        <tag>自定义指令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法和实例总结：du]]></title>
    <url>%2Flinux%2Fshell-command-du.html</url>
    <content type="text"><![CDATA[概要Linux下shell命令用法和实例总结：du。 博客IT老兵博客。 正文命令格式 du [选项] [文件] 命令功能 du（disk usage 的简称）命令用于检查计算机上文件和目录的磁盘使用情况，可以递归显示文件和目录。显示每个文件和目录的磁盘使用空间。 命令参数 -a或-all：显示所有文件的大小，不仅仅是目录。 -b或-bytes：显示目录或文件大小时，以byte为单位。 -c或–total：除了显示所有目录或文件的大小外，同时也显示所有目录或文件的总和。 -k或–kilobytes：以KB(1024bytes)为单位输出。 -m或–megabytes：以MB为单位输出。 -s或–summarize：仅显示总计，只列出最后加总的值。 -h或–human-readable：以K，M，G为单位，提高信息的可读性。 -x或–one-file-xystem：以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。 -L&lt;符号链接&gt;或–dereference&lt;符号链接&gt;：显示选项中所指定符号链接的源文件大小。 -S或–separate-dirs：显示个别目录的大小时，并不含其子目录的大小。 -X&lt;文件&gt;或–exclude-from=&lt;文件&gt;：在&lt;文件&gt;指定目录或文件。 –exclude=&lt;目录或文件&gt;：略过指定的目录或文件。 -D或–dereference-args ：显示指定符号链接的源文件大小。 -H或–si：与-h参数相同，但是K，M，G是以1000为换算单位。 -l或–count-links：重复计算硬件链接的文件。 实用命令实例： 找出 /root 目录树及其每个子目录的磁盘使用情况摘要描述：以下命令的输出显示了 /root 目录以及其子目录的磁盘块数。命令：du /root 实例： 以人类可读格式也就是 kb、mb 等显示文件/目录大小命令：du -h /root 实例： 目录的总磁盘使用大小摘要（s 表示 summary，总体概述一下）命令：du -sh /root 实例： 所有文件和目录的磁盘使用情况命令：du -a /root 实例： 总的使用磁盘空间描述：-c选项在最后一行提供了一个总的使用磁盘空间。命令：du -c /root 实例： 排除给定模式的文件或目录描述：在计算/root的总大小时排除.ssh文件。命令：du -h –exclude=”.ssh” /root 参考http://man7.org/linux/man-pages/man1/du.1.html]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>du</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL：DDL的一些整理]]></title>
    <url>%2Fmysql%2Fmysql-ddl-handy-book.html</url>
    <content type="text"><![CDATA[完整的MongoDB学习笔记位于IT老兵博客。 MySQL：DDL的一些整理。 前言对于MYSQL对于表结构的修改，一直存在一些函数的地方，这里，参考一篇外贴，做一下好好的整理。 正文 MySQL ALTER TABLE: ALTER vs CHANGE vs MODIFY COLUMNWhenever I have to change a column in MySQL (which isn’t that often), I always forget the difference between ALTER COLUMN, CHANGE COLUMN, and MODIFY COLUMN. Here’s a handy reference. 这哥们也遇到了对于alter，change和modify的困扰，所以做了一个整理。 ALTER COLUMNUsed to set or remove the default value for a column. Example:ALTER TABLE MyTable ALTER COLUMN foo SET DEFAULT ‘bar’;ALTER TABLE MyTable ALTER COLUMN foo DROP DEFAULT; alter 常常用来设置或者移除一列的默认值。 CHANGE COLUMNUsed to rename a column, change its datatype, or move it within the schema. Example:ALTER TABLE MyTable CHANGE COLUMN foo bar VARCHAR(32) NOT NULL FIRST;ALTER TABLE MyTable CHANGE COLUMN foo bar VARCHAR(32) NOT NULL AFTER baz; change用来重命名一列，修改数据类型，或者在模式中移动它。 MODIFY COLUMNUsed to do everything CHANGE COLUMN can, but without renaming the column. Example:ALTER TABLE MyTable MODIFY COLUMN foo VARCHAR(32) NOT NULL AFTER baz;The official documentation for ALTER TABLE (for MySQL 5.1) is here. modify除了不能重命名一个列，可以做change的所有工作。 官网的摘录： CHANGE:Can rename a column and change its definition, or both.Has more capability than MODIFY, but at the expense of convenience for some operations. CHANGE requires naming the column twice if not renaming it.With FIRST or AFTER, can reorder columns.MODIFY:Can change a column definition but not its name.More convenient than CHANGE to change a column definition without renaming it.With FIRST or AFTER, can reorder columns.ALTER: Used only to change a column default value. 参考https://hoelz.ro/ref/mysql-alter-table-alter-change-modify-columnhttps://dev.mysql.com/doc/refman/5.7/en/alter-table.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>modify</tag>
        <tag>change</tag>
        <tag>alter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：真实 merge]]></title>
    <url>%2Fgit%2Fgit-git-merge-true-merge.html</url>
    <content type="text"><![CDATA[前言Git：真实 merge 是一种 merge 的方式，除去真实 merge，肯定还有不真实的 merge，就是那种 FF （FAST-FORWARD MERGE）的 merge，这个 FF，曾经出现在在收音机上，出现在录像机上，出现在视频播放器上，就是快进的意思。 博客IT老兵驿站。 前言这里准备碎片化地去解读和理解 Git 的一些功能。关于 git-merge 的总结一直没有做，但是几乎每天都会遇到 git-merge，而且会遇到很多随着 merge 而产生的问题，所以只好碎片化地去做一做整理。 正文git-merge 有两种 merge 方式，ff 方式和 true merge 方式，关于 ff 的方式，另外一篇文章有讲过，这里不再赘述，这里整理一下 true merge，真实 merge。 介绍先摘录一段： TRUE MERGEExcept in a fast-forward merge (see above), the branches to be merged must be tied together by a merge commit that has both of them as its parents. 真实 merge 是真的去 merge 一下，而不是像 ff 那样，只是把一个 commit 放过来，从这个角度来说，ff 不是有点像 cherry-pick 吗？ A merged version reconciling the changes from all branches to be merged is committed, and your HEAD, index, and working tree are updated to it. It is possible to have modifications in the working tree as long as they do not overlap; the update will preserve them. 一个 merge 的版本融合了（reconcile 的意思是调节，放在这里有点不好理解，所以解释为融合）所有要被 merge 的分支的改变，并且会被提交，这样你的 HEAD、index、working tree 都会被更新。 When it is not obvious how to reconcile the changes, the following happens:The HEAD pointer stays the same.The MERGE_HEAD ref is set to point to the other branch head.Paths that merged cleanly are updated both in the index file and in your working tree.For conflicting paths, the index file records up to three versions: stage 1 stores the version from the common ancestor, stage 2 from HEAD, and stage 3 from MERGE_HEAD (you can inspect the stages with git ls-files -u). The working tree files contain the result of the “merge” program; i.e. 3-way merge results with familiar conflict markers &lt;&lt;&lt; === &gt;&gt;&gt;. 3路 merge，公共祖先、HEAD、MERGE_HEAD（是指另外那个分支的 HEAD） 这三个方面来 merge。 这里是三个版本的关系，公共祖先的版本、HEAD（本地仓库的版本）、MERGE_HEAD（另外一个分支想要merge过来的版本），所以叫3-way merge，三路合并。 If you tried a merge which resulted in complex conflicts and want to start over, you can recover with git merge –abort. 当你搞不定了，使用 git merge --abort 回滚吧。 关于HEAD、index、worktree、local repository、remote repository的关系，请参考这里，这个挺重要，随后要整理一下。 合并策略： MERGE STRATEGIESThe merge mechanism (git merge and git pull commands) allows the backend merge strategies to be chosen with -s option. Some strategies can also take their own options, which can be passed by giving -X arguments to git merge and/or git pull. git merge 和 git pull 命令使用的 merge 机制允许通过 -s 选项后面的 merge 策略被选择。 resolveThis can only resolve two heads (i.e. the current branch and another branch you pulled from) using a 3-way merge algorithm. It tries to carefully detect criss-cross merge ambiguities and is considered generally safe and fast. recursiveThis can only resolve two heads using a 3-way merge algorithm. When there is more than one common ancestor that can be used for 3-way merge, it creates a merged tree of the common ancestors and uses that as the reference tree for the 3-way merge. This has been reported to result in fewer merge conflicts without causing mismerges by tests done on actual merge commits taken from Linux 2.6 kernel development history. Additionally this can detect and handle merges involving renames, but currently cannot make use of detected copies. This is the default merge strategy when pulling or merging one branch.The recursive strategy can take the following options:oursThis option forces conflicting hunks to be auto-resolved cleanly by favoring our version. Changes from the other tree that do not conflict with our side are reflected to the merge result. For a binary file, the entire contents are taken from our side.This should not be confused with the ours merge strategy, which does not even look at what the other tree contains at all. It discards everything the other tree did, declaring our history contains all that happened in it.theirsThis is the opposite of ours; note that, unlike ours, there is no theirs merge strategy to confuse this merge option with.…… 使用-X&lt;option&gt;参数，可以指定合并策略，上面摘录了两种，一种是 resolve，一种是recursive，第一种策略看上去似乎是可以自动解决冲突，第二种是 Git 默认的 merge 策略，会产生一些少量的冲突，而不会进行错误的合并，它还有几个选项，就是合并时，只选择本地的（ours），或者只选择别人的（theirs）。 参考https://git-scm.com/docs/git-mergehttps://stackoverflow.com/questions/3689838/whats-the-difference-between-head-working-tree-and-index-in-git]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git merge</tag>
        <tag>true merge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：merge的时候全部采用某一个端的文件]]></title>
    <url>%2Fgit%2Fgit-merge-choose-one-side-code.html</url>
    <content type="text"><![CDATA[前言Git：merge的时候全部采用某一个端的文件。 这个帖子原本感觉没有完全搞明白这个地方。 博客IT老兵驿站。 正文在Git使用过程中，有的时候进行 merge，可能需要会全部采用某一端的文件，换句话说，就是完全采用本地的，或者完全采用远程的，怎么实现这个功能呢？ 使用 merge 命令：1234# keep remote filesgit merge --strategy-option theirs# keep local filesgit merge --strategy-option ours 官网是这样写的 --strategy-option=&lt;option&gt;Pass merge strategy specific option through to the merge strategy. 这里的策略选项可以参考另外一篇文章《Git：真实merge》。 —-其实上面写的是不完全对的，上面这个只是配置在某一种 merge 策略下的 option，另外一篇笔记《Git：git-merge 的用法总结》后面对这些策略和选项再做一次完整的总结。 git-pull 也有同样的功能：1git pull -X theirs 原理是一样的。 参考https://stackoverflow.com/questions/6650215/how-to-keep-the-local-file-or-the-remote-file-during-merge-using-git-and-the-comhttps://stackoverflow.com/questions/10697463/resolve-git-merge-conflicts-in-favor-of-their-changes-during-a-pull]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git merge</tag>
        <tag>冲突</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：merge后如何检查是否还存在冲突没有处理]]></title>
    <url>%2Fgit%2Fgit-merge-exist-conflict.html</url>
    <content type="text"><![CDATA[概要Git：merge 后如何检查是否还存在冲突没有处理 博客原帖收藏于IT老兵博客 前言在工作中，遇到一个问题，在git merge后，发生冲突，而当冲突较多的时候，逐个检查冲突，有的时候会遗漏一些文件，导致带有冲突标记的文件上传到了 Git 服务器上，如何解决这个问题呢？ 正文使用以下命令可以快速检查是否还存在有带有冲突标记的文件。git diff --check 说明： –checkWarn if changes introduce conflict markers or whitespace errors. What are considered whitespace errors is controlled by core.whitespace configuration. By default, trailing whitespaces (including lines that consist solely of whitespaces) and a space character that is immediately followed by a tab character inside the initial indent of the line are considered whitespace errors. Exits with non-zero status if problems are found. Not compatible with –exit-code. 上文摘录自手册官网，这个命令会检查 conflict markers（冲突标记）和whitespace errors（空格错误）。 总结Git 是一个相对比较复杂的工具，我发现，复杂往往源自于过于灵活，它的功能非常强大，掌握好了，就可以事半功倍，但是，把它掌握好，是需要花费一些时间成本的。感觉学习 Git，可能需要既需要正面地一篇篇去阅读手册，也需要这样涓涓细流地不断总结各种场景下的用法，也许，这本身就是学习的一种方法。 参考https://git-scm.com/docs/git-diffhttps://ardalis.com/detect-git-conflict-markers]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git merge</tag>
        <tag>冲突</tag>
        <tag>存在</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue：学习笔记（六）-组件基础]]></title>
    <url>%2Fjs%2Fjs-vue-note-components.html</url>
    <content type="text"><![CDATA[Vue：学习笔记（六）-组件基础。 提醒原帖完整收藏于IT老兵驿站，并会不断更新。 前言最近在工作中频繁遇到组件，所以就没按照顺序，先总结组件这一章节的内容了，将来再做调整和修改。 （2018-10-24注：这一章粗读了两遍，感觉下面有好些内容都没有理解，有一些难度，看不明白就先修改一会儿，先干点别的。） 正文基本示例这是一个例子：123456789// 定义一个名为 button-counter 的新组件Vue.component(&apos;button-counter&apos;, &#123; data: function () &#123; return &#123; count: 0 &#125; &#125;, template: &apos;&lt;button v-on:click=&quot;count++&quot;&gt;You clicked me &#123;&#123; count &#125;&#125; times.&lt;/button&gt;&apos;&#125;) 在我看来，组件就是模板、数据，加上对于数据处理的逻辑，这些显示和逻辑，基本是固定的，所以以组件的形式固化出来。（感觉，这话一说出来，立刻就清楚了） 123&lt;div id=&quot;components-demo&quot;&gt; &lt;button-counter&gt;&lt;/button-counter&gt;&lt;/div&gt; 1new Vue(&#123; el: &apos;#components-demo&apos; &#125;) id等于“components-demo”是Vue对象的根元素，在它下面创建组件。 组件的复用这里是将把组件应该理解成类，每一个实例是对象，对象之间是没有关系的。 data 必须是一个函数只有将data实现为一个函数，才能实现上面的对象之间没有关系的复用。 组件的组织 这里有两种组件的注册类型：全局注册和局部注册。至此，我们的组件都只是通过 Vue.component 全局注册的：123Vue.component(&apos;my-component-name&apos;, &#123; // ... options ...&#125;) 通过 Prop 向子组件传递数据 Prop 是你可以在组件上注册的一些自定义特性。当一个值传递给一个 prop 特性的时候，它就变成了那个组件实例的一个属性。为了给博文组件传递一个标题，我们可以用一个 props 选项将其包含在该组件可接受的 prop 列表中： 1234Vue.component(&apos;blog-post&apos;, &#123; props: [&apos;title&apos;], template: &apos;&lt;h3&gt;&#123;&#123; title &#125;&#125;&lt;/h3&gt;&apos;&#125;) 感觉这个props仅仅是用来约束，如果没有这个约束，所有在组件的属性中送过来的属性都会被接受。 单个根元素这一章节讲了涉及多个元素的组件是如何组织的。例如：12&lt;h3&gt;&#123;&#123; title &#125;&#125;&lt;/h3&gt;&lt;div v-html=&quot;content&quot;&gt;&lt;/div&gt; 如果模板涉及了多个元素，那么需要将这些元素放在一个根元素内，上面这个模板就涉及到两个元素，这个时候需要在外面再包裹一个元素：1234&lt;div class=&quot;blog-post&quot;&gt; &lt;h3&gt;&#123;&#123; title &#125;&#125;&lt;/h3&gt; &lt;div v-html=&quot;content&quot;&gt;&lt;/div&gt;&lt;/div&gt; 下面的例子涉及到如何设计传递的参数。12345678&lt;blog-post v-for=&quot;post in posts&quot; v-bind:key=&quot;post.id&quot; v-bind:title=&quot;post.title&quot; v-bind:content=&quot;post.content&quot; v-bind:publishedAt=&quot;post.publishedAt&quot; v-bind:comments=&quot;post.comments&quot;&lt;/blog-post&gt; 合理的设计应该是：12345&lt;blog-post v-for=&quot;post in posts&quot; v-bind:key=&quot;post.id&quot; v-bind:post=&quot;post&quot;&gt;&lt;/blog-post&gt; 123456789Vue.component(&apos;blog-post&apos;, &#123; props: [&apos;post&apos;], template: ` &lt;div class=&quot;blog-post&quot;&gt; &lt;h3&gt;&#123;&#123; post.title &#125;&#125;&lt;/h3&gt; &lt;div v-html=&quot;post.content&quot;&gt;&lt;/div&gt; &lt;/div&gt; `&#125;) 通过事件向父级组件发送消息这段没想明白应该怎么去总结，完全照搬过来没有什么意义。 123&lt;button v-on:click=&quot;$emit(&apos;enlarge-text&apos;)&quot;&gt; Enlarge text&lt;/button&gt; 这个是孩子组件（位于父组件之内），可以使用$emit向父组件发送一个自定义的消息。 1234&lt;blog-post ... v-on:enlarge-text=&quot;postFontSize += 0.1&quot;&gt;&lt;/blog-post&gt; 父组件使用v-on去监听这个事件。 使用事件抛出一个值接着上面的例子，如果想给父组件发送消息的同时，传递参数，需要这样：孩子组件：123&lt;button v-on:click=&quot;$emit(&apos;enlarge-text&apos;, 0.1)&quot;&gt; Enlarge text&lt;/button&gt; 父组件：1234&lt;blog-post ... v-on:enlarge-text=&quot;postFontSize += $event&quot;&gt;&lt;/blog-post&gt; 这里第二个参数，可以是一个值，或者是一个方法12345methods: &#123; onEnlargeText: function (enlargeAmount) &#123; this.postFontSize += enlargeAmount &#125;&#125; 好像不能是对象，这个还要确认一下。 在组件上使用 v-model有一点含糊了，prop和v-model在这里有什么区别呢？这个部分讲的感觉又有点不清楚。1&lt;input v-model=&quot;searchText&quot;&gt; 等价于1234&lt;input v-bind:value=&quot;searchText&quot; v-on:input=&quot;searchText = $event.target.value&quot;&gt; 用在自定义的组件上（上面是标准的HTML元素），则是1234&lt;custom-input v-bind:value=&quot;searchText&quot; v-on:input=&quot;searchText = $event&quot;&gt;&lt;/custom-input&gt; 和上面大体接近，只是最后的$event有区别。 为了让它正常工作，这个组件内的 &lt;input&gt; 必须：将其 value 特性绑定到一个名叫 value 的 prop 上在其 input 事件被触发时，将新的值通过自定义的 input 事件抛出写成代码之后是这样的：123456789Vue.component(&apos;custom-input&apos;, &#123; props: [&apos;value&apos;], template: ` &lt;input v-bind:value=&quot;value&quot; v-on:input=&quot;$emit(&apos;input&apos;, $event.target.value)&quot; &gt; `&#125;) 现在 v-model 就应该可以在这个组件上完美地工作起来了：&lt;custom-input v-model=&quot;searchText&quot;&gt;&lt;/custom-input&gt; 意思是：1234&lt;custom-input v-bind:value=&quot;searchText&quot; v-on:input=&quot;searchText = $event&quot;&gt;&lt;/custom-input&gt; 这么写和1&lt;custom-input v-model=&quot;searchText&quot;&gt;&lt;/custom-input&gt; 这么写是一个意思，组件里面都要这么写：123456789Vue.component(&apos;custom-input&apos;, &#123; props: [&apos;value&apos;], template: ` &lt;input v-bind:value=&quot;value&quot; v-on:input=&quot;$emit(&apos;input&apos;, $event.target.value)&quot; &gt; `&#125;) 对event进行了一次转发。 通过插槽分发内容这里讲的是如何把元素的内容传递给组件，叫做分发内容，使用&lt;slot&gt;这样的关键字来接收。123&lt;alert-box&gt; Something bad happened.&lt;/alert-box&gt; 上面是一个自定义组件，给这个组件直接传递内容，会得到不正确的结果，需要有能接收这个内容的东西，这个就叫插槽。这个地方感觉讲的不清楚，自己尝试了一下。如上图，这里并没有显示出内容来，没有用于接收的东西。 改为：12345678Vue.component(&apos;alert-box&apos;, &#123; template: ` &lt;div class=&quot;demo-alert-box&quot;&gt; &lt;strong&gt;Error!&lt;/strong&gt; &lt;slot&gt;&lt;/slot&gt; &lt;/div&gt; `&#125;) 则：这样才显示出来。 动态组件这段又没看懂。 解析 DOM 模板时的注意事项这段和上面那段有关联，但是好像也是没写清楚，继续等待深度理解。 总结断断续续，花了三天，终于大体看明白了（花了很多时间看这个slot，也把后面的关于slot的看了一遍，基本梳理清楚），感觉这一章节是Vue的核心内容，还需要不断完善这篇笔记。 参考https://cn.vuejs.org/v2/guide/components.html]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>JS</tag>
        <tag>Vue</tag>
        <tag>组件基础</tag>
        <tag>component</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue：学习笔记（五）-Class 与 Style 绑定]]></title>
    <url>%2Fjs%2Fjs-vue-note-css-style-binding.html</url>
    <content type="text"><![CDATA[Vue：学习笔记（五）-Class 与 Style 绑定。 提醒原帖完整收藏于IT老兵驿站，并会不断更新。 前言本篇继续对Vue的【Class 与 Style 绑定】篇进行总结学习。 正文 操作元素的 class 列表和内联样式是数据绑定的一个常见需求。因为它们都是属性，所以我们可以用 v-bind 处理它们：只需要通过表达式计算出字符串结果即可。不过，字符串拼接麻烦且易错。因此，在将 v-bind 用于 class 和 style 时，Vue.js 做了专门的增强。表达式结果的类型除了字符串之外，还可以是对象或数组。 绑定 HTML Class对象语法 我们可以传给 v-bind:class 一个对象，以动态地切换 class：&lt;div v-bind:class=&quot;{ active: isActive }&quot;&gt;&lt;/div&gt; 所谓对象语法，就是样式这里使用一个对象（大括号括起来的是对象，JavaScript的对象写法，这里是一个匿名对象），前缀是class，这个对象的域是在data对象中。1234data: &#123; isActive: true, hasError: false&#125; 还可以这么写：&lt;div v-bind:class=&quot;classObject&quot;&gt;&lt;/div&gt; 123456data: &#123; classObject: &#123; active: true, &apos;text-danger&apos;: false &#125;&#125; 这里是一个具名对象。 这个对象也可以是一个计算属性。 数组语法数组的原理是一样的，语法类似下面：&lt;div v-bind:class=&quot;[activeClass, errorClass]&quot;&gt;&lt;/div&gt; 以数组的形式嵌入，定义在：1234data: &#123; activeClass: &apos;active&apos;, errorClass: &apos;text-danger&apos;&#125; data对象的两个域。 用在组件上还没有看到组件一节，先略过。 绑定内联样式对象语法 v-bind:style 的对象语法十分直观——看着非常像 CSS，但其实是一个 JavaScript 对象。CSS 属性名可以用驼峰式 (camelCase) 或短横线分隔 (kebab-case，记得用单引号括起来) 来命名： 这里使用的还是对象，不过前缀变成了style。 12345&lt;div v-bind:style=&quot;&#123; color: activeColor, fontSize: fontSize + &apos;px&apos; &#125;&quot;&gt;&lt;/div&gt;data: &#123; activeColor: &apos;red&apos;, fontSize: 30&#125; 这个是匿名对象的例子，感觉语法和class一样，可能只是优先级不一样。 具名对象：1234567&lt;div v-bind:style=&quot;styleObject&quot;&gt;&lt;/div&gt;data: &#123; styleObject: &#123; color: &apos;red&apos;, fontSize: &apos;13px&apos; &#125;&#125; 数组语法类似上面。 自动添加前缀这里涉及浏览器前缀的一些知识，参考这里。 摘抄一段： 浏览器厂商们有时会给一些在试验阶段和非标准阶段的css属性或JavaScript API添加前缀, 这样开发者就可以在使用这些试验阶段的代码时能够确保不会被其他标准代码所依赖而导致破坏标准WEB代码的问题。开发人员应该等到浏览器行为被标准化之后再取消前缀。 例如经常看到的： -webkit- (谷歌, Safari, 新版Opera浏览器等)-moz- (火狐浏览器)-o- (旧版Opera浏览器等)-ms- (IE浏览器 和 Edge浏览器) 多重值摘抄记录： 从 2.3.0 起你可以为 style 绑定中的属性提供一个包含多个值的数组，常用于提供多个带前缀的值，例如：&lt;div :style=&quot;{ display: [&#39;-webkit-box&#39;, &#39;-ms-flexbox&#39;, &#39;flex&#39;] }&quot;&gt;&lt;/div&gt;这样写只会渲染数组中最后一个被浏览器支持的值。在本例中，如果浏览器支持不带浏览器前缀的 flexbox，那么就只会渲染 display: flex。 总结这篇内容主要涉及了在Vue中怎么设置CSS，这里涉及了CSS的class和style，这里需要总结一下。 设置CSS一共有三种方式： 内联 - 使用HTML元素的style属性，例如： &lt;h1 style=&quot;color:blue;&quot;&gt;This is a Blue Heading&lt;/h1&gt; 内部 - 使用&lt;head&gt;区域的&lt;style&gt;元素，例如： 123456789&lt;html&gt;&lt;head&gt;&lt;style&gt;body &#123;background-color: powderblue;&#125;h1 &#123;color: blue;&#125;p &#123;color: red;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt; 外部 - 使用外部的CSS文件 123456789101112&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;This is a heading&lt;/h1&gt;&lt;p&gt;This is a paragraph.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 还有一个优先级的关系，可以参考这里。 参考https://cn.vuejs.org/v2/guide/class-and-style.htmlhttps://developer.mozilla.org/zh-CN/docs/Glossary/Vendor_Prefixhttps://www.w3schools.com/html/html_css.asp]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>CSS</tag>
        <tag>JS</tag>
        <tag>Vue</tag>
        <tag>Style</tag>
        <tag>绑定</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[十月份计划]]></title>
    <url>%2Fscheme%2F2018-10.html</url>
    <content type="text"><![CDATA[十月份计划。 前言写博客，缺乏了一些计划，感觉有些零零散散，应该梳理一下计划。每次有文章的更新，就在当月计划的这篇文章里面记录一下，这样可以记录下自己的足迹。 正文十月份还剩十天，因为最近的工作要用到Vue，所以计划本月整理Vue的笔记，对照官网，再完成几篇对Vue的学习。以现在的进度，可能两三天才能整理一篇，尽管如此，不想光追求速度，还是需要保证质量，“君子勿速成”，速生的东西往往速死，着急赶出来的东西，往往缺乏价值，饭一口一口吃，事情一件一件办。 今年还剩两个多月，计划把MongoDB的笔记再丰富一些，把基本的使用都总结清楚，整理成一个系列；其余的，现在也说不定，就边走边看吧。]]></content>
      <categories>
        <category>计划</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Vue：学习笔记（四）-计算属性和侦听器]]></title>
    <url>%2Fjs%2Fjs-vue-note-computed.html</url>
    <content type="text"><![CDATA[Vue：学习笔记（四）-计算属性和侦听器。 提醒原帖完整收藏于IT老兵驿站，并会不断更新。 前言参考官网的这里和中文版，总体学习一下计算属性，感觉这一章节总体是比较简单的，做一下笔记来进行总结。思路是，原文写的很清楚的，只做简单的概括；对原文存在疑问的地方，摘抄原文，列举问题，总结概括。 正文计算属性摘录原则： 所以，对于任何复杂逻辑，你都应当使用计算属性。 基础例子1234&lt;div id=&quot;example&quot;&gt; &lt;p&gt;Original message: &quot;&#123;&#123; message &#125;&#125;&quot;&lt;/p&gt; &lt;p&gt;Computed reversed message: &quot;&#123;&#123; reversedMessage &#125;&#125;&quot;&lt;/p&gt;&lt;/div&gt; 12345678910111213var vm = new Vue(&#123; el: &apos;#example&apos;, data: &#123; message: &apos;Hello&apos; &#125;, computed: &#123; // 计算属性的 getter reversedMessage: function () &#123; // `this` 指向 vm 实例 return this.message.split(&apos;&apos;).reverse().join(&apos;&apos;) &#125; &#125;&#125;) 这里的reversedMessage实际上是和message绑定在一起了，message发生变化，则reversedMessage也发生变化。 那这里，存在一个疑问了，计算属性是不是也可以不和基本属性（例如message）发生绑定关系呢？ 计算属性缓存 vs 方法这一段刚好解决了上面提出的这个问题，看来不断地提一提问题，对于理解是很有帮助的，“学而不思则罔”。 我们可以将同一函数定义为一个方法而不是一个计算属性。两种方式的最终结果确实是完全相同的。然而，不同的是计算属性是基于它们的依赖进行缓存的。只在相关依赖发生改变时它们才会重新求值。这就意味着只要 message 还没有发生改变，多次访问 reversedMessage 计算属性会立即返回之前的计算结果，而不必再次执行函数。 这段话很好地解释了什么时候要使用计算属性，为了提高效率，很合理地去利用架构提供的缓存的功能。 计算属性 vs 侦听属性 当你有一些数据需要随着其它数据变动而变动时，你很容易滥用 watch——特别是如果你之前使用过 AngularJS。然而，通常更好的做法是使用计算属性而不是命令式的 watch 回调。 去年学习AngularJS，使用watch函数进行监控，后来在网站上看到很多对于这个watch方式的诟病，说这样会发生抖动，会导致DOM树不断被渲染，这个还需要再深度研究一下。 计算属性的 setter12345678910111213141516// ...computed: &#123; fullName: &#123; // getter get: function () &#123; return this.firstName + &apos; &apos; + this.lastName &#125;, // setter set: function (newValue) &#123; var names = newValue.split(&apos; &apos;) this.firstName = names[0] this.lastName = names[names.length - 1] &#125; &#125;&#125;// ... 侦听器1234567&lt;div id=&quot;watch-example&quot;&gt; &lt;p&gt; Ask a yes/no question: &lt;input v-model=&quot;question&quot;&gt; &lt;/p&gt; &lt;p&gt;&#123;&#123; answer &#125;&#125;&lt;/p&gt;&lt;/div&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!-- 因为 AJAX 库和通用工具的生态已经相当丰富，Vue 核心代码没有重复 --&gt;&lt;!-- 提供这些功能以保持精简。这也可以让你自由选择自己更熟悉的工具。 --&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/axios@0.12.0/dist/axios.min.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/lodash@4.13.1/lodash.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;var watchExampleVM = new Vue(&#123; el: &apos;#watch-example&apos;, data: &#123; question: &apos;&apos;, answer: &apos;I cannot give you an answer until you ask a question!&apos; &#125;, watch: &#123; // 如果 `question` 发生改变，这个函数就会运行 question: function (newQuestion, oldQuestion) &#123; this.answer = &apos;Waiting for you to stop typing...&apos; this.debouncedGetAnswer() &#125; &#125;, created: function () &#123; // `_.debounce` 是一个通过 Lodash 限制操作频率的函数。 // 在这个例子中，我们希望限制访问 yesno.wtf/api 的频率 // AJAX 请求直到用户输入完毕才会发出。想要了解更多关于 // `_.debounce` 函数 (及其近亲 `_.throttle`) 的知识， // 请参考：https://lodash.com/docs#debounce this.debouncedGetAnswer = _.debounce(this.getAnswer, 500) &#125;, methods: &#123; getAnswer: function () &#123; if (this.question.indexOf(&apos;?&apos;) === -1) &#123; this.answer = &apos;Questions usually contain a question mark. ;-)&apos; return &#125; this.answer = &apos;Thinking...&apos; var vm = this axios.get(&apos;https://yesno.wtf/api&apos;) .then(function (response) &#123; vm.answer = _.capitalize(response.data.answer) &#125;) .catch(function (error) &#123; vm.answer = &apos;Error! Could not reach the API. &apos; + error &#125;) &#125; &#125;&#125;)&lt;/script&gt; 对于这个例子，我是这么理解的，要监控中间的变化（例如一段文字输入的中间过程），这个时候使用侦听器就是合理的。 这里还涉及了一个知识点 _.debounce(func, [wait=0], [options={}])Creates a debounced function that delays invoking func until after wait milliseconds have elapsed since the last time the debounced function was invoked. 这个方法是对函数进行延时调用，避免函数被调用过于频繁，在上面的例子中，就是避免因为用户输入很快，而反复被调用，所以中间等待500毫秒。 总结阅读了两遍，总结了一遍，大体把这个章节搞明白了，大概耗时三个小时。 参考https://cn.vuejs.org/v2/guide/computed.html]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>JS</tag>
        <tag>Vue</tag>
        <tag>计算属性</tag>
        <tag>computed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue：学习笔记（三）-模板语法]]></title>
    <url>%2Fjs%2Fjs-vue-note-syntax.html</url>
    <content type="text"><![CDATA[Vue：学习笔记（三）-模板语法。 提醒原帖完整收藏于IT老兵驿站，并会不断更新。 前言忙了三周，又度过一个丰富的十一，现在腾出手来，继续我的学习和总结。最近找到了Vue的中文网站，但是我不想放弃对英文网站的学习，那样可以更准确地理解原意，可以提高自己的英文水平，所以基于英文网站，对照着中文，这样来学习—-人还是应该有些追求。我发现一个问题，有的章节内容多，有的章节内容少，内容多的，可能一天总结不完，那就可能需要拆成几篇连续的笔记来记录了。 正文模板语法 Vue.js uses an HTML-based template syntax that allows you to declaratively bind the rendered DOM to the underlying Vue instance’s data. All Vue.js templates are valid HTML that can be parsed by spec-compliant browsers and HTML parsers.Under the hood, Vue compiles the templates into Virtual DOM render functions. Combined with the reactivity system, Vue is able to intelligently figure out the minimal number of components to re-render and apply the minimal amount of DOM manipulations when the app state changes.If you are familiar with Virtual DOM concepts and prefer the raw power of JavaScript, you can also directly write render functions instead of templates, with optional JSX support. 我对这里的理解是，模板是一种工具，它不需要你去查找元素，进行赋值等处理（传统的方式），而是进行了单向或者双向的绑定，这样你直接操作这个变量，就是在操作DOM中的那个元素（虚拟DOM树的概念），另外，模板会在合适的时候，进行渲染，这样减少因为频繁的渲染页面的抖动。 插值（Interpolations）最早接触这个概念是在对AngularJS的学习中，应该是AngularJS最早引入了这个概念。 文本1&lt;span&gt;Message: &#123;&#123; msg &#125;&#125;&lt;/span&gt; 双大括号的语法，里面是插值的变量名，变量发生改变，这里也会同时发生改变。 &lt;span v-once&gt;这个将不会改变: &lt;/span&gt;使用 v-once 指令，执行一次性地插值。 原始HTML12&lt;p&gt;Using mustaches: &#123;&#123; rawHtml &#125;&#125;&lt;/p&gt;&lt;p&gt;Using v-html directive: &lt;span v-html=&quot;rawHtml&quot;&gt;&lt;/span&gt;&lt;/p&gt; 双大括号里面包含的内容，会以纯文本的形式显示出来，不会交由浏览器去解释。而想要浏览器去解释这些内容，则需要使用v-html，例如上例。 rawHtml的内容其实是&lt;span style=&quot;color: red&quot;&gt;This should be red.&lt;/span&gt;，则上例的实际显示如下（这个例子原帖讲的有一点不清楚）： Using mustaches: &lt;span style=”color: red”&gt;This should be red.&lt;/span&gt; Using v-html directive: This should be red.（这里应该是红色，为了让这里显示红色，我还研究了一下MD语法，参考这里） 不过一般不建议这么用，因为这样就太容易给XSS（跨站攻击，互联网最常见的一种攻击形式，将来有机会也会总结一下）攻击创造机会。 特性（attribute）这一节其实应该叫属性，不过可能是为了和property区别，这里刻意翻译成了特性，其实是指HTML里面元素的属性，关于HTML元素的名、值、属性的关系可以参考早年写的一篇帖子，那篇讲的是XML，HTML其实一种特殊化的XML，原理是一样的。因为习惯了，以下我还是称呼这个为属性。 属性没法使用Mustache语法，所以就需要有新的指令（directive），指令也应该是AngularJS引入的一个概念，其实是可以被Vue解释的一些固定的字符串，可以接收参数，具有一定的功能。 1&lt;div v-bind:id=&quot;dynamicId&quot;&gt;&lt;/div&gt; 这样id属性就和dynamicId绑定起来了。 不过，对于disabled属性，有点区别，只有当它为true的时候才会被渲染。 使用 JavaScript 表达式Vue支持单个表达式的绑定，如下：123&#123;&#123; number + 1 &#125;&#125;&#123;&#123; ok ? &apos;YES&apos; : &apos;NO&apos; &#125;&#125;&#123;&#123; message.split(&apos;&apos;).reverse().join(&apos;&apos;) &#125;&#125; 但是不支持：12&#123;&#123; var a = 1 &#125;&#125;&#123;&#123; if (ok) &#123; return message &#125; &#125;&#125; 有几点很关键： 这些表达式会在所属 Vue 实例的数据作用域下作为 JavaScript 被解析。有个限制就是，每个绑定都只能包含单个表达式，所以下面的例子都不会生效。 模板表达式都被放在沙盒中，只能访问全局变量的一个白名单，如 Math 和 Date 。你不应该在模板表达式中试图访问用户定义的全局变量。 指令 指令 (Directives) 是带有 v- 前缀的特殊特性。指令特性的值预期是单个 JavaScript 表达式 (v-for 是例外情况，稍后我们再讨论)。指令的职责是，当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM。 指令可以理解成为一些已经有固化逻辑的函数，它把DOM树和用户的变量关联起来。 参数&lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt;&lt;a v-on:click=&quot;doSomething&quot;&gt;...&lt;/a&gt;上面这两个指令是带有参数的，分别是href和click。 &lt;p v-if=&quot;seen&quot;&gt;现在你看到我了&lt;/p&gt;上面这个指令是不带参数的。 修饰符 修饰符 (Modifiers) 是以半角句号 . 指明的特殊后缀，用于指出一个指令应该以特殊方式绑定。例如，.prevent 修饰符告诉 v-on 指令对于触发的事件调用 event.preventDefault()： &lt;form v-on:submit.prevent=&quot;onSubmit&quot;&gt;...&lt;/form&gt; 这个地方有点含糊，这里涉及到了Web API里面的event，这个地方的意思应该是对于submit事件，绑定onSubmit这个方法，并且调用event.preventDefault()，组织默认的行为发生。 缩写v-bind的缩写：12345&lt;!-- 完整语法 --&gt;&lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt;&lt;!-- 缩写 --&gt;&lt;a :href=&quot;url&quot;&gt;...&lt;/a&gt; v-on的缩写：12345&lt;!-- 完整语法 --&gt;&lt;a v-on:click=&quot;doSomething&quot;&gt;...&lt;/a&gt;&lt;!-- 缩写 --&gt;&lt;a @click=&quot;doSomething&quot;&gt;...&lt;/a&gt; 总结 读了原帖大概两遍，阅读、理解加上写笔记，一共花费了大概三个小时，感觉再写下去，耐心就会降低，质量就会降低，会有一些应付的情绪，只好先告一段落，这个部分的内容分为两篇文章了。 今天感觉，剩余的内容也不是很多了，还是合为一篇笔记比较合理，便于将来复习。 参考https://vuejs.org/v2/guide/syntax.htmlhttps://cn.vuejs.org/v2/guide/components.htmlhttps://blog.csdn.net/liuhw4598/article/details/78279737https://developer.mozilla.org/en-US/docs/Web/API/GlobalEventHandlers/onsubmithttps://developer.mozilla.org/en-US/docs/Web/API/Event/preventDefault]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>JS</tag>
        <tag>Vue</tag>
        <tag>模板语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS：对“this”的学习]]></title>
    <url>%2Fjs%2Fjs-this.html</url>
    <content type="text"><![CDATA[JS：对“this”的学习。 前言更多笔记，请参看IT老兵驿站。 有半个多月没有更新博客了，这半个多月一直在加班，实在没有精力更新，现在到了调整期，可以将前一段时间的工作进行一下整理。 之前对JS的this的理解一直有点模糊，这次总结一下，因为在工作中总遇到this的问题，如果一直这么模模糊糊，将会在以后的工作中带来麻烦，而对于这种躲不开的麻烦，早解决肯定要比晚解决好。 这篇帖子是针对参考中的w3schools的一篇帖子进行学习、翻译和理解，但我感觉w3schools这篇帖子层次有点不是太清楚。 正文this是什么例子：12345678var person = &#123; firstName: &quot;John&quot;, lastName : &quot;Doe&quot;, id : 5566, fullName : function() &#123; return this.firstName + &quot; &quot; + this.lastName; &#125;&#125;; 这里的this指代的是什么呢？ What is “this”?In a function definition, this refers to the “owner” of the function.In the example above, this refers to the person object.The person object “owns” the fullName method. 在函数定义中，this指代函数的“拥有者”，例如上面例子中，this就代表person这个对象。 默认绑定 Default BindingWhen used alone, this refers to the Global object.In a browser the Global object is [object Window]: 默认的绑定，单独使用时，this就指代全局对象。个人理解，这一条和上一条不矛盾，this还是指代拥有这个变量或者函数的对象，这个时候是全局变量拥有这个变量，所以就指向了全局变量。 In strict mode, this will be undefined, because strict mode does not allow default binding: 但是在严格模式下，this将会是undefined，因为严格模式不允许默认绑定。 明确绑定 Explicit Function BindingThe call() and apply() methods are predefined JavaScript methods.They can both be used to call an object method with another object as argument. 明确的函数绑定，call()和apply()是JS预定义的方法，他们可以被用于使用另外一个对象作为参数，调用这个对象的方法。 12345678910var person1 = &#123; fullName: function() &#123; return this.firstName + &quot; &quot; + this.lastName; &#125;&#125;var person2 = &#123; firstName:&quot;John&quot;, lastName: &quot;Doe&quot;,&#125;person1.fullName.call(person2); // Will return &quot;John Doe&quot; 看上面这个例子，this指向了person2，最终输出的是person2的属性。 总结这样一梳理，感觉对于this的理解就清晰了。 参考https://www.w3schools.com/js/js_this.asp]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JS</tag>
        <tag>this</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue：学习笔记（二）-实例]]></title>
    <url>%2Fjs%2Fjs-vue-note-instance.html</url>
    <content type="text"><![CDATA[Vue：学习笔记-实例。 提醒原帖完整收藏于IT老兵驿站，并会不断更新。 前言书接前文。继续学习Vue，距离上一篇笔记又有十几天了，因为最近实在是工作太忙了，但还是应该努力坚持。 正文创建Vue实例如何创建一个Vue的实例呢？ var vm = new Vue({ // options }) Vue参考了MVVM模型，这里的vm指代ViewModel。当你创建一个Vue实例的时候，你可以创建一个options对象给它，这个将来会再具体讨论。 数据和方法当一个Vue实例被创建时，它会把所有位于data对象里面的属性加入到Vue响应式系统里面，这样一旦这些属性发生了变化，视图会响应，并且更新相应的值。1234567891011121314151617181920212223// Our data object 这是一个自定义的对象 var data = &#123; a: 1 &#125; // The object is added to a Vue instance 这个对象被加入进了vm实例 var vm = new Vue(&#123; data: data // 这里是一个赋值语句，前面一个data是vm的关键属性对象，后面是上面我们定义的变量 &#125;) // Getting the property on the instance // returns the one from the original data //这里做了一个判断，判断vm的a属性和外面的data的a属性是否相等，因为它们其实是一致的，指向一个实际的对象，所以是相等的。不过，这里的写法是vm.a，而不是vm.data.a。 vm.a == data.a // =&gt; true // Setting the property on the instance // also affects the original data // 用赋值来进行判断 vm.a = 2 data.a // =&gt; 2 // ... and vice-versa // 上面这句话叫做反之亦然 data.a = 3 vm.a // =&gt; 3 这是官网的一个例子，加了一些翻译和注释，方便理解，从中可以看到变量data和vm的关系。 需要注意一个地方，data对象里面的属性只有是在Vue实例创建时就存在的，才会被纳入响应式系统里面去，这也就是说，后来加进去的属性，是不会具有上面这种响应式的能力的。所以，一旦你希望有一个属性能够具有这种能力，但是在一开始你又不确定它的值的话，做法就很明显了，你需要先定义这个属性，并且赋一个初始值，例如： data: { newTodoText: &apos;&apos;, visitCount: 0, hideCompletedTodos: false, todos: [], error: null } 例外的，还有一个用法，Object.freeze()，这个用法是用来不让属性被修改。 var obj = { foo: &apos;bar&apos; } Object.freeze(obj) new Vue({ el: &apos;#app&apos;, data: obj }) &lt;div id=&quot;app&quot;&gt; &lt;p&gt;{{ foo }}&lt;/p&gt; &lt;!-- this will no longer update `foo`! --&gt; &lt;button v-on:click=&quot;foo = &apos;baz&apos;&quot;&gt;Change it&lt;/button&gt; &lt;/div&gt; Vue自身还有一些实例属性和方法，为了和用户定义的区分开，以“$”为前缀。 var data = { a: 1 } var vm = new Vue({ el: &apos;#example&apos;, data: data }) vm.$data === data // =&gt; true vm.$el === document.getElementById(&apos;example&apos;) // =&gt; true // $watch is an instance method vm.$watch(&apos;a&apos;, function (newValue, oldValue) { // This callback will be called when `vm.a` changes }) 实例生命周期的钩子 Each Vue instance goes through a series of initialization steps when it’s created - for example, it needs to set up data observation, compile the template, mount the instance to the DOM, and update the DOM when data changes. Along the way, it also runs functions called lifecycle hooks, giving users the opportunity to add their own code at specific stages. 这里的意思实际是：实例的生命周期被定义为了几个阶段，每个阶段会有一个回调函数来暴露给用户，让用户来进行一些工作，这个很像安卓的设计。 例如： new Vue({ data: { a: 1 }, created: function () { // `this` points to the vm instance console.log(&apos;a is: &apos; + this.a) } }) 这个就是在Vue创建时暴露给用户。 完整的生命周期参考下图： 总结这么细细地读一遍，同时总结一遍，感觉很踏实，其实有些笔记是为了输出，以飨他人，虽然辛苦一点，总是有意义，“好好活，就是做好多有意义的事情；有意义，就是好好活” 参考https://vuejs.org/v2/guide/instance.html]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>JS</tag>
        <tag>Vue</tag>
        <tag>笔记</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue：学习笔记（一）-介绍]]></title>
    <url>%2Fjs%2Fjs-vue-note-introduction.html</url>
    <content type="text"><![CDATA[Vue：学习笔记-介绍。 提醒原帖完整收藏于IT老兵驿站，并会不断更新。 前言17年上半年，学习了一些Vue的知识，但是现在反观回去，感觉在那个时候，因为着急做项目，很多东西消化的不够清楚，这一点同样体现在对angular的学习上，现在有点时间进行修整，那就花点时间去好好整理一下。 正文 Vue (pronounced /vjuː/, like view) is a progressive framework for building user interfaces. Unlike other monolithic frameworks, Vue is designed from the ground up to be incrementally adoptable. The core library is focused on the view layer only, and is easy to pick up and integrate with other libraries or existing projects. On the other hand, Vue is also perfectly capable of powering sophisticated Single-Page Applications when used in combination with modern tooling and supporting libraries. 大体翻译：Vue是一个进步的框架（progressive framework），用来构建用户界面。它不像别的大而全的框架，由很小的部分，逐步增量吸收完善。它的核心库仅仅专注于view层，很容易使用，或者说是和项目的其他库集成。 这段前言，好像是第一次这么清楚地读明白，枉费了作者的一番心血。 如何引入在你的index.html中引入1&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue/dist/vue.js&quot;&gt;&lt;/script&gt; 这个是开发版本，在控制台会有一些有用的输出。或者： 1&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue&quot;&gt;&lt;/script&gt; 生产版本，优化了大小和速度。 还有一些别的安装帮助，在Installation ，初学者不建议立刻使用vue-cli（类似一个脚手架），这样你会搞不清原理，但我相信，大多数的人还是会立刻去使用这个，因为立刻可以做出一些东西来，能做出东西来就行，谁会在意什么原理不原理呢？ 陈述式的渲染直接渲染数据到DOM树：html文件中（下文中，上面的代码段都表示是在html文件中，下面的代码段表示是在js文件中，可以在这个在线模拟器上进行尝试）：123&lt;div id=&quot;app&quot;&gt; &#123;&#123; message &#125;&#125;&lt;/div&gt; js文件中：123456var app = new Vue(&#123; el: &apos;#app&apos;, data: &#123; message: &apos;Hello Vue!&apos; &#125;&#125;) 绑定元素属性123456&lt;div id=&quot;app-2&quot;&gt; &lt;span v-bind:title=&quot;message&quot;&gt; Hover your mouse over me for a few seconds to see my dynamically bound title! &lt;/span&gt;&lt;/div&gt; 123456var app2 = new Vue(&#123; el: &apos;#app-2&apos;, data: &#123; message: &apos;You loaded this page on &apos; + new Date().toLocaleString() &#125;&#125;) 条件和循环条件123&lt;div id=&quot;app-3&quot;&gt; &lt;span v-if=&quot;seen&quot;&gt;Now you see me&lt;/span&gt;&lt;/div&gt; 123456var app3 = new Vue(&#123; el: &apos;#app-3&apos;, data: &#123; seen: true &#125;&#125;) 循环1234567&lt;div id=&quot;app-4&quot;&gt; &lt;ol&gt; &lt;li v-for=&quot;todo in todos&quot;&gt; &#123;&#123; todo.text &#125;&#125; &lt;/li&gt; &lt;/ol&gt;&lt;/div&gt; 12345678910var app4 = new Vue(&#123; el: &apos;#app-4&apos;, data: &#123; todos: [ &#123; text: &apos;Learn JavaScript&apos; &#125;, &#123; text: &apos;Learn Vue&apos; &#125;, &#123; text: &apos;Build something awesome&apos; &#125; ] &#125;&#125;) 处理用户输入1234&lt;div id=&quot;app-5&quot;&gt; &lt;p&gt;&#123;&#123; message &#125;&#125;&lt;/p&gt; &lt;button v-on:click=&quot;reverseMessage&quot;&gt;Reverse Message&lt;/button&gt;&lt;/div&gt; 1234567891011var app5 = new Vue(&#123; el: &apos;#app-5&apos;, data: &#123; message: &apos;Hello Vue.js!&apos; &#125;, methods: &#123; reverseMessage: function () &#123; this.message = this.message.split(&apos;&apos;).reverse().join(&apos;&apos;) &#125; &#125;&#125;) 1234&lt;div id=&quot;app-6&quot;&gt; &lt;p&gt;&#123;&#123; message &#125;&#125;&lt;/p&gt; &lt;input v-model=&quot;message&quot;&gt;&lt;/div&gt; 123456var app6 = new Vue(&#123; el: &apos;#app-6&apos;, data: &#123; message: &apos;Hello Vue!&apos; &#125;&#125;) 用组件来构成组件的概念，是一个预定义好的一些选项的Vue的实例。 定义一个组件，语法如下：1234// Define a new component called todo-itemVue.component(&apos;todo-item&apos;, &#123; template: &apos;&lt;li&gt;This is a todo&lt;/li&gt;&apos;&#125;) 其实，这个就相当于自定义了一个HTTP元素，并且这个元素是在js中得到解释的，解释成HTML原生的元素。这样可以把它组装在另外一个组件的模板里：1234&lt;ol&gt; &lt;!-- Create an instance of the todo-item component --&gt; &lt;todo-item&gt;&lt;/todo-item&gt;&lt;/ol&gt; 但是这样，这个组件的内容是固定的，这样没有太大意义，所以，这个内容应该是一个变量，由使用者来定义，所以，这里又设计一个props，来定义这个变量，如下：1234567Vue.component(&apos;todo-item&apos;, &#123; // The todo-item component now accepts a // &quot;prop&quot;, which is like a custom attribute. // This prop is called todo. props: [&apos;todo&apos;], template: &apos;&lt;li&gt;&#123;&#123; todo.text &#125;&#125;&lt;/li&gt;&apos;&#125;) 123456789101112131415&lt;div id=&quot;app-7&quot;&gt; &lt;ol&gt; &lt;!-- Now we provide each todo-item with the todo object it&apos;s representing, so that its content can be dynamic. We also need to provide each component with a &quot;key&quot;, which will be explained later. --&gt; &lt;todo-item v-for=&quot;item in groceryList&quot; v-bind:todo=&quot;item&quot; v-bind:key=&quot;item.id&quot;&gt; &lt;/todo-item&gt; &lt;/ol&gt;&lt;/div&gt; 1234Vue.component(&apos;todo-item&apos;, &#123; props: [&apos;todo&apos;], template: &apos;&lt;li&gt;&#123;&#123; todo.text &#125;&#125;&lt;/li&gt;&apos;&#125;) 12345678910var app7 = new Vue(&#123; el: &apos;#app-7&apos;, data: &#123; groceryList: [ &#123; id: 0, text: &apos;Vegetables&apos; &#125;, &#123; id: 1, text: &apos;Cheese&apos; &#125;, &#123; id: 2, text: &apos;Whatever else humans are supposed to eat&apos; &#125; ] &#125;&#125;) 参考https://vuejs.org/v2/guide/]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>JS</tag>
        <tag>Vue</tag>
        <tag>笔记</tag>
        <tag>介绍</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04下升级Python到3.6.5]]></title>
    <url>%2Fpython%2Fupgrade-python-under-ubuntu1604.html</url>
    <content type="text"><![CDATA[原帖存于IT老兵博客。 Ubuntu16.04下升级Python到3.6.5 前言开发一个Python的系统，需要安装Python3.6以上的版本，由于使用的操作系统是Ubuntu16.04，默认带的Python是2.7.12和3.5，不满足需求，所以需要升级Python。 正文这里 有一篇帖子是说从源代码开始安装，这种方式原来尝试过，需要删除系统默认的软链命令，感觉比较粗暴，现在在想有没有更好的方式呢？ 找到一个帖子：http://ubuntuhandbook.org/index.php/2017/07/install-python-3-6-1-in-ubuntu-16-04-lts/，感觉简单了很多，经过了尝试，成功完成。 增加ppa仓库： 1sudo add-apt-repository ppa:jonathonf/python-3.6 add-apt-repository是一个增加apt仓库的命令，参考这里。 升级apt索引，更新python。 123sudo apt-get updatesudo apt-get install python3.6 更换系统默认的软链命令Python3到新的Python3.6。 123sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 1sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 2 update-alternatives命令可以修改系统默认命令的软链指向，参考这里，上面两句指令就是修改了系统默认的/usr/bin/python3 的软链指向，指向了两个位置，最后面的1和2是优先级。 通过以下命令，可以切换Python3的指向。 1sudo update-alternatives --config python3 由此，配置完成。 总结初步感觉，这样的修改要好于源代码安装那种方式，这样三个版本的Python可以共存，并且可以切换。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Ubuntu16.04</tag>
        <tag>升级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链：比特币数据存储]]></title>
    <url>%2Fblockchain%2Fbitcoin-data-storage.html</url>
    <content type="text"><![CDATA[完整的比特币系列笔记收藏于IT老兵驿站。区块链：比特币数据存储。 前言研究一下比特币的数据存储的格式。 正文 There are basically four pieces of data that are maintained:blocks/blk*.dat: the actual Bitcoin blocks, in network format, dumped in raw on disk. They are only needed for rescanning missing transactions in a wallet, reorganizing to a different part of the chain, and serving the block data to other nodes that are synchronizing. 简单翻译： blocks/blk*.dat，实际的区块数据，使用网络格式，直接存储在磁盘上。他们仅仅被用于：一个钱包重新扫描丢失的交易，重新组织去区块链的一部分，给另外的节点进行同步提供区块数据。 blocks/index/*: this is a LevelDB database that contains metadata about all known blocks, and where to find them on disk. Without this, finding a block would be very slow. 简单翻译： blocks/index/*，这是一个LevelDB数据库，包含了所有知道的区块的元数据，和在磁盘的哪里可以找到它们。没有这个，寻找一个区块是非常慢的。 chainstate/*: this is a LevelDB database with a compact representation of all currently unspent transaction outputs and some metadata about the transactions they are from. The data here is necessary for validating new incoming blocks and transactions. It can theoretically be rebuilt from the block data (see the -reindex command line option), but this takes a rather long time. Without it, you could still theoretically do validation indeed, but it would mean a full scan through the blocks (7 GB as of may 2013) for every output being spent. 简单翻译： 这是一个levelDB数据库，以压缩的形式存储所有当前未花费的交易输出（UTXO）以及关于这些交易来源的一些元数据。这里的数据对于验证新传入的块和交易是必要的。这些数据理论上可以从区块数据中重建（参看-reindex 命令选项），但是这需要花费很长时间。没有这些数据，理论上你也可以进行验证，但是它意味着一个全面地对区块 (2013年5月已经达到7GB ) 的扫描，来检查每一笔输出是否被花费。 blocks/rev*.dat: these contain “undo” data. You can see blocks as ‘patches’ to the chain state (they consume some unspent outputs, and produce new ones), and see the undo data as reverse patches. They are necessary for rolling back the chainstate, which is necessary in case of reorganisations.Note that the LevelDB’s are redundant in the sense that they can be rebuilt from the block data. But validation and other operations would become intolerably slow without them. 简单翻译： blocks/rev*.dat，包含着“undo”数据。你可以把区块数据看成是区块链状态（它们消费一些未花费的输出，然后产生新的一些）的补丁，可以把这些undo数据看做是反向的补丁。它们对于回滚区块状态非常重要，而回滚对于重新组织构建这种情况又是非常重要的。 参考https://en.bitcoin.it/wiki/Bitcoin_Core_0.11_(ch_2):_Data_Storage]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>比特币</tag>
        <tag>数据存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-pull的用法总结]]></title>
    <url>%2Fgit%2Fgit-pull-1.html</url>
    <content type="text"><![CDATA[前言本篇文章总结一下git-pull 的用法，主要过程是基于对官网的完整阅读，记录关键笔记和样例，加上自己的理解。整个过程是这样： 认真读完官网之后，才会知道它到底有多少内容，这样要比一次一次碎片化地去查要节省很多的时间，不这样读一遍，你怎么能知道git-pull有多少功能呢，如果不知道，回头遇到了需要这个功能的时候，都不知道怎么去查，要了解这个命令的外延。 当然，很多内容一下子是记不住的。记录适当的，或者说关键性的笔记来辅助记忆，将来可以多次去查看。 记录学习的心得。 粗读了一遍git-pull的文档，内容很多，恐怕一篇笔记不足以总结到位，可能要分为多篇笔记来总结。 博客IT老兵驿站。 正文语法git pull的作用是从一个仓库或者本地的分支拉取并且整合代码。 1git pull [&lt;options&gt;] [&lt;repository&gt; [&lt;refspec&gt;…​]] 描述git pull相当于 git fetch 跟着一个 git merge FETCH_HEAD。&lt;repository&gt;是仓库的名字，&lt;refspec&gt; 是分支的名字。如果都不写，会有一个默认值。 一个例子： 12345 A---B---C master on origin /D---E---F---G master ^ origin/master in your repository 远程的master分支到了C，本地的开发到了G。 123 A---B---C origin/master / \D---E---F---G---H master git pull之后会生成一个新的H，合并两个分支。 如果发生了冲突，可以使用git reset --merge进行回退。 options（选项）下面摘录几个常用的选项。 –allow-unrelated-historiesBy default, git merge command refuses to merge histories that do not share a common ancestor. This option can be used to override this safety when merging histories of two projects that started their lives independently. As that is a very rareoccasion, no configuration variable to enable this by default exists and will not be added. 允许无关的历史，这个选项，更多是在更改远程仓库的时候用到。 –ffWhen the merge resolves as a fast-forward, only update the branch pointer, without creating a merge commit. This is the default behavior.–no-ffCreate a merge commit even when the merge resolves as a fast-forward. This is the default behaviour when merging an annotated (and possibly signed) tag that is not stored in its natural place in refs/tags/ hierarchy.–ff-onlyRefuse to merge and exit with a non-zero status unless the current HEAD is already up to date or the merge can be resolved as a fast-forward. ff选项，这几个选项是说合并时是否开启fast-forward，快速合并，这个有在另外一篇帖子中详细讲解，这里就不赘述了。 实例实例：默认使用方式 1git pull 按照git branch 设置的默认跟踪的服务器和分支来拉取。 实例： 拉取远程服务器origin的master分支 1git pull origin master 总结git-pull的用法先总结到这里，还有很多需要细化的地方，一口吃不下，需要一口一口来。 参考https://git-scm.com/docs/git-pullhttps://www.atlassian.com/git/tutorials/syncing/git-pull]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git pull</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记（三）：文档]]></title>
    <url>%2Fmongodb%2Fmongodb-study-note-3.html</url>
    <content type="text"><![CDATA[完整的MongoDB学习笔记位于IT老兵博客。MongoDB学习笔记：文档。 前言最近有点忙，足有一周没有继续这个系列（而原计划是用一到两个月的时间完成这个系列的笔记的），按照20英里法则，这样的学习效果不好，今天抽空还是写了一篇，很多事，贵在坚持。 上一篇文章，其实题目起错了，不应该包含文档，因为上一篇文章的内容并没有文档的内容，不过CSDN的MD这种方式，修改题目总是不成功，只好在自己的博客修改，这样两篇文章的题目有些不一致了。 正文文档结构MongoDB的文档相当于MySQL的行，但是格式都是JSON的，而存储的格式是BSON，是二进制的保存JSON的格式。 文档的结构是域（field）-值（value）对，类似如下的结构： 1234567&#123; field1: value1, field2: value2, field3: value3, ... fieldN: valueN&#125; 值字段可以包含任意BSON数据类型，或者其他文档，或者数组，文档数组，例如： 12345678var mydoc = &#123; _id: ObjectId(&quot;5099803df3f4948bd2f98391&quot;), name: &#123; first: &quot;Alan&quot;, last: &quot;Turing&quot; &#125;, birth: new Date(&apos;Jun 23, 1912&apos;), death: new Date(&apos;Jun 07, 1954&apos;), contribs: [ &quot;Turing machine&quot;, &quot;Turing test&quot;, &quot;Turingery&quot; ], views : NumberLong(1250000) &#125; 域的名字和值各有一些限制，这个一般不会触及，所以暂时忽略—-当然，触及的时候，可以查一下手册。 点符号Array（数组）访问数组的方式，是数组名+“.”+索引（索引是从0开始），如下： 1&quot;&lt;array&gt;.&lt;index&gt;&quot; 举个例子（摘自官网）： 12345&#123; ... contribs: [ &quot;Turing machine&quot;, &quot;Turing test&quot;, &quot;Turingery&quot; ], ...&#125; 想访问数组的第三个元素“Turingery”，就使用“contribs.2”。 Embedded Documents（嵌入的文档）想访问嵌入的文档，使用文档名+“.”+“域名”的访问方式，语法如下： 1&quot;&lt;embedded document&gt;.&lt;field&gt;&quot; 举例： 123456&#123; ... name: &#123; first: &quot;Alan&quot;, last: &quot;Turing&quot; &#125;, contact: &#123; phone: &#123; type: &quot;cell&quot;, number: &quot;111-222-3333&quot; &#125; &#125;, ...&#125; “name.last”表示访问name的last域。“contact.phone.number”表示访问contact的phone域的number域。 文档的限制文档的大小最大的BSON文档的大小是16M。 文档域的顺序文档遵循着写的操作顺序 MongoDB preserves the order of the document fields following writeoperations… 文档保留着写操作的顺序–这句话有点没理解，是按照第一次写入的顺序吗？除了： _id域永远是第一位的。 更新有可能会改变顺序。 _id域_id域是主键，如果插入时忽略了，系统会自动加上这个字段。 By default, MongoDB creates a unique index on the _id field during the creation of a collection.The _id field is always the first field in the documents. If the server receives a document that does not have the _id field first, then the server will move the field to the beginning.The _id field may contain values of any BSON data type, other than an array. _id是作为唯一索引的。 _id永远是文档的第一个域。 _id可以是任何BSON类型，除了数组。 文档结构的其他使用用于查询过滤器（这篇帖子已经完成），更新规格文档、索引规格文档，这些需要将来细化，这里就不赘述了，官网这里也是给了一个引子，如果不具体看相应的内容，是没有用的。 总结足有一周没有继续这个系列了，心里有些慌，今天终于又完成一篇，心里略微踏实了一些—-尽快有些仓促。 参考https://docs.mongodb.com/manual/core/document/#bson-document-format]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>文档</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法和实例总结：df]]></title>
    <url>%2Flinux%2Fshell-command-df.html</url>
    <content type="text"><![CDATA[概要Linux下shell命令用法和实例总结：df。 博客最终的栖息地，IT老兵博客。 前言关于 Linux 的命令，之前做过一些整理，为什么整理呢，因为总用，总要一步一步去查，感觉还是应该做些整理，这样查的效率也会高一些，另外做了整理，很多命令可能也就记住了。不过呢，之前的整理，总是感觉有些问题，一时却没有发觉问题在哪里，因为感觉总是没有真正提高效率，还是且行且发现吧。 （2019-12-02补充）Linux 的命令需要反复使用，使用中记忆，越用越熟。 正文df命令用于显示文件系统磁盘空间使用情况。 命令格式 df [选项] [文件] 命令功能 df（disk filesystem 的简称）用于显示文件系统磁盘空间使用情况。默认显示单位为KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 命令参数 -a或–all：全部文件系统列表。 -h或–human-readable：方便阅读方式显示。 -H或–si：等于“-h”，但是计算式，1K=1000，而不是1K=1024。 -i或–inodes：显示inode信息。 -k或–kilobytes：区块为1024字节。 -l或–local：只显示本地文件系统。 -m或–megabytes：区块为1048576字节。 –no-sync：忽略sync命令。 -P或–portability：输出格式为POSIX。 –sync：在取得磁盘信息前，先执行sync命令。 -T或–print-type：文件系统类型。 –block-size=&lt;区块大小&gt;：指定区块大小。 -t&lt;文件系统类型&gt;或–type=&lt;文件系统类型&gt;：只显示选定文件系统的磁盘信息。 -x&lt;文件系统类型&gt;或–exclude-type=&lt;文件系统类型&gt;：不显示选定文件系统的磁盘信息。 –help：显示帮助信息。 –version：显示版本信息。 实用命令常用的命令就是对以上命令参数的单独使用、结合使用。 实例： 检查文件系统磁盘空间使用情况命令：df输出： 123456789101112Filesystem 1K-blocks Used Available Use% Mounted onudev 8196892 0 8196892 0% /devtmpfs 1643224 181376 1461848 12% /run/dev/mapper/ubuntu--vg-root 48914748 37149080 9257892 81% /tmpfs 8216100 0 8216100 0% /dev/shmtmpfs 5120 0 5120 0% /run/locktmpfs 8216100 0 8216100 0% /sys/fs/cgroup/dev/sda1 482922 478464 0 100% /boottmpfs 100 0 100 0% /run/lxcfs/controllers/dev/sdb 980385892 73288 930488860 1% /mnt/datatmpfs 1643224 0 1643224 0% /run/user/0tmpfs 1643224 0 1643224 0% /run/user/1002 上面各列分别是设备名称、总块数、总磁盘空间、已用磁盘空间、可用磁盘空间和文件系统上的挂载点。 实例： 使用字节单位显示本地磁盘（-h 指令的解释是human-readable，就是使用字节单位K、M、G等单位来显示；-l 表示本地），这个命令是最常用的命令命令：df -lh输出：123456789101112Filesystem Size Used Avail Use% Mounted onudev 7.9G 0 7.9G 0% /devtmpfs 1.6G 178M 1.4G 12% /run/dev/mapper/ubuntu--vg-root 47G 36G 8.9G 81% /tmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup/dev/sda1 472M 468M 0 100% /boottmpfs 100K 0 100K 0% /run/lxcfs/controllers/dev/sdb 935G 72M 888G 1% /mnt/datatmpfs 1.6G 0 1.6G 0% /run/user/0tmpfs 1.6G 0 1.6G 0% /run/user/1002 实例： 显示文件系统的类型命令：df -hT输出： 123456789101112Filesystem Type Size Used Avail Use% Mounted onudev devtmpfs 7.9G 0 7.9G 0% /devtmpfs tmpfs 1.6G 178M 1.4G 12% /run/dev/mapper/ubuntu--vg-root ext4 47G 36G 8.9G 81% /tmpfs tmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs tmpfs 5.0M 0 5.0M 0% /run/locktmpfs tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup/dev/sda1 ext2 472M 468M 0 100% /boottmpfs tmpfs 100K 0 100K 0% /run/lxcfs/controllers/dev/sdb ext4 935G 72M 888G 1% /mnt/datatmpfs tmpfs 1.6G 0 1.6G 0% /run/user/0tmpfs tmpfs 1.6G 0 1.6G 0% /run/user/1002 实例： 显示特定分区的信息描述：-hT将以可读格式显示/root的信息。命令：df -hT /root输出： 1/dev/vda1 ext4 296G 197G 84G 71% / 总结以上总结了一些自己常用的命令，遇到别的需求，可以结合上面的参数，思考怎么可以达到目的，所以就没有必要一一列举了，以后遇到还有很常用的实例，再总结附上，嗯，感觉这样就差不多了。这样就有点思路了，关键是要把用法和参数都总结出来，至于实例，则是总结一些常用的就好了。 参考https://linux.die.net/man/1/df]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>shell</tag>
        <tag>df</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 学习笔记：数据库、集合和文档]]></title>
    <url>%2Fmongodb%2Fmongodb-study-note-2.html</url>
    <content type="text"><![CDATA[概要MongoDB 学习笔记：数据库、集合和文档。 博客IT老兵博客。 前言一直在想，这个笔记应该按照什么思路，或者说，原则来记录呢？如果说按照当时学习的路线，那么一定是一条非常弯曲的曲线。还是按照官网的路线，配合着书籍，结合着自己的学习和工作过程来记录吧，这应该是最高效的学习路线—-尽管一开始看上去可能是较为漫长的。 这里，梳理一下总体的一些概念。 正文MongoDB 推出了一个云服务，叫做 Atlas，Atlas 和本地安装是两种选择，因为基本用的是本地安装，对这一部分还没有深入研究，所以暂时先跳过。 基本概念MongoDB 的三个概念：数据库、集合和文档，对应于关系型数据库中的数据库、表和行，这样更容易进行记忆。但是，它们是存在区别的。 如何创建数据库，总结在这里。 如何创建集合，总结在这里。 如何创建（插入）一条文档，总结在这里。 文档验证MongoDB 的文档是没有predefined shemas的，没有预定义的模式，这和 SQL 型的数据库不一样，它的每一条文档的结构都是可以不一样的，同时，MongoDB 也提供给你一些对文档进行约束的功能，帮助你可以去约束文档。 对于修改文档的结构，这个就更加简单了，你在更新文档的同时就可以去更新文档的结构，就不需要像 MySQL 那样去使用 DDL 语言。 总结重新整理了一下创建数据库、创建集合、创建文档，由这篇文章作为纲领，这样可以提纲挈领，看的比较明白。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>文档</tag>
        <tag>数据库</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记（一）：启程]]></title>
    <url>%2Fmongodb%2Fmongodb-study-note-1.html</url>
    <content type="text"><![CDATA[完整的MongoDB学习笔记位于IT老兵博客。MongoDB学习笔记。 概要MongoDB学习笔记。 博客IT老兵博客。 前言先介绍一下学习MongoDB的历程。 对于MongoDB，是从14年听说的，当时大概地看了一下，没有仔细研究， 说实话，当时心里其实是有点排斥的。因为“浸淫” SQL 多年（花了好些年去研究和使用 MySQL、Oracle，研究各种范式，做的项目都是基于 SQL的），突然出来一种NoSQL和反范式设计的概念，尤其是，突然有一堆刚毕业的，完全不懂范式，没有用范式做过系统的小朋友，跟你说“SQL已经过时”的时候，内心是非常排斥的，“你们连范式都还没有搞明白，都没有做出什么有点规模的系统，居然跟我奢谈说SQL已经过时”。 此外，还有一个因素，那就是一个老程序员，对于新技术的冲击，本能的有些排斥。新技术似乎会降低我们自身的价值，这个时候，我们就变成了保守派，想捍卫一些东西—-这是不可取的。 16年，在一家小公司任职，带领着团队研究新技术（小公司往往有机会去研究和使用新技术），大概看了看（看了一本好像是国人写的 MongoDB 的书），自以为大体是掌握了—-自以为是了。 18年，开始真正要用 MongoDB 做一个项目，才发现之前掌握的，完全不到位。这个时候，端正了心态，踏踏实实地开始学习，到现在有了一些小的心得。 这个时候回想，如果当初不要那么浮光略影地看书，而是踏踏实实地，结合着书籍，写一些实例应用，可能会掌握得更加清楚。这样，总共加起来，花费的学习时间应该会缩短很多，有的时候，要把有的事情做到位才会有效果，正是“纸上得来终觉浅，绝知此事要躬行”。 所以，为了总结这一个过程，记录笔记来跟踪整个学习的过程，到现在为止，已经总结了几篇 MongoDB 的使用方式，感觉还是不足以完整地记录整个学习过程，所以，再用这种方式记录一下，串联起来。 这个笔记，计划是花一个多月的时间，争取每周输出几篇，每一篇的篇幅不会太长，根据一个统一的原则来进行每篇内容的拆分，最终达到对MongoDB 的外围和内延的认识达到一个深度。总共的篇幅，暂时还不确定，达到了最终的目的为止，最后输出一个可以对学习 MongoDB 很有帮助的系列性的笔记。 本篇笔记，稍微啰嗦一下，现在可以理解很多书籍的前言了，这个时候总想表达一下写作的目的，中间的经历和艰辛，很多心得和感悟，从这个角度来说，本篇其实就是前言了。 准备“工欲善其事必先利其器”，想学好 MongoDB，先得选择好的学习资料。从从业十多年的经验中，我得到一个认知，“因快得慢”。举一个例子，当年快毕业，准备出去面试，需要学习linux的一些基本原理，这个时候，摆在面前的有几种资料，一种是《Linux与unix shell编程指南》，这是当时很经典的书籍，但是篇幅较长；另外一种是，速成的教程（原谅我连名字都不记得了，因为实在是没有太多的价值）。当时我果断的选择了后者，结果因为这种速成的教程往往只是讲了一些最没有价值的东西，就像快餐一样，真正有营养的东西，需要慢慢去吸收和消化。后来，其实又花了很多气力，踏踏实实地重来一遍，这样算来，第一遍花费的时间，一点都没有意义，所以，这就是计算机学习，为什么要去读经典的原理，这才是最节省时间的方式，走得越扎实，其实才是走的最快的方式。 官网，https://docs.mongodb.com/， 永远是学习的第一选择，MongoDB 的手册官网做的稍微有些复杂，但是它的例子比较丰富，这对于学习起来，帮助很大。不过，官网是英文的，对于很多人来说，是有些困难的（当然，如果想成为一个好的程序员，这个困难是需要客服的）。 Stack Overflow 网站，https://stackoverflow.com/， 寻找某些问题答案最好的网站，从10年开始，我已经把它看做是和官网差不多比重的资料网站了。 《MongoDB权威指南》，O’REILLY 的书一般质量都还不错，当然，在京东上看看评论，找一本其他评价高的书来作为资料都是可以的。 好了，神器在手，天下我有，准备启程。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《6 Rules of Thumb for MongoDB Schema Design》 Part 1 翻译和笔记]]></title>
    <url>%2Fmongodb%2Fmongodb-schema-design-note.html</url>
    <content type="text"><![CDATA[原帖位于IT老兵博客，沉淀着一个IT老兵对于这个行业的多年的认知。MongoDB如何设计数据模型。 前言在工作中遇到了要使用MongoDB，学习MongoDB，肯定不能仅仅停留于对一些指令的简单操作的掌握，就像当初学习MySQL一样，要了解一下如何使用MongoDB来设计数据库。这里，找到一篇很好的文章，转载在下面，配上一定的翻译和学习笔记，原文也不是很复杂，贴上原文，是为了不误导读者，也不误导自己，将来可以不断再纠正其中理解不准确的地方。 正文 By William Zola, Lead Technical Support Engineer at MongoDB “I have lots of experience with SQL, but I’m just a beginner with MongoDB. How do I model a one-to-N relationship?” This is one of the more common questions I get from users attending MongoDB office hours. I don’t have a short answer to this question, because there isn’t just one way, there’s a whole rainbow’s worth of ways. MongoDB has a rich and nuanced vocabulary for expressing what, in SQL, gets flattened into the term “One-to-N”. Let me take you on a tour of your choices in modeling One-to-N relationships. 笔记： MongoDB的新手往往会遇到一个问题，我应该怎么去定义一个one-to-N的关系呢？“there’s a whole rainbow’s worth of ways. ”这句应该怎么理解呢？ There’s so much to talk about here, I’m breaking this up into three parts. In this first part, I’ll talk about the three basic ways to model One-to-N relationships. In the second part I’ll cover more sophisticated schema designs, including denormalization and two-way referencing. And in the final part, I’ll review the entire rainbow of choices, and give you some suggestions for choosing among the thousands (really – thousands) of choices that you may consider when modeling a single One-to-N relationship. 笔记： 这里有很多需要讨论，笔记会将它分为三个部分来讨论。第一部分，也就是本篇文章，来讨论三种建立One-to-N关系模型的基本的方法；第二部分，讨论更复杂的模型设计，包括反范式（denormalization）和双向参考（two-way referencing）；最后一部分，将会复习整个选择的过程，并且给你们一些建立，来在上千的建立一个One-to-N关系的选择中做出判断。 Many beginners think that the only way to model “One-to-N” in MongoDB is to embed an array of sub-documents into the parent document, but that’s just not true. Just because you can embed a document, doesn’t mean you should embed a document. 笔记： 很多初学者会认为在MongoDB中建立一个“One-to-N”的模型只有一种方法，就是嵌入一个子文档的数组（array），这不是事实。确实是这样，看到的很多帖子就是这么去误导别人。 When designing a MongoDB schema, you need to start with a question that you’d never consider when using SQL: what is the cardinality of the relationship? Put less formally: you need to characterize your “One-to-N” relationship with a bit more nuance: is it “one-to-few”, “one-to-many”, or “one-to-squillions”? Depending on which one it is, you’d use a different format to model the relationship. 笔记： 在开始设计一个MongoDB的模式时，你需要考虑一个在使用SQL从来不需要考虑的问题：关系的基数是什么？具体来说，就是要考虑“one-to-few”，“one-to-many”, 或者“one-to-squillions”，这个基数不同，设计的格式也不同。 Basics: Modeling One-to-Few An example of “one-to-few” might be the addresses for a person. This is a good use case for embedding – you’d put the addresses in an array inside of your Person object: 123456789db.person.findOne()&#123; name: &apos;Kate Monster&apos;, ssn: &apos;123-456-7890&apos;, addresses : [ &#123; street: &apos;123 Sesame St&apos;, city: &apos;Anytown&apos;, cc: &apos;USA&apos; &#125;, &#123; street: &apos;123 Avenue Q&apos;, city: &apos;New York&apos;, cc: &apos;USA&apos; &#125; ]&#125; This design has all of the advantages and disadvantages of embedding. The main advantage is that you don’t have to perform a separate query to get the embedded details; the main disadvantage is that you have no way of accessing the embedded details as stand-alone entities. 笔记： 上面这是一个常见One-to-Few的例子，个人信息和地址的关系。好处在于你不用单独执行一个查询去获取嵌入的信息；坏处在于你无法根据作为一个单独的条目去访问一个嵌入的内容。这个例子很形象，在那本MySQL实例中，也涉及到人和地址的关系处理。就是说大千世界的一对多的关系其实不是那么一刀切的，而SQL对这个的处理能力是有限的，或者说SQL原本的设计是没有太多考虑这个因素的。这个应该结合那本书一起来讨论，待完成…… For example, if you were modeling a task-tracking system, each Person would have a number of Tasks assigned to them. Embedding Tasks inside the Person document would make queries like “Show me all Tasks due tomorrow” much more difficult than they need to be. I will cover a more appropriate design for this use case in the next post. Basics: One-to-Many An example of “one-to-many” might be parts for a product in a replacement parts ordering system. Each product may have up to several hundred replacement parts, but never more than a couple thousand or so. (All of those different-sized bolts, washers, and gaskets add up.) This is a good use case for referencing – you’d put the ObjectIDs of the parts in an array in product document. (For these examples I’m using 2-byte ObjectIDs because they’re easier to read: real-world code would use 12-byte ObjectIDs.) Each Part would have its own document: 12345678db.parts.findOne()&#123; _id : ObjectID(&apos;AAAA&apos;), partno : &apos;123-aff-456&apos;, name : &apos;#4 grommet&apos;, qty: 94, cost: 0.94, price: 3.99 Each Product would have its own document, which would contain an array of ObjectID references to the Parts that make up that Product: 1234567891011db.products.findOne()&#123; name : &apos;left-handed smoke shifter&apos;, manufacturer : &apos;Acme Corp&apos;, catalog_number: 1234, parts : [ // array of references to Part documents ObjectID(&apos;AAAA&apos;), // reference to the #4 grommet above ObjectID(&apos;F17C&apos;), // reference to a different Part ObjectID(&apos;D2AA&apos;), // etc ] You would then use an application-level join to retrieve the parts for a particular product: 1234// Fetch the Product document identified by this catalog numberproduct = db.products.findOne(&#123;catalog_number: 1234&#125;);// Fetch all the Parts that are linked to this Productproduct_parts = db.parts.find(&#123;_id: &#123; $in : product.parts &#125; &#125; ).toArray() ; 笔记： 这个例子是产品和配件的关系，是One-to-Many的关系。产品会有很多的配件，所以这里使用ObjectID来关联，这是一个单项关联。这个例子也是很常见的用来描述One-to-Many关系的。 For efficient operation, you’d need to have an index on ‘products.catalog_number’. Note that there will always be an index on ‘parts._id’, so that query will always be efficient. This style of referencing has a complementary set of advantages and disadvantages to embedding. Each Part is a stand-alone document, so it’s easy to search them and update them independently. One trade off for using this schema is having to perform a second query to get details about the Parts for a Product. (But hold that thought until we get to denormalizing in part 2.) 笔记： 好处在于每一个配件都有一个独立的文档，很容易查询和更新。交换就是需要单独执行一个查询去获取配件信息。 As an added bonus, this schema lets you have individual Parts used by multiple Products, so your One-to-N schema just became an N-to-N schema without any need for a join table! Basics: One-to-Squillions An example of “one-to-squillions” might be an event logging system that collects log messages for different machines. Any given host could generate enough messages to overflow the 16 MB document size, even if all you stored in the array was the ObjectID. This is the classic use case for “parent-referencing” – you’d have a document for the host, and then store the ObjectID of the host in the documents for the log messages. 12345678910111213db.hosts.findOne()&#123; _id : ObjectID(&apos;AAAB&apos;), name : &apos;goofy.example.com&apos;, ipaddr : &apos;127.66.66.66&apos;&#125;db.logmsg.findOne()&#123; time : ISODate(&quot;2014-03-28T09:42:41.382Z&quot;), message : &apos;cpu is on fire!&apos;, host: ObjectID(&apos;AAAB&apos;) // Reference to the Host document&#125; You’d use a (slightly different) application-level join to find the most recent 5,000 messages for a host: 1234// find the parent ‘host’ documenthost = db.hosts.findOne(&#123;ipaddr : &apos;127.66.66.66&apos;&#125;); // assumes unique index// find the most recent 5000 log message documents linked to that hostlast_5k_msg = db.logmsg.find(&#123;host: host._id&#125;).sort(&#123;time : -1&#125;).limit(5000).toArray() 笔记： 主机和日志的关系来体现One-to-Squillions，区别在于关系建立在了孩子身上，孩子指向了父亲。 Recap So, even at this basic level, there is more to think about when designing a MongoDB schema than when designing a comparable relational schema. You need to consider two factors: Will the entities on the “N” side of the One-to-N ever need to stand alone?What is the cardinality of the relationship: is it one-to-few; one-to-many; or one-to-squillions? 笔记： 在设计关系时，你需要考虑两个因素： One-to-N的“N”这边需要单独作为一个条目吗？关系的基数是什么：one-to-few；one-to-many；或者 one-to-squillions？Based on these factors, you can pick one of the three basic One-to-N schema designs: Embed the N side if the cardinality is one-to-few and there is no need to access the embedded object outside the context of the parent objectUse an array of references to the N-side objects if the cardinality is one-to-many or if the N-side objects should stand alone for any reasonsUse a reference to the One-side in the N-side objects if the cardinality is one-to-squillions笔记： 基于这些因素，你可以考虑这三个基本模式设计： 如果基数是one-to-few，并且在父对象的上下文之外没有访问嵌入的对象的需求，那么嵌入N边。如果基数是one-to-many，或者N边的对象基于一些原因需要单独展示，那么使用一个数组来指向N边的对象。如果基数是one-to-squillions，使用一个参考去指向One那边。 总结学习和梳理了这篇文章，感觉思路清晰了很多，MongoDB是在One-to-N这个领域做了很多设计，这可能也是跟当前的One-to-N的需求越来越多，而SQL对这个支持有限有关系。 待办的事情，配合总结一下MySQL的设计模式。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>schema</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java：Shiro的架构学习笔记]]></title>
    <url>%2Fjava%2Fjava-shiro-note.html</url>
    <content type="text"><![CDATA[概要Java：Shiro的架构学习笔记。 博客IT老兵博客 前言张开涛的第一章 Shiro简介——《跟我学Shiro》，其实是解读了一下Shiro的架构这篇文章，本着寻根究底的态度，我再一次去阅读这篇文章。为什么说是再一次呢？因为之前读过好几次了，不过就是没有完全理解明白，自己也说不好卡在哪里了，包括张开涛的文章，我也读过两遍了，这次第三遍读，一下子豁然开朗，然后不明白之前为啥就没读明白–这可能就是“书读百遍，其义自见”的道理吧。 正文3个主要的概念：Subject, SecurityManager和Realms。 Subject可以是一个用户，但不仅仅可以代表一个用户，所有对这个系统的外部请求的主体都可以看成是一个Subject，例如一个service，这里是做了一个抽象概括的设计。（这个我能理解，如果你理解不了的话，那说明你还没有接触过相关的业务，例如SSO，那就先把它理解成一个用户，也没有关系。将来总有一天，你会明白，会回来和我一起唱这首《当当当》。） SecurityManager Shiro设计的核心的逻辑都在这里面，但是，我们应该可以先不理会它是怎么工作的，先把它当做一个黑匣子，它有它自己运行的逻辑，Shiro的核心调度逻辑是在这里完成的。 Realms 这个单词的意思是领域，范围。原文这么说： Realms act as the ‘bridge’ or ‘connector’ between Shiro and your application’s security data. When it comes time to actually interact with security-related data like user accounts to perform authentication (login) and authorization (access control), Shiro looks up many of these things from one or more Realms configured for an application. In this sense a Realm is essentially a security-specific DAO: it encapsulates connection details for data sources and makes the associated data available to Shiro as needed. When configuring Shiro, you must specify at least one Realm to use for authentication and/or authorization. The SecurityManager may be configured with multiple Realms, but at least one is required. 就是说和安全相关数据（security-specific）打交道的是这个对象，有关登录认证、授权（访问控制）都是通过它来打交道，或者说，通过不同的realm来和相关的“机构”（打个比方）打交道，每个机构有自己的realm，再或者说，realm可以理解成DAO，去访问相关的数据。 这个地方可以这么理解，Shiro是把认证、获取授权相关的流程，相对固定的流程固化在代码中，把变化的内容暴露出来给使用者去配置，这里的realm就是把认证的信息、授权的信息暴露出来给使用者来进行配置。 更具体的分析： Subject：A security-specific ‘view’ of the entity (user, 3rd-party service, cron job, etc) currently interacting with the software. 一个实体的安全相关的view–这个概念还需要好好理解一下，怎么被称为一个view呢？这里应该是以MVC的概念去划分，那么这里称成一个view就相对来说，好理解一些，因为它是一些信息的聚合体。 SecurityManager又分为了一些子模块： Authenticator Authenticator (org.apache.shiro.authc.Authenticator)The Authenticator is the component that is responsible for executing and reacting to authentication (log-in) attempts by users. When a user tries to log-in, that logic is executed by the Authenticator. The Authenticator knows how to coordinate with one or more Realms that store relevant user/account information. The data obtained from these Realms is used to verify the user’s identity to guarantee the user really is who they say they are.Authentication Strategy (org.apache.shiro.authc.pam.AuthenticationStrategy)If more than one Realm is configured, the AuthenticationStrategy will coordinate the Realms to determine the conditions under which an authentication attempt succeeds or fails (for example, if one realm succeeds but others fail, is the attempt successful? Must all realms succeed? Only the first?). Authenticator：认证器，用来负责用户登录认证，它对应着一个或者多个Realm。Authentication Strategy：认证策略，如果多个Realm 被配置，那么Authentication Strategy来负责协调这些Realm 产生矛盾的时候，该如何处理，例如一个realm成功，而其它的失败了，改怎么办，等等。在这一点上，张开涛的文章解释的不是太准确。 Authorizer The Authorizer is the component responsible determining users’ access control in the application. It is the mechanism that ultimately says if a user is allowed to do something or not. Like the Authenticator, the Authorizer also knows how to coordinate with multiple back-end data sources to access role and permission information. The Authorizer uses this information to determine exactly if a user is allowed to perform a given action. Authorizer：授权器，负责确认用户的访问权限。这些信息需要由使用者来维护，所以也是暴露出来，交由使用者配置一个数据源给Shiro去调用。 SessionManager SessionManager (org.apache.shiro.session.mgt.SessionManager)The SessionManager knows how to create and manage user Session lifecycles to provide a robust Session experience for users in all environments. This is a unique feature in the world of security frameworks - Shiro has the ability to natively manage user Sessions in any environment, even if there is no Web/Servlet or EJB container available. By default, Shiro will use an existing session mechanism if available, (e.g. Servlet Container), but if there isn’t one, such as in a standalone application or non-web environment, it will use its built-in enterprise session management to offer the same programming experience. The SessionDAO exists to allow any datasource to be used to persist sessions.SessionDAO (org.apache.shiro.session.mgt.eis.SessionDAO)The SessionDAO performs Session persistence (CRUD) operations on behalf of the SessionManager. This allows any data store to be plugged in to the Session Management infrastructure. SessionManager：session管理器，Shiro没有完全依赖HTTP的session，而是设计了一个独立的session。 SessionDAO：session的DAO，用来处理session数据的保存。 CacheManager The CacheManager creates and manages Cache instance lifecycles used by other Shiro components. Because Shiro can access many back-end data sources for authentication, authorization and session management, caching has always been a first-class architectural feature in the framework to improve performance while using these data sources. Any of the modern open-source and/or enterprise caching products can be plugged in to Shiro to provide a fast and efficient user-experience. CacheManager：缓存管理器，用于加快访问速度，但是这里只是一个管理器，具体的缓存工具应该是由使用者来定义的。 Cryptography Cryptography is a natural addition to an enterprise security framework. Shiro’s crypto package contains easy-to-use and understand representations of crytographic Ciphers, Hashes (aka digests) and different codec implementations. All of the classes in this package are carefully designed to be very easy to use and easy to understand. Anyone who has used Java’s native cryptography support knows it can be a challenging animal to tame. Shiro’s crypto APIs simplify the complicated Java mechanisms and make cryptography easy to use for normal mortal human beings. Cryptography：加密模块。 Realms As mentioned above, Realms act as the ‘bridge’ or ‘connector’ between Shiro and your application’s security data. When it comes time to actually interact with security-related data like user accounts to perform authentication (login) and authorization (access control), Shiro looks up many of these things from one or more Realms configured for an application. You can configure as many Realms as you need (usually one per data source) and Shiro will coordinate with them as necessary for both authentication and authorization. Realms：上面介绍过。 SecurityManager As stated previously, the application’s SecurityManager performs security operations and manages state for all application users. In Shiro’s default SecurityManager implementations, this includes:AuthenticationAuthorizationSession ManagementCache ManagementRealm coordinationEvent propagation“Remember Me” ServicesSubject creationLogout and more. 在我的理解，其实SecurityManager就是Shiro的核心，它决定了业务的处理流，在什么样的时机去调用哪一个服务，这些服务会有父类来占位，可以由使用者定义子类来进行具体实现的修改。 总结又阅读了一遍架构这篇文章，结合着张开涛的文章，感觉明白了不少，现在感觉Shiro 还是挺简单的，有个两三天应该就大体理解了，不明白当时怎么就堵住了，陷入了思维的死胡同。 参考https://shiro.apache.org/architecture.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-reflog的用法总结]]></title>
    <url>%2Fgit%2Fgit-reflog.html</url>
    <content type="text"><![CDATA[概要git-reflog的用法总结。 博客博客地址：IT老兵驿站。 前言git-reflog是用来恢复本地错误操作很重要的一个命令，所以在这里对它进行一下整理。 正文语法1git reflog &lt;subcommand&gt; &lt;options&gt; 具体的用法 1234git reflog [show] [log-options] [&lt;ref&gt;]git reflog expire [--expire=&lt;time&gt;] [--expire-unreachable=&lt;time&gt;] [--rewrite] [--updateref] [--stale-fix] [--dry-run | -n] [--verbose] [--all | &lt;refs&gt;…​]git reflog delete [--rewrite] [--updateref] [--dry-run | -n] [--verbose] ref@&#123;specifier&#125;…​git reflog exists &lt;ref&gt; Reference logs, or “reflogs”, record when the tips of branches andother references were updated in the local repository. 这句话怎么理解呢，记录了“when”，时间？这个地方理解错误了，应该是在branches和其它参考在本地的仓库中被更新时来做记录。 翻译：Reference logs（参考日志），或者叫做”reflogs”，记录了分支的tips（提示信息？）或者其他参考在本地仓库被更新的时间（when）。 问题来了，这个参考日志的作用是什么，和日志又有什么区别呢？ 找到了这篇帖子： 1234567git log shows the current HEAD and its ancestry. That is, it prints the commit HEAD points to, then its parent, its parent, and so on. It traverses back through the repo&apos;s ancestry, by recursively looking up each commit&apos;s parent.(In practice, some commits have more than one parent. To see a more representative log, use a command like git log --oneline --graph --decorate.)git reflog doesn&apos;t traverse HEAD&apos;s ancestry at all. The reflog is an ordered list of the commits that HEAD has pointed to: it&apos;s undo history for your repo. The reflog isn&apos;t part of the repo itself (it&apos;s stored separately to the commits themselves) and isn&apos;t included in pushes, fetches or clones; it&apos;s purely local.Aside: understanding the reflog means you can&apos;t really lose data from your repo once it&apos;s been committed. If you accidentally reset to an older commit, or rebase wrongly, or any other operation that visually &quot;removes&quot; commits, you can use the reflog to see where you were before and git reset --hard back to that ref to restore your previous state. Remember, refs imply not just the commit but the entire history behind it. 上面就讲的比较清楚了，总结一下： git log是显示当前的HEAD和它的祖先的，递归是沿着当前指针的父亲，父亲的父亲，……，这样的原则。git reflog根本不遍历HEAD的祖先。它是HEAD所指向的一个顺序的提交列表：它的undo历史。reflog并不是repo（仓库）的一部分，它单独存储，而且不包含在pushes，fetches或者clones里面，它纯属是本地的。reflog可以很好地帮助你恢复你误操作的数据，例如你错误地reset了一个旧的提交，或者rebase，……，这个时候你可以使用reflog去查看在误操作之前的信息，并且使用git reset --hard 去恢复之前的状态。下面研究一下这个命令的具体用法。 先了解一下git的版本表示方法： HEAD@{2} means “where HEAD used to be two moves ago”, master@{one.week.ago}means “where master used to point to one week ago in this local repository” HEAD@{2}表示HEAD指针在两次移动之前的情况；而 master@{one.week.ago}表示master在本地仓库一周之前的情况。 “show”子命令显示所指定的参考的日志。 实例： 显示HEAD的reflog。 123456789101112$ git reflog showef64f10 (HEAD -&gt; BlueLake_theme) HEAD@&#123;0&#125;: commit: 新增ethereum-programming-intr oduction122e0ec (origin/BlueLake_theme) HEAD@&#123;1&#125;: commit: 移除了冗余的ethereum-rationale 文章c17fbbb HEAD@&#123;2&#125;: commit: 新增git-change-server-password文章1603d1a HEAD@&#123;3&#125;: pull: Merge made by the &apos;recursive&apos; strategy.0ce1e93 HEAD@&#123;4&#125;: commit: 新增了以太坊原理c73503c HEAD@&#123;5&#125;: commit: 修改了-X-Frame-Options的关键字6af02f6 HEAD@&#123;6&#125;: commit: 新增了git-tag的文章；修改了git其他的文章，规范了名字、 关键字9087fbd HEAD@&#123;7&#125;: commit: 新增了gti-reset文章039d95c HEAD@&#123;8&#125;: commit: 移除了没用的目录ff72601 HEAD@&#123;9&#125;: commit: 修改成了next主题ef64f10 (HEAD -&gt; BlueLake_theme) HEAD@&#123;0&#125;: commit: 新增ethereum-programming-intr oduction 从上图可以看到，几乎所有的操作都记录在其中，这个就像MySQL，随时可以回滚。 “expire”子命令会删除掉更老的reflog条目。 “delete”子命令从reflog中删除一个条目。 “exists”子命令检查一个ref是否有一个reflog。 这几个命令就相对比较简单了，以后再尝试了。 参考https://git-scm.com/docs/git-reflog]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>用法</tag>
        <tag>git reflog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链：教程 | 以太坊智能合约编程之菜鸟教程及学习笔记]]></title>
    <url>%2Fblockchain%2Fethereum-programming-introduction.html</url>
    <content type="text"><![CDATA[区块链：教程 | 以太坊智能合约编程之菜鸟教程及学习笔记。 这篇介绍以太坊合约的文章写得很好，在查找了这么多资料，进行对比之后，感觉阅读这一篇就可以大体理解以太坊编程的原理，如果对个别的知识点还有点含糊，可以相应地去查一查，就是以这篇为主干，别的资料为辅。稍微整理了一下格式，以及修改了一些半角符号。 译注：原文首发于ConsenSys开发者博客，原作者为Eva以及ConsenSys的开发团队。如果您想要获取更多及时信息，可以访问ConsenSys首页点击左下角Newsletter订阅邮件。本文的翻译获得了ConsenSys创始人Lubin先生的授权。 有些人说以太坊太难对付，于是我们(译注：指Consensys, 下同)写了这篇文章来帮助大家学习如何利用以太坊编写智能合约和应用。这里所用到的工具，钱包，应用程序以及整个生态系统仍处于开发状态，它们将来会更好用！ 第一部分概述，讨论了关键概念，几大以太坊客户端以及写智能合约用到的编程语言。 第二部分讨论了总体的工作流程，以及目前流行的一些DApp框架和工具。 第三部分主要关于编程，我们将学习如何使用Truffle来为智能合约编写测试和构建DApp。 第一部分 概述如果你对诸如比特币以及其工作原理等密码学货币的概念完全陌生，我们建议你先看看Andreas Antonopoulos所著的Bitcoin Book的头几章，然后读一下以太坊白皮书。(译注：以太坊白皮书中文版请看 http://ethfans.org/posts/ethereum-whitepaper) 如果你觉得白皮书中的章节太晦涩，也可以直接动手来熟悉以太坊。在以太坊上做开发并不要求你理解所有那些“密码经济计算机科学”(crypto economic computer science)，而白皮书的大部分是关于以太坊想对于比特币架构上的改进。 新手教程ethereum.org提供了官方的新手入门教程，以及一个代币合约和众筹合约的教程。合约语言Solidity也有官方文档。学习智能合约的另一份不错的资料（也是我的入门资料）是dappsForBeginners，不过现在可能有些过时了。 这篇文章的目的是成为上述资料的补充，同时介绍一些基本的开发者工具，使入门以太坊，智能合约以及构建DApps(decentralized apps, 分布式应用)更加容易。我会试图按照我自己(依然是新手)的理解来解释工作流程中的每一步是在做什么，我也得到了ConsenSys酷酷的开发者们的许多帮助。 基本概念了解这些名词是一个不错的开始： 公钥加密系统。 Alice有一把公钥和一把私钥。她可以用她的私钥创建数字签名，而Bob可以用她的公钥来验证这个签名确实是用Alice的私钥创建的，也就是说，确实是Alice的签名。当你创建一个以太坊或者比特币钱包的时候，那长长的0xdf...5f地址实质上是个公钥，对应的私钥保存某处。类似于Coinbase的在线钱包可以帮你保管私钥，你也可以自己保管。如果你弄丢了存有资金的钱包的私钥，你就等于永远失去了那笔资金，因此你最好对私钥做好备份。过来人表示：通过踩坑学习到这一点是非常痛苦的… 点对点网络。 就像BitTorrent, 以太坊分布式网络中的所有节点都地位平等，没有中心服务器。(未来会有半中心化的混合型服务出现为用户和开发者提供方便，这我们后面会讲到。) 区块链。 区块链就像是一个全球唯一的帐簿，或者说是数据库，记录了网络中所有交易历史。 以太坊虚拟机(EVM)。 它让你能在以太坊上写出更强大的程序（比特币上也可以写脚本程序）。它有时也用来指以太坊区块链，负责执行智能合约以及一切。 节点。 你可以运行节点，通过它读写以太坊区块链，也即使用以太坊虚拟机。完全节点需要下载整个区块链。轻节点仍在开发中。 矿工。 挖矿，也就是处理区块链上的区块的节点。这个网页可以看到当前活跃的一部分以太坊矿工：stats.ethdev.com。 工作量证明。 矿工们总是在竞争解决一些数学问题。第一个解出答案的(算出下一个区块)将获得以太币作为奖励。然后所有节点都更新自己的区块链。所有想要算出下一个区块的矿工都有与其他节点保持同步，并且维护同一个区块链的动力，因此整个网络总是能达成共识。(注意：以太坊正计划转向没有矿工的权益证明系统(POS)，不过那不在本文讨论范围之内。) 以太币。 缩写ETH。一种你可以购买和使用的真正的数字货币。这里是可以交易以太币的其中一家交易所的走势图。在写这篇文章的时候，1个以太币价值65美分。 Gas。(汽油) 在以太坊上执行程序以及保存数据都要消耗一定量的以太币，Gas是以太币转换而成。这个机制用来保证效率。 DApp。 以太坊社区把基于智能合约的应用称为去中心化的应用程序(Decentralized App)。DApp的目标是(或者应该是)让你的智能合约有一个友好的界面，外加一些额外的东西，例如IPFS（可以存储和读取数据的去中心化网络，不是出自以太坊团队但有类似的精神)。DApp可以跑在一台能与以太坊节点交互的中心化服务器上，也可以跑在任意一个以太坊平等节点上。(花一分钟思考一下：与一般的网站不同，DApp不能跑在普通的服务器上。他们需要提交交易到区块链并且从区块链而不是中心化数据库读取重要数据。相对于典型的用户登录系统，用户有可能被表示成一个钱包地址而其它用户数据保存在本地。许多事情都会与目前的web应用有不同架构。) 如果想看看从另一个新手视角怎么理解这些概念，请读Just Enough Bitcoin for Ethereum。 以太坊客户端，智能合约语言编写和部署智能合约并不要求你运行一个以太坊节点。下面有列出基于浏览器的IDE和API。但如果是为了学习的话，还是应该运行一个以太坊节点，以便理解其中的基本组件，何况运行节点也不难。 运行以太坊节点可用的客户端以太坊有许多不同语言的客户端实现（即多种与以太坊网络交互的方法），包括C++, Go, Python, Java, Haskell等等。为什么需要这么多实现？不同的实现能满足不同的需求（例如Haskell实现的目标是可以被数学验证），能使以太坊更加安全，能丰富整个生态系统。 在写作本文时，我使用的是Go语言实现的客户端geth (go-ethereum)，其他时候还会使用一个叫testrpc的工具, 它使用了Python客户端pyethereum。后面的例子会用到这些工具。 注: 我曾经使用过C++的客户端，现在仍然在用其中的ethminer组件和geth配合挖矿，因此这些不同的组件是可以一起工作的。关于挖矿：挖矿很有趣，有点像精心照料你的室内盆栽，同时又是一种了解整个系统的方法。虽然以太币现在的价格可能连电费都补不齐，但以后谁知道呢。人们正在创造许多酷酷的DApp, 可能会让以太坊越来越流行。 交互式控制台。 客户端运行起来后，你就可以同步区块链，建立钱包，收发以太币了。使用geth的一种方式是通过Javascript控制台（JavaScript console, 类似你在chrome浏览器里面按F12出来的那个，只不过是跑在终端里）。此外还可以使用类似cURL的命令通过JSON RPC来与客户端交互。本文的目标是带大家过一边DApp开发的流程，因此这块就不多说了。但是我们应该记住这些命令行工具是调试，配置节点，以及使用钱包的利器。 在测试网络运行节点。 如果你在正式网络运行geth客户端，下载整个区块链与网络同步会需要相当时间。（你可以通过比较节点日志中打印的最后一个块号和stats.ethdev.com上列出的最新块来确定是否已经同步。) 另一个问题是在正式网络上跑智能合约需要实实在在的以太币。在测试网络上运行节点的话就没有这个问题。此时也不需要同步整个区块链，创建一个自己的私有链就勾了，对于开发来说更省时间。 testrpc。 用geth可以创建一个测试网络，另一种更快的创建测试网络的方法是使用testrpc。Testrpc可以在启动时帮你创建一堆存有资金的测试账户。它的运行速度也更快因此更适合开发和测试。你可以从testrpc起步，然后随着合约慢慢成型，转移到geth创建的测试网络上 - 启动方法很简单，只需要指定一个networkid：geth --networkid &quot;12345&quot;。这里是testrpc的代码仓库，下文我们还会再讲到它。 接下来我们来谈谈可用的编程语言，之后就可以开始真正的编程了。 写智能合约用的编程语言用Solidity就好。 要写智能合约有好几种语言可选：有点类似Javascript的Solidity, 文件扩展名是.sol和Python接近的Serpent, 文件名以.se结尾。还有类似Lisp的LLL。Serpent曾经流行过一段时间，但现在最流行而且最稳定的要算是Solidity了，因此用Solidity就好。听说你喜欢Python? 用Solidity。 solc编译器。 用Solidity写好智能合约之后，需要用solc来编译。它是一个来自C++客户端实现的组件（又一次，不同的实现产生互补），这里是安装方法。如果你不想安装solc也可以直接使用基于浏览器的编译器，例如Solidity real-time compiler或者Cosmo。后文有关编程的部分会假设你安装了solc。 注意：以太坊正处于积极的开发中，有时候新的版本之间会有不同步。确认你使用的是最新的dev版本，或者稳定版本。如果遇到问题可以去以太坊项目对应的Gitter聊天室或者forums.ethereum.org上问问其他人在用什么版本。 web3.js API。 当Solidity合约编译好并且发送到网络上之后，你可以使用以太坊的web3.js JavaScript API来调用它，构建能与之交互的web应用。 以上就是在以太坊上编写智能合约和构建与之交互的DApp所需的基本工具。 第二部分 DApp框架，工具以及工作流程DApp开发框架虽然有上文提到的工具就可以进行开发了，但是使用社区大神们创造的框架会让开发更容易。 Truffle and Embark。 是Truffle把我领进了门。在Truffle出现之前的那个夏天，我目睹了一帮有天分的学生是如何不眠不休的参加一个hackathon（编程马拉松）活动的，虽然结果相当不错，但我还是吓到了。然后Truffle出现了，帮你处理掉大量无关紧要的小事情，让你可以迅速进入写代码-编译-部署-测试-打包DApp这个流程。另外一个相似的DApp构建与测试框架是Embark。我只用过Truffle, 但是两个阵营都拥有不少DApp大神。 Meteor。 许多DApp开发者使用的另一套开发栈由web3.js和Meteor组成，Meteor是一套通用webapp开发框架（ethereum-meteor-wallet项目提供了一个很棒的入门实例，而SilentCiero正在构建大量Meteor与web3.js和DApp集成的模板）。我下载并运行过一些不错的DApp是以这种方式构造的。在11月9日至13日的以太坊开发者大会ÐΞVCON1上将有一些有趣的讨论，是关于使用这些工具构建DApp以及相关最佳实践的（会议将会在YouTube上直播）。 APIs。 BlockApps.net打算提供一套RESTful API给DApp使用以免去开发者运行本地节点的麻烦，这个中心化服务是基于以太坊Haskell实现的。这与DApp的去中心化模型背道而驰，但是在本地无法运行以太坊节点的场合非常有用，比如在你希望只有浏览器或者使用移动设备的用户也能使用你的DApp的时候。BlockApps提供了一个命令行工具bloc，注册一个开发者帐号之后就可以使用。 许多人担心需要运行以太坊节点才能使用DApp的话会把用户吓跑，其实包括BlockApps在内的许多工具都能解决这个问题。Metamask允许你在浏览器里面使用以太坊的功能而无需节点，以太坊官方提供的AlethZero或者AlethOne是正在开发中有易用界面的客户端，ConsenSys正在打造一个轻钱包LightWallet，这些工具都会让DApp的使用变得更容易。轻客户端和水平分片(sharding)也在计划和开发之中。这是一个能进化出混合架构的P2P生态系统。 智能合约集成开发环境 (IDE)IDE。 以太坊官方出品了用来编写智能合约的Mix IDE，我还没用过但会尽快一试。 基于浏览器的IDE。 Solidity real-time compiler和Cosmo都可以让你快速开始在浏览器中编写智能合约。你甚至可以让这些工具使用你的本地节点，只要让本地节点开一个端口（注意安全！这些工具站点必须可信，而且千万不要把你的全部身家放在这样一个本地节点里面！Cosmo UI上有如何使用geth做到这一点的指引）。在你的智能合约调试通过之后，可以用开发框架来给它添加用户界面和打包成DApp，这正是Truffle的工作，后面的编程章节会有详细讲解。 Ether.Camp正在开发另一个强大的企业级浏览器IDE。他们的IDE将支持沙盒测试网络，自动生成用于测试的用户界面（取代后文将展示的手动编写测试），以及一个测试交易浏览器test.ether.camp。当你的合约准备正式上线之前，使用他们的测试网络会是确保你的智能合约在一个接近真实的环境工作正常的好方法。他们也为正式网络提供了一个交易浏览器frontier.ether.camp，上面可以看到每一笔交易的细节。在本文写作时Ether.Camp的IDE还只能通过邀请注册，预计很快会正式发布。 合约和Dapp示例。 在Github上搜索DApp仓库和.sol文件可以看到进行中的有趣东西。这里有一个DApp大列表：dapps.ethercasts.com，不过其中一些项目已经过时。Ether.fund/contracts上有一些Solidity和Serpent写的合约示例，但是不清楚这些例子有没有经过测试或者正确性验证。11月12日的开发者大会ÐΞVCON1将会有一整天的DApp主题演讲。 部署智能合约的流程流程如下： 启动一个以太坊节点 (例如geth或者testrpc)。 使用solc 编译 智能合约。 =&gt; 获得二进制代码。 将编译好的合约部署到网络。（这一步会消耗以太币，还需要使用你的节点的默认地址或者指定地址来给合约签名。） =&gt; 获得合约的区块链地址和ABI（合约接口的JSON表示，包括变量，事件和可以调用的方法）。(译注：作者在这里把ABI与合约接口弄混了。ABI是合约接口的二进制表示。) 用web3.js提供的JavaScript API来调用合约。（根据调用的类型有可能会消耗以太币。） 下图详细描绘了这个流程： 你的DApp可以给用户提供一个界面先部署所需合约再使用之（如图1到4步），也可以假设合约已经部署了（常见方法），直接从使用合约（如图第6步）的界面开始。 第三部分 编程在Truffle中进行测试Truffle用来做智能合约的测试驱动开发(TDD)非常棒，我强烈推荐你在学习中使用它。它也是学习使用JavaScript Promise的一个好途径，例如deferred和异步调用。Promise机制有点像是说“做这件事，如果结果是这样，做甲，如果结果是那样，做乙… 与此同时不要在那儿干等着结果返回，行不？”。Truffle使用了包装web3.js的一个JS Promise框架Pudding（因此它为为你安装web3.js）。(译注：Promise是流行于JavaScript社区中的一种异步调用模式。它很好的封装了异步调用，使其能够灵活组合，而不会陷入callback hell.) Transaction times。 Promise对于DApp非常有用，因为交易写入以太坊区块链需要大约12-15秒的时间。即使在测试网络上看起来没有那么慢，在正式网络上却可能会要更长的时间（例如你的交易可能用光了Gas，或者被写入了一个孤儿块）。 下面让我们给一个简单的智能合约写测试用例吧。 使用Truffle首先确保你 1.安装好了solc以及 2.testrpc。（testrpc需要Python和pip。如果你是Python新手，你可能需要用virtualenv来安装，这可以将Python程序库安装在一个独立的环境中。） PS：在windows安装这个非常麻烦，要有mingw环境（在windows上模拟linux的环境），要安装python和pip，然后还会报告pkg-config的错误，参考这里的帖子，最后还是存在问题。后来在这里找到一个js的版本，终于解决。 接下来安装 3.Truffle（你可以使用NodeJS’s npm来安装：npm install -g truffle, -g开关可能会需要sudo）。安装好之后，在命令行中输入truffle list来验证安装成功。然后创建一个新的项目目录（我把它命名为’conference’），进入这个目录，运行truffle init。该命令会建立如下的目录结构： 现在让我们在另一个终端里通过执行testrpc来启动一个节点（你也可以用geth）： 回到之前的终端中，输入truffle deploy。这条命令会部署之前truffle init产生的模板合约到网络上。任何你可能遇到的错误信息都会在testrpc的终端或者执行truffle的终端中输出。 在开发过程中你随时可以使用truffle compile命令来确认你的合约可以正常编译（或者使用solc YourContract.sol），truffle deploy来编译和部署合约，最后是truffle test来运行智能合约的测试用例。 PS：运行之后，是什么样呢？怎么查看呢？我的终端上显示如下，这个说明什么呢？ 1234567891011121314151617181920212223242526272829net_versioneth_accountsnet_versioneth_accountseth_accountseth_accountsnet_versionnet_versioneth_sendTransaction Transaction: 0x86d59c95d54482646fb1cca5b81a72123f833cb35168b2de9249921845643c20 Contract created: 0x4b46552283603e7f90da3d41c33b7b19d68bf248 Gas usage: 277462 Block Number: 1 Block Time: Fri Aug 10 2018 10:56:31 GMT+0800 (中国标准时间)eth_newBlockFiltereth_getFilterChangeseth_getTransactionReceipteth_getCodeeth_uninstallFiltereth_sendTransaction Transaction: 0x66d6279e0d0bb0798395af14a7092c9d6ce35ceb243ec57d7ce0cc9c18f6b31e Gas usage: 42008 Block Number: 2 Block Time: Fri Aug 10 2018 10:56:32 GMT+0800 (中国标准时间)eth_getTransactionReceipt PS：关于truffle的内容有些过时，需要重新整理一篇文章。 第一个合约下面是一个针对会议的智能合约，通过它参会者可以买票，组织者可以设置参会人数上限，以及退款策略。本文涉及的所有代码都可以在这个代码仓库找到。 contract Conference { address public organizer; mapping (address =&gt; uint) public registrantsPaid; uint public numRegistrants; uint public quota; event Deposit(address _from, uint _amount); // so you can log these events event Refund(address _to, uint _amount); function Conference() { // Constructor organizer = msg.sender; quota = 500; numRegistrants = 0; } function buyTicket() public returns (bool success) { if (numRegistrants &gt;= quota) { return false; } registrantsPaid[msg.sender] = msg.value; numRegistrants++; Deposit(msg.sender, msg.value); return true; } function changeQuota(uint newquota) public { if (msg.sender != organizer) { return; } quota = newquota; } function refundTicket(address recipient, uint amount) public { if (msg.sender != organizer) { return; } if (registrantsPaid[recipient] == amount) { address myAddress = this; if (myAddress.balance &gt;= amount) { recipient.send(amount); registrantsPaid[recipient] = 0; numRegistrants--; Refund(recipient, amount); } } } function destroy() { // so funds not locked in contract forever if (msg.sender == organizer) { suicide(organizer); // send funds to organizer } } } 接下来让我们部署这个合约。（注意：本文写作时我使用的是Mac OS X 10.10.5, solc 0.1.3+ (通过brew安装)，Truffle v0.2.3, testrpc v0.1.18 (使用venv)） 部署合约 (译注：图中步骤翻译如下：） 使用truffle部署智能合约的步骤：1. truffle init (在新目录中) =&gt; 创建truffle项目目录结构2. 编写合约代码，保存到contracts/YourContractName.sol文件。3. 把合约名字加到config/app.json的’contracts’部分。4. 启动以太坊节点（例如在另一个终端里面运行testrpc）。5. truffle deploy（在truffle项目目录中) 添加一个智能合约。 在truffle init执行后或是一个现有的项目目录中，复制粘帖上面的会议合约到contracts/Conference.sol文件中。然后打开config/app.json文件，把’Conference’加入’deploy’数组中。 启动testrpc。 在另一个终端中启动testrpc。 编译或部署。 执行truffle compile看一下合约是否能成功编译，或者直接truffle deploy一步完成编译和部署。这条命令会把部署好的合约的地址和ABI（应用接口）加入到配置文件中，这样之后的truffle test和truffle build步骤可以使用这些信息。 出错了？ 编译是否成功了？记住，错误信息即可能出现在testrpc终端也可能出现在truffle终端。 重启节点后记得重新部署！ 如果你停止了testrpc节点，下一次使用任何合约之前切记使用truffle deploy重新部署。testrpc在每一次重启之后都会回到完全空白的状态。 合约代码解读让我们从智能合约头部的变量声明开始： address public organizer; mapping (address =&gt; uint) public registrantsPaid; uint public numRegistrants; uint public quota; address。 地址类型。第一个变量是会议组织者的钱包地址。这个地址会在合约的构造函数function Conference()中被赋值。很多时候也称呼这种地址为’owner’（所有人）。 uint。 无符号整型。区块链上的存储空间很紧张，保持数据尽可能的小。 public。 这个关键字表明变量可以被合约之外的对象使用。private修饰符则表示变量只能被本合约(或者衍生合约)内的对象使用。如果你想要在测试中通过web3.js使用合约中的某个变量，记得把它声明为public。 Mapping或数组。（译注：Mapping类似Hash, Directory等数据类型，不做翻译。）在Solidity加入数组类型之前，大家都使用类似mapping (address =&gt; uint)的Mapping类型。这个声明也可以写作address registrantsPaid[]，不过Mapping的存储占用更小(smaller footprint)。这个Mapping变量会用来保存参加者（用他们的钱包地址表示）的付款数量以便在退款时使用。 关于地址。 你的客户端（比如testrpc或者geth）可以生成一个或多个账户/地址。testrpc启动时会显示10个可用地址： 第一个地址, accounts[0]，是发起调用的默认地址，如果没有特别指定的话。 组织者地址 vs 合约地址。 部署好的合约会在区块链上拥有自己的地址（与组织者拥有的是不同的地址）。在Solidity合约中可以使用this来访问这个合约地址，正如refundTicket函数所展示的：address myAddress = this; Suicide, Solidity的好东西。（译注：suicide意为’自杀’，Solidity提供的关键字，不做翻译。）转给合约的资金会保存于合约（地址）中。最终这些资金通过destroy函数被释放给了构造函数中设置的组织者地址。这是通过suicide(orgnizer);这行代码实现的。没有这个，资金可能被永远锁定在合约之中（reddit上有些人就遇到过），因此如果你的合约会接受资金一定要记得在合约中使用这个方法！ 如果想要模拟另一个用户或者对手方（例如你是卖家想要模拟一个买家），你可以使用可用地址数组中另外的地址。假设你要以另一个用户，accounts[1], 的身份来买票，可以通过from参数设置： conference.buyTicket({ from: accounts[1], value: some_ticket_price_integer }); 函数调用可以是交易。 改变合约状态（修改变量值，添加记录，等等）的函数调用本身也是转账交易，隐式的包含了发送人和交易价值。因此web3.js的函数调用可以通过指定{ from: __, value: __ }参数来发送以太币。在Solidity合约中，你可以通过msg.sender和msg.value来获取这些信息： function buyTicket() public { ... registrantsPaid[msg.sender] = msg.value; ... } 事件(Event)。 可选的功能。合约中的Deposit（充值）和Send（发送）事件是会被记录在以太坊虚拟机日志中的数据。它们实际上没有任何作用，但是用事件(Event)把交易记录进日志是好的做法。 好了，现在让我们给这个智能合约写一个测试，来确保它能工作。 写测试把项目目录test/中的example.js文件重命名为conference.js，文件中所有的’Example’替换为’Conference’。 contract(&apos;Conference&apos;, function(accounts) { it(&quot;should assert true&quot;, function(done) { var conference = Conference.at(Conference.deployed_address); assert.isTrue(true); done(); // stops tests at this point }); }); 在项目根目录下运行truffle test，你应该看到测试通过。在上面的测试中truffle通过Conference.deployed_address获得合约部署在区块链上的地址。 让我们写一个测试来初始化一个新的Conference，然后检查变量都正确赋值了。将conference.js中的测试代码替换为： contract(&apos;Conference&apos;, function(accounts) { it(&quot;Initial conference settings should match&quot;, function(done) { var conference = Conference.at(Conference.deployed_address); // same as previous example up to here Conference.new({ from: accounts[0] }) .then(function(conference) { conference.quota.call().then( function(quota) { assert.equal(quota, 500, &quot;Quota doesn&apos;t match!&quot;); }).then( function() { return conference.numRegistrants.call(); }).then( function(num) { assert.equal(num, 0, &quot;Registrants should be zero!&quot;); return conference.organizer.call(); }).then( function(organizer) { assert.equal(organizer, accounts[0], &quot;Owner doesn&apos;t match!&quot;); done(); // to stop these tests earlier, move this up }).catch(done); }).catch(done); }); }); 构造函数。 Conference.new({ from: accounts[0] })通过调用合约构造函数创造了一个新的Conference实例。由于不指定from时会默认使用accounts[0]，它其实可以被省略掉： Conference.new({ from: accounts[0] }); // 和Conference.new()效果相同 Promise。 代码中的那些then和return就是Promise。它们的作用写成一个深深的嵌套调用链的话会是这样： conference.numRegistrants.call().then( function(num) { assert.equal(num, 0, &quot;Registrants should be zero!&quot;); conference.organizer.call().then( function(organizer) { assert.equal(organizer, accounts[0], &quot;Owner doesn&apos;t match!&quot;); }).then( function(...)) }).then( function(...)) // Because this would get hairy... Promise减少嵌套，使代码变得扁平，允许调用异步返回，并且简化了表达“成功时做这个”和“失败时做那个”的语法。Web3.js通过回调函数实现异步调用，因此你不需要等到交易完成就可以继续执行前端代码。Truffle借助了用Promise封装web3.js的一个框架，叫做Pudding，这个框架本身又是基于Bluebird的，它支持Promise的高级特性。 call。 我们使用call来检查变量的值，例如conference.quota.call().then(...，还可以通过传参数，例如call(0), 来获取mapping在index 0处的元素。Solidity的文档说这是一种特殊的“消息调用”因为 1.不会为矿工记录和 2.不需要从钱包账户/地址发起（因此它没有被账户持有者私钥做签名）。另一方面，交易/事务(Transaction)会被矿工记录，必须来自于一个账户（也就是有签名），会被记录到区块链上。对合约中数据做的任何修改都是交易。仅仅是检查一个变量的值则不是。因此在读取变量时不要忘记加上call()！否则会发生奇怪的事情。（此外如果在读取变量是遇到问题别忘记检查它是否是public。）call()也能用于调用不是交易的函数。如果一个函数本来是交易，但你却用call()来调用，则不会在区块链上产生交易。 断言。 标准JS测试中的断言（如果你不小心拼成了复数形式’asserts’，truffle会报错，让你一头雾水），assert.equal是最常用的，其他类型的断言可以在Chai的文档中找到。 再一次运行truffle test确保一切工作正常。 测试合约函数调用现在我们测试一下改变quote变量的函数能工作。在tests/conference.js文件的contract(&#39;Conference&#39;, function(accounts) {...};)的函数体中添加如下测试用例： it(&quot;Should update quota&quot;, function(done) { var c = Conference.at(Conference.deployed_address); Conference.new({from: accounts[0] }).then( function(conference) { conference.quota.call().then( function(quota) { assert.equal(quota, 500, &quot;Quota doesn&apos;t match!&quot;); }).then( function() { return conference.changeQuota(300); }).then( function(result) { // result here is a transaction hash console.log(result); // if you were to print this out it’d be long hex - the transaction hash return conference.quota.call() }).then( function(quota) { assert.equal(quota, 300, &quot;New quota is not correct!&quot;); done(); }).catch(done); }).catch(done); }); 这里的新东西是调用changeQuota函数的那一行。console.log对于调试很有用，用它能在运行truffle的终端中输出信息。在关键点插入console.log可以查看执行到了哪一步。记得把Solidity合约中changeQuota函数被声明为public，否则你不能调用它： function changeQuota(uint newquota) public { } 测试交易现在让我们调用一个需要发起人发送资金的函数。 Wei。 以太币有很多种单位（这里有个很有用的转换器）,在合约中通常用的是Wei，最小的单位。Web3.js提供了在各单位与Wei之间互相转换的便利方法，形如web3.toWei(.05, &#39;ether&#39;)。JavaScript在处理很大的数字时有问题，因此web3.js使用了程序库BigNumber，并建议在代码各处都以Wei做单位，直到要给用户看的时候（文档。 账户余额。 Web3.js提供了许多提供方便的方法，其中另一个会在下面测试用到的是web3.eth.getBalance(some_address)。记住发送给合约的资金会由合约自己持有直到调用suicide。 在contract(Conference, function(accounts) {...};)的函数体中插入下面的测试用例。在高亮显示的方法中，测试用例让另一个用户(accounts[1])以ticketPrice的价格买了一张门票。然后它检查合约的账户余额增加了ticketPrice，以及购票用户被加入了参会者列表。 这个测试中的buyTicket是一个交易函数： it(&quot;Should let you buy a ticket&quot;, function(done) { var c = Conference.at(Conference.deployed_address); Conference.new({ from: accounts[0] }).then( function(conference) { var ticketPrice = web3.toWei(.05, &apos;ether&apos;); var initialBalance = web3.eth.getBalance(conference.address).toNumber(); conference.buyTicket({ from: accounts[1], value: ticketPrice }).then( function() { var newBalance = web3.eth.getBalance(conference.address).toNumber(); var difference = newBalance - initialBalance; assert.equal(difference, ticketPrice, &quot;Difference should be what was sent&quot;); return conference.numRegistrants.call(); }).then(function(num) { assert.equal(num, 1, &quot;there should be 1 registrant&quot;); return conference.registrantsPaid.call(accounts[1]); }).then(function(amount) { assert.equal(amount.toNumber(), ticketPrice, &quot;Sender&apos;s paid but is not listed&quot;); done(); }).catch(done); }).catch(done); }); 交易需要签名。 和之前的函数调用不同，这个调用是一个会发送资金的交易，在这种情况下购票用户(accounts[1])会用他的私钥对buyTicket()调用做签名。（在geth中用户需要在发送资金之前通过输入密码来批准这个交易或是解锁钱包的账户。） toNumber()。 有时我们需要把Solidity返回的十六进制结果转码。如果结果可能是个很大的数字可以用web3.toBigNumber(numberOrHexString)来处理因为JavaScript直接对付大数要糟。 测试包含转账的合约最后，为了完整性，我们确认一下refundTicket方法能正常工作，而且只有会议组织者能调用。下面是测试用例： it(&quot;Should issue a refund by owner only&quot;, function(done) { var c = Conference.at(Conference.deployed_address); Conference.new({ from: accounts[0] }).then( function(conference) { var ticketPrice = web3.toWei(.05, &apos;ether&apos;); var initialBalance = web3.eth.getBalance(conference.address).toNumber(); conference.buyTicket({ from: accounts[1], value: ticketPrice }).then( function() { var newBalance = web3.eth.getBalance(conference.address).toNumber(); var difference = newBalance - initialBalance; assert.equal(difference, ticketPrice, &quot;Difference should be what was sent&quot;); // same as before up to here // Now try to issue refund as second user - should fail return conference.refundTicket(accounts[1], ticketPrice, {from: accounts[1]}); }).then( function() { var balance = web3.eth.getBalance(conference.address).toNumber(); assert.equal(web3.toBigNumber(balance), ticketPrice, &quot;Balance should be unchanged&quot;); // Now try to issue refund as organizer/owner - should work return conference.refundTicket(accounts[1], ticketPrice, {from: accounts[0]}); }).then( function() { var postRefundBalance = web3.eth.getBalance(conference.address).toNumber(); assert.equal(postRefundBalance, initialBalance, &quot;Balance should be initial balance&quot;); done(); }).catch(done); }).catch(done); }); 这个测试用例覆盖的Solidity函数如下： function refundTicket(address recipient, uint amount) public returns (bool success) { if (msg.sender != organizer) { return false; } if (registrantsPaid[recipient] == amount) { address myAddress = this; if (myAddress.balance &gt;= amount) { recipient.send(amount); Refund(recipient, amount); registrantsPaid[recipient] = 0; numRegistrants--; return true; } } return false; } 合约中发送以太币。 address myAddress = this展示了如何获取该会议合约实例的地址，以变接下来检查这个地址的余额（或者直接使用this.balance）。合约通过recipient.send(amount)方法把资金发回了购票人。 交易无法返回结果给web3.js。 注意这一点！refundTicket函数会返回一个布尔值，但是这在测试中无法检查。因为这个方法是一个交易函数（会改变合约内数据或是发送以太币的调用），而web3.js得到的交易运行结果是一个交易哈希（如果打印出来是一个长长的十六进制/怪怪的字符串）。既然如此为什么还要让refundTicket返回一个值？因为在Solidity合约内可以读到这个返回值，例如当另一个合约调用refundTicket()的时候。也就是说Solidity合约可以读取交易运行的返回值，而web3.js不行。另一方面，在web3.js中你可以用事件机制（Event, 下文会解释）来监控交易运行，而合约不行。合约也无法通过call()来检查交易是否修改了合约内变量的值。 关于sendTransaction()。 当你通过web3.js调用类似buyTicket()或者refundTicket()的交易函数时（使用web3.eth.sendTransaction），交易并不会立即执行。事实上交易会被提交到矿工网络中，交易代码直到其中一位矿工产生一个新区块把交易记录进区块链之后才执行。因此你必须等交易进入区块链并且同步回本地节点之后才能验证交易执行的结果。用testrpc的时候可能看上去是实时的，因为测试环境很快，但是正式网络会比较慢。 事件/Event。 在web3.js中你应该监听事件而不是返回值。我们的智能合约示例定义了这些事件： event Deposit(address _from, uint _amount); event Refund(address _to, uint _amount); 它们在buyTicket()和refundTicket()中被触发。触发时你可以在testrpc的输出中看到日志。要监听事件，你可以使用web.js监听器(listener)。在写本文时我还不能在truffle测试中记录事件，但是在应用中没问题： Conference.new({ from: accounts[0] }).then( function(conference) { var event = conference.allEvents().watch({}, &apos;&apos;); // or use conference.Deposit() or .Refund() event.watch(function (error, result) { if (error) { console.log(&quot;Error: &quot; + error); } else { console.log(&quot;Event: &quot; + result.event); } }); // ... 过滤器/Filter。 监听所有事件可能会产生大量的轮询，作为替代可以使用过滤器。它们可以更灵活的开始或是停止对事件的监听。更多过滤器的信息可查看Solidity文档。 总的来说，使用事件和过滤器的组合比检查变量消耗的Gas更少，因而在验证正式网络的交易运行结果时非常有用。 Gas。 （译注：以太坊上的燃料，因为代码的执行必须消耗Gas。直译为汽油比较突兀，故保留原文做专有名词。）直到现在我们都没有涉及Gas的概念，因为在使用testrpc时通常不需要显式的设置。当你转向geth和正式网络时会需要。在交易函数调用中可以在{from: __, value: __, gas: __}对象内设置Gas参数。Web3.js提供了web3.eth.gasPrice调用来获取当前Gas的价格，Solidity编译器也提供了一个参数让你可以从命令行获取合约的Gas开销概要：solc --gas YouContract.sol。下面是Conference.sol的结果： 为合约创建DApp界面下面的段落会假设你没有网页开发经验。 上面编写的测试用例用到的都是在前端界面中也可以用的方法。你可以把前端代码放到app/目录中，运行truffle build之后它们会和合约配置信息一起编译输出到build/目录。在开发时可以使用truffle watch命令在app/有任何变动时自动编译输出到build/目录。然后在浏览器中刷新页面即可看到build/目录中的最新内容。（truffle serve可以启动一个基于build/目录的网页服务器。） app/目录中有一些样板文件帮助你开始： index.html会加载app.js： 因此我们只需要添加代码到app.js就可以了。 默认的app.js会在浏览器的console(控制台)中输出一条”Hello from Truffle!”的日志。在项目根目录中运行truffle watch，然后在浏览器中打开build/index.html文件，再打开浏览器的console就可以看到。（大部分浏览器例如Chrome中，单击右键 -&gt; 选择Inspect Element然后切换到Console即可。） 在app.js中，添加一个在页面加载时会运行的window.onload调用。下面的代码会确认web3.js已经正常载入并显示所有可用的账户。（注意：你的testrpc节点应该保持运行。） window.onload = function() { var accounts = web3.eth.accounts; console.log(accounts); } 看看你的浏览器console中看看是否打印出了一组账户地址。 现在你可以从tests/conference.js中复制一些代码过来（去掉只和测试有关的断言），将调用返回的结果输出到console中以确认代码能工作。下面是个例子： window.onload = function() { var accounts = web3.eth.accounts; var c = Conference.at(Conference.deployed_address); Conference.new({ from: accounts[0] }).then( function(conference) { var ticketPrice = web3.toWei(.05, &apos;ether&apos;); var initialBalance = web3.eth.getBalance(conference.address).toNumber(); console.log(&quot;The conference&apos;s initial balance is: &quot; + initialBalance); conference.buyTicket({ from: accounts[1], value: ticketPrice }).then( function() { var newBalance = web3.eth.getBalance(conference.address).toNumber(); console.log(&quot;After someone bought a ticket it&apos;s: &quot; + newBalance); return conference.refundTicket(accounts[1], ticketPrice, {from: accounts[0]}); }).then( function() { var balance = web3.eth.getBalance(conference.address).toNumber(); console.log(&quot;After a refund it&apos;s: &quot; + balance); }); }); }; 上面的代码应该输出如下： (console输出的warning信息可忽略。) 现在起你就可以使用你喜欢的任何前端工具，jQuery, ReactJS, Meteor, Ember, AngularJS，等等等等，在app/目录中构建可以与以太坊智能合约互动的DApp界面了！接下来我们给出一个极其简单基于jQuery的界面作为示例。 这里是index.html的代码，这里是app.js的代码。 通过界面测试了智能合约之后我意识到最好加入检查以保证相同的用户不能注册两次。另外由于现在是运行在testrpc节点上，速度很快，最好是切换到geth节点并确认交易过程依然能及时响应。否则的话界面上就应该显示提示信息并且在处理交易时禁用相关的按钮。 尝试geth。 如果你使用geth, 可以尝试以下面的命令启动 - 在我这儿(geth v1.2.3)工作的很好： build/bin/geth --rpc --rpcaddr=&quot;0.0.0.0&quot; --rpccorsdomain=&quot;*&quot; --mine --unlock=&apos;0 1&apos; --verbosity=5 --maxpeers=0 --minerthreads=&apos;4&apos; --networkid &apos;12345&apos; --genesis test-genesis.json 这条命令解锁了两个账户, 0和1。1. 在geth控制台启动后你可能需要输入这两个账户的密码。2. 你需要在test-genesis.json文件里面的’alloc’配置中加入你的这两个账户，并且给它们充足的资金。3. 最后，在创建合约实例时加上gas参数： Conference.new({from: accounts[0], gas: 3141592}) 然后把整个truffle deploy, truffle build流程重来一遍。 教程中的代码。 在这篇基础教程中用到的所有代码都可以在这个代码仓库中找到。 自动为合约生成界面。 SilentCicero制作了一个叫做DApp Builder的工具，可以用Solidity合约自动生成HTML, jQuery和web.js的代码。这种模式也正在被越来越多的正在开发中的开发者工具采用。 教程到此结束！ 最后一章我们仅仅学习了一套工具集，主要是Truffle和testrpc. 要知道即使在ConsenSys内部，不同的开发者使用的工具和框架也不尽相同。你可能会发现更适合你的工具，这里所说的工具可能很快也会有改进。但是本文介绍的工作流程帮助我走上了DApp开发之路。 (⊙ω⊙) wonk wonk 感谢Joseph Chow的校阅和建议，Christian Lundkvist, Daniel Novy, Jim Berry, Peter Borah和Tim Coulter帮我修改文字和debug，以及Tim Coulter, Nchinda Nchinda和Mike Goldin对DApp前端步骤图提供的帮助。]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>以太坊</tag>
        <tag>智能合约</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：Repository not found.]]></title>
    <url>%2Fgit%2Fgit-change-server-password.html</url>
    <content type="text"><![CDATA[原帖收藏于IT老兵驿站，传递一个IT老兵在凋零前的光和氧。Git：服务端更换密码后，报告“remote: Repository not found.”。 昨天Github提醒我需要修改密码，所以我将Github的密码修改了，结果，回到家，在mac笔记本上git pull代码时报告remote: Repository not found.，在网上找了半天，都是在说git路径存在问题，这肯定不是我的问题，我的问题应该就是更换了密码，只不过git 报告了一个奇怪的错误，应该是需要修改本地原本保存的密码。 找到一个帖子说： 分别打开下面2个文件,将[User]部分完全删除后保存,再使用Git命令的时候就会提示输入帐号密码了.$ vi .git/config (工程当前路径的Git配置文件) $ vi ~/.gitconfig (全局的Git配置文件) 这里是删除掉了用户信息，不过，不管用。 继续寻找原因：12xxx$ git config -lcredential.helper=osxkeychain 这里有个变量credential.helper，凭证帮助者，怀疑是这里保存了密码，后面指向了电脑的keychain。 查看官网解释： credential.helperSpecify an external helper to be called when a username or password credential is needed; the helper may consult external storage to avoid prompting the user for the credentials. Note that multiple helpers may be defined. See gitcredentials[7] for details. 这里确认了，这里是外部的一个凭证助手，用来保存用户名和密码。 然后找到mac的keychain（钥匙串访问），果然发现了一条Github的条目，这里应该保存在原本的用户名和密码，把它删掉（这里也是可以直接修改密码的，只是第一次尝试的是删除），这个时候再git pull就出现了提示请输入用户名了，至此，问题解决，相关的疑问也搞明白了。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Repository not found</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链：以太坊的工作原理（转）--阅读笔记]]></title>
    <url>%2Fblockchain%2Fethereum-rationale-introduction.html</url>
    <content type="text"><![CDATA[区块链：以太坊的工作原理–阅读笔记。 前言找到一篇写的很好的对以太坊的介绍，6月21日阅读了一遍，并且进行了转帖，今天感觉还有很多内容没有读明白，开始第二遍研究，记录一些笔记。当然，会对原文中的一些有点问题的格式进行一下调整。 正文这篇文章主要讲解以太坊的基本原理，对技术感兴趣的朋友可以看看。 翻译作者: 许莉原文地址：How does Ethereum work, anyway? 简介不管你们知不知道以太坊（Ethereum blockchain）是什么，但是你们大概都听说过以太坊。最近在新闻里出现过很多次，包括一些专业杂志的封面，但是如果你们对以太坊到底是什么没有一个基本的了解的话，看这些文章就会感觉跟看天书一样。 所以，什么是以太坊？本质上，就是一个保存数字交易永久记录的公共数据库。重要的是，这个数据库不需要任何中央权威机构来维持和保护它。相反的它以一个“无信任”的交易系统来运行—一个个体在不需要信任任何第三方或对方的情况下进行点对点交易的架构。（PS：加了标红，感觉这句话需要着重理解） 依然感到很困惑？这就是这篇文章存在的理由。我的目标是在技术层面来解释以太坊的工作原理，但是不会出现很复杂的数学问题或看起来很可怕的公式。即使你不是一个程序员，我希望你看完之后最起码对技术有个更好的认识。如果有些部分技术性太强不好理解，这是非常正常的，真的没有必要完全理解每一个小细节。我建议只要宏观的理解一下事物就行了。 这篇文章中的很多议点都是以太坊黄皮书中讨论过的概念的细分。我添加了我自己的解释和图表使理解以太坊更加简单一点。那些足够勇敢的人可以挑战一下技术，去阅读一下以太坊的黄皮书。 好了，让我们开始吧！ 区块链定义区块链就是一个具有共享状态的密码性安全交易的单机(cryptographically secure transactional singleton machine with shared-state)。[1]这有点长，是吧？让我们将它分开来看： “密码性安全(Cryptographically secure)”是指用一个很难被解开的复杂数学机制算法来保证数字货币生产的安全性。将它想象成类似于防火墙的这种。它们使得欺骗系统近乎是一个不可能的事情（比如：构造一笔假的交易，消除一笔交易等等）。 “交易的单机(Transactional singleton machine)”是指只有一个权威的机器实例为系统中产生的交易负责任。换句话说，只有一个全球真相是大家所相信的。 “具有共享状态(With shared-state)”是指在这台机器上存储的状态是共享的，对每个人都是开放的。 以太坊实现了区块链的这个范例。 PS：这段概念说的有些绕，理解起来有点费劲。对这里面的singleton machine with shared-state不是太理解。同时发现了以太坊总是在强调一个概念，就是state。 以太坊模型说明以太坊的本质就是一个基于交易的状态机(transaction-based state machine)。（PS：比特币不也是这样吗？）在计算机科学中，一个 状态机是指可以读取一系列的输入，然后根据这些输入，会转换成一个新的状态出来的东西。 根据以太坊的状态机，我们从创世纪状态(genesis state)开始。这差不多类似于一片空白的石板，在网络中还没有任何交易的产生状态。当交易被执行后，这个创世纪状态就会转变成最终状态。在任何时刻，这个最终状态都代表着以太坊当前的状态。 以太坊的状态有百万个交易。这些交易都被“组团”到一个区块中。一个区块包含了一系列的交易，每个区块都与它的前一个区块链接起来。 为了让一个状态转换成下一个状态，交易必须是有效的。为了让一个交易被认为是有效的，它必须要经过一个验证过程，此过程也就是挖矿。挖矿就是一组节点（即电脑）用它们的计算资源来创建一个包含有效交易的区块出来。 任何在网络上宣称自己是矿工的节点都可以尝试创建和验证区块。世界各地的很多矿工都在同一时间创建和验证区块。每个矿工在提交一个区块到区块链上的时候都会提供一个数学机制的“证明”，这个证明就像一个保证：如果这个证明存在，那么这个区块一定是有效的。 为了让一个区块添加到主链上，一个矿工必须要比其他矿工更快的提供出这个“证明”。通过矿工提供的一个数学机制的“证明”来证实每个区块的过程称之为工作量证明(proof of work)。 证实了一个新区块的矿工都会被奖励一定价值的奖赏。奖赏是什么？以太坊使用一种内在数字代币—以太币(Ether)作为奖赏。每次矿工证明了一个新区块，那么就会产生一个新的以太币并被奖励给矿工。 你也许会在想：什么能确保每个人都只在区块的同一条链上呢？我们怎么能确定不会存在一部分矿工创建一个他们自己的链呢？ 前面，我们定义了区块链就是一个具有共享状态的交易单机。使用这个定义，我们可以知道正确的当前状态是一个全球真相，所有人都必须要接受它。拥有多个状态（或多个链）会摧毁这个系统，因为它在哪个是正确状态的问题上不可能得到统一结果。如果链分叉了，你有可能在一条链上拥有10个币，一条链上拥有20个币，另一条链上拥有40个币。在这种场景下，是没有办法确定哪个链才是最”有效的“。 不论什么时候只要多个路径产生了，一个”分叉“就会出现。我们通常都想避免分叉，因为它们会破坏系统，强制人们去选择哪条链是他们相信的链。 为了确定哪个路径才是最有效的以及防止多条链的产生，以太坊使用了一个叫做“GHOST协议(GHOST protocol)”的数学机制。 GHOST = Greedy Heaviest Observed Subtree 简单来说，GHOST协议就是让我们必须选择一个在其上完成计算最多的路径。一个方法确定路径就是使用最近一个区块（叶子区块）的区块号，区块号代表着当前路径上总的区块数（不包含创世纪区块）。区块号越大，路径就会越长，就说明越多的挖矿算力被消耗在此路径上以达到叶子区块。使用这种推理就可以允许我们赞同当前状态的权威版本。 PS：以上是总体的一个概念。 现在你大概对区块链是什么有个理性的认识，让我们在再深入地了解一下以太坊系统主要组成部分： 账户(accounts) 状态(state) 损耗和费用(gas and fees) 交易(transactions) 区块(blocks) 交易执行(transaction execution) 挖矿(mining) 工作量证明(proof of work) 在开始之前需要注意的是：每当我说某某的Hash， 我指的都是KECCAK-256 hash, 以太坊就是使用这个Hash算法。 账户以太坊的全局“共享状态”是有很多小对象（账户）来组成的，这些账户可以通过消息传递来与对方进行交互。每个账户都有一个与之关联的状态(state)和一个20字节的地址(address)。在以太坊中一个地址是160位的标识符，用来识别账户。（PS：这个状态又是怎么呈现的呢？外部拥有账户是有一个余额的概念，那么合约账户呢？怎么呈现当前的状态呢？需要检索以前所有和这个合约发生过关系的交易？） 两种不同类型的账户： 外部拥有的账户，被私钥控制且没有任何代码与之关联 合约账户，被它们的合约代码控制且有代码与之关联 外部拥有账户与合约账户的比较理解外部拥有账户和合约账户的基本区别是很重要的。一个外部拥有账户可以通过创建和用自己的私钥来对交易进行签名，来发送消息给另一个外部拥有账户或合约账户。在两个外部拥有账户之间传送的消息只是一个简单的价值转移。但是从外部拥有账户到合约账户的消息会激活合约账户的代码，允许它执行各种动作。（比如转移代币，写入内部存储，挖出一个新代币，执行一些运算，创建一个新的合约等等）。 不像外部拥有账户，合约账户不可以自己发起一个交易。相反，合约账户只有在接收到一个交易之后(从一个外部拥有账户或另一个合约账户处)，为了响应此交易而触发一个交易。我们将会在“交易和消息”章节来了解关于合约与合约之间的通信。 因此，在以太坊上任何的动作，总是被外部拥有账户触发的交易所发动的。 账户状态账户状态有四个组成部分，不论账户类型是什么，都存在这四个组成部分： nonce：如果账户是一个外部拥有账户，nonce代表从此账户地址发送的交易序号。如果账户是一个合约账户，nonce代表此账户创建的合约序号 balance： 此地址拥有Wei的数量。1Ether=10^18Wei storageRoot： Merkle Patricia树的根节点Hash值（我们后面在解释Merkle树）。Merkle树会将此账户存储内容的Hash值进行编码，默认是空值 codeHash：此账户EVM（以太坊虚拟机，后面细说）代码的hash值。对于合约账户，就是被Hash的代码并作为codeHash保存。对于外部拥有账户，codeHash域是一个空字符串的Hash值 （PS：这个地方存在几个问题： storageRoot和codeHash存储的都是hash值，那么原本的内容被存在哪里？ 合约代码会发生改变吗？代码里面的状态值是怎样体现改变的？ 账户的唯一性是由nonce来确定的吗？） （PS：账户的信息又是怎么保存在区块链上呢？） 世界状态好了，我们知道了以太坊的全局状态就是由账户地址和账户状态组成的一个映射。这个映射被保存在一个叫做Merkle Patricia树的数据结构中 Merkle Tree（也被叫做Merkle trie）是一种由一系列节点组成的二叉树，这些节点包括： 在树底的大量叶子节点，这些叶子节点包含了源数据 一系列的中间节点，这些节点是两个子节点的Hash值 一个根节点，同样是两个子节点的Hash值，代表着整棵树 树底的数据是通过分开我们想要保存到chunks的数据产生的，然后将chunks分成buckets，再然后获取每个bucket的hash值并一直重复直到最后只剩下一个Hash：根Hash。 PS：这段话翻译得不好，原文是： The data at the bottom of the tree is generated by splitting the data that we want to store into chunks, then splitting the chunks into buckets, and then taking the hash of each bucket and repeating the same process until the total number of hashes remaining becomes only one: the root hash. 不是分开，是把我们想要保存的数据分隔开（splitting），保存到chunks上，然后再分隔chunks到bucket上。这是两个组织数据的单位。不过怎么体现在区块上呢？ 这棵树要求存在里面的值（value）都有一个对应的key。从树的根节点开始，key会告诉你顺着哪个子节点可以获得对应的值，这个值存在叶子节点。在以太坊中，key/value是地址和与地址相关联的账户之间状态的映射，包括每个账户的balance, nonce, codeHash和storageRoot（storageRoot自己就是一颗树）。 （PS：到这里明白了，账户信息是保存在state树中的） 同样的树结构也用来存储交易和收据。更具体的说，每个块都有一个头(header)，头中保存了三个Merkle树结构的根节点Hash，三个Merkle树分别为： 状态树 交易树 收据树 Merkle树中存储信息的高效性在以太坊的“轻客户端”和“轻节点”中相当的有用。记住区块链就是一群节点来维持的。广泛的说，有两种节点类型：全节点和轻节点。 全节点通过下载整条链来进行同步，从创世纪块到当前块，执行其中包含的所有交易。通常，矿工会存储全节点，因为他们在挖矿过程中需要全节点。也有可能下载一个全节点而不用执行所有的交易。无论如何，一个全节点包含了整个链。（PS：为什么要执行交易呢？交易的结果状态没有被记录吗？） 不过除非一个节点需要执行所有的交易或轻松访问历史数据，不然没必要保存整条链。这就是轻节点概念的来源。比起下载和存储整个链以及执行其中所有的交易，轻节点仅仅下载链的头，从创世纪块到当前块的头，不执行任何的交易或检索任何相关联的状态。由于轻节点可以访问区块头，而头中包含了3个Merkle树的根Hash值，所有轻节点依然可以很容易生成和接收关于交易、事件、余额等可验证的答案。 这个可以行的通是因为在Merkle树中Hash值是向上传播的—如果一个恶意用户试图用一个假交易来交换Merkle树底的交易，这个会改变它上面节点的Hash值，而它上面节点的值的改变也会导致上上一个节点Hash值的改变，以此类推，一直到树的根节点。 任何节点想要验证一些数据都可以通过Merkle证明来进行验证，Merkle 证明的组成： 一块需要验证的数据 树的根节点Hash值 一个“分支”（从 chunk到根这个路径上所有的Hash值） 任何可以读取证明的人都可以验证分支的Hash值是连贯的，因此给出的块在树中实际的位置就是在此处。 总之，使用Merkle Patricia树的好处就是该结构的根节点加密取决于存储在树中的数据，而且根节点的Hash值还可以作为该数据的安全标识。由于块的头包含了状态树、交易树、收据树的根Hash值，所以任何节点都可以验证以太坊的一小部分状态而不用保存整个状态，这整个状态的的大小可能是非常大的。 Gas和费用在以太坊中一个比较重要的概念就是费用(fees)，由以太坊网络上的交易而产生的每一次计算，都会产生费用—没有免费的午餐。这个费用是以”gas”来支付。 Gas就是用来衡量在一个具体计算中要求的费用单位。gas price就是你愿意在每个gas上花费Ether的数量，以“gwei”进行衡量。“Wei”是Ether的最小单位，1Ether=10^18Wei，1gwei=1,000,000,000 Wei。 对每个交易，发送者设置gas limit和gas price。gas limit和gas price就代表着发送者愿意为执行交易支付的Wei的最大值。 例如，假设发送者设置gas limit为50,000，gas price为20gwei。这就表示发送者愿意最多支付50,000 * 20gwei = 1,000,000,000,000,000 Wei = 0.001 Ether来执行此交易。 记住gas limit代表用户愿意花费在gas上费用的最大值。如果在他们的账户余额中有足够的Ether来支付这个最大值费用，那么就没问题。在交易结束时任何未使用的gas都会被返回给发送者，以原始费率兑换。 在发送者没有提供足够的gas来执行交易，那么交易执行就会出现“gas不足”然后被认为是无效的。在这种情况下，交易处理就会被终止以及所有已改变的状态将会被恢复，最后我们就又回到了交易之前的状态—完完全全的之前状态就像这笔交易从来没有发生。因为机器在耗尽gas之前还是为计算做出了努力，所以理论上，将不会有任何的gas被返回给发送者。 这些gas的钱到底去了哪里？发送者在gas上花费的所有费用都被发送到“受益人”的地址，通常情况下就是矿工的地址。因为矿工为了计算和验证交易做出了努力，所以矿工接收gas的费用作为奖励。 通常，发送者愿意支付更高的gas price，矿工从这笔交易中就能获得更多的价值。因此，矿工也就更加愿意选择这笔交易。这样的话，矿工可以自由的选择自己愿意验证或忽略的交易。为了引导发送者设置合理的gas price，矿工可以选择建议一个最小的gas值，此值代表自己愿意执行交易的最低价格。 存储也有费用Gas不仅仅是用来支付计算这一步的费用，而且也用来支付存储的费用。存储的总费用与所使用的32位字节的最小倍数成比例。 存储费用有一些比较细微的方面。比如，由于增加的存储增加了所有节点上的以太坊状态数据库的大小，所以激励保持数据存储量小。为了这个原因，如果一个交易的执行有一步是清除一个存储实体，那么为执行这个操作的费用就会被放弃，并且由于释放存储空间的退款就会被返回给发送者。 费用的作用是什么？以太坊可以运作的一个重要方面就是每个网络执行的操作同时也被全节点所影响。然而，计算的操作在以太坊虚拟机上是非常昂贵的。因此，以太坊智能合约最好是用来执行最简单的任务，比如运行一个简单的业务逻辑或者验证签名和其他密码对象，而不是用于复杂的操作，比如文件存储，电子邮件，或机器学习，这些会给网络造成压力。施加费用防止用户使网络超负荷。 以太坊是一个图灵完备语言（短而言之，图灵机器就是一个可以模拟任何电脑算法的机器。对于图灵机器不太熟悉的人可以看看这个和这个）。这就允许有循环，并使以太坊受到停机问题的影响，这个问题让你无法确定程序是否无限制的运行。如果没有费用的话，恶意的执行者通过执行一个包含无限循环的交易就可以很容易的让网络瘫痪而不会产生任何反响。因此，费用保护网络不受蓄意攻击。 你也许会想，“为什么我们还需要为存储付费？”其实就像计算一样，以太坊网络上的存储是整个网络都必须要负担的成本。 交易和消息之前说过以太坊是一个基于交易的状态机。换句话说，在两个不同账户之间发生的交易才让以太坊的全局状态从一个状态转换成另一个状态。 最基本的概念，一个交易就是指被外部拥有账户生成的加密签名的一段指令，序列化之后提交给区块链。 有两种类型的交易：消息通信(message calls)和合约创建(contract creations)(也就是交易产生一个新的以太坊合约)。 不管什么类型的交易，都包含： nonce：发送者发送交易数的计数 gasPrice：发送者愿意支付执行交易所需的每个gas的Wei数量 gasLimit：发送者愿意为执行交易支付gas数量的最大值。此值设置之后在任何计算完成之前就会被提前扣掉 to：接收者的地址。在合约创建交易中，合约账户的地址还没有存在，所以值先空着 value：从发送者转移到接收者Wei的数量。在合约创建交易中，value作为新建合约账户的开始余额 v,r,s：用于产生标识交易发送者的签名 init（只有在合约创建交易中存在）：用来初始化新合约账户的EVM代码片段。init值会执行一次，然后就会被丢弃。当init第一次执行的时候，它返回一个账户代码体，也就是永久与合约账户关联的一段代码。 data（可选域，只有在消息通信中存在）：消息通信中的输入数据(也就是参数)。例如，如果智能合约就是一个域名注册服务，那么调用合约可能就会期待输入参数：域名和IP地址 在“账户”这个章节中我们学到交易—消息通信和合约创建交易两者都总是被外部拥有账户触发并提交到区块链的。换种思维思考就是，交易是外部世界和以太坊内部状态的桥梁。 但是这也并不代表一个合约与另一个合约无法通信。在以太坊状态全局范围内的合约可以与在相同范围内的合约进行通信。他们是通过“消息”或者“内部交易”进行通信的。（PS：没看懂）我们可以认为消息或内部交易类似于交易，不过与交易有着最大的不同点—它们不是由外部拥有账户产生的。相反，他们是被合约产生的。它们是虚拟对象，与交易不同，没有被序列化而且只存在于以太坊执行环境。 当一个合约发送一个内部交易给另一个合约，存在于接收者合约账户相关联的代码就会被执行。 一个需要注意的重要事情是内部交易或者消息不包含gasLimit。因为gas limit是由原始交易的外部创建者决定的（也就是外部拥有账户）。外部拥有账户设置的gas limit必须要高到足够将交易完成，包括由于此交易而产生的任何”子执行”，例如合约到合约的消息。如果，在一个交易或者信息链中，其中一个消息执行造成gas不足，那么这个消息的执行会被还原，包括任何被此执行触发的子消息。不过，父执行没必要被还原。 区块所有的交易都被组成一个”块”。一个区块链包含了一系列这样链在一起的区块。 在以太坊中，一个区块包含： 区块头 关于包含在此区块中交易集的信息 与当前块的ommers相关的一系列其他区块头 Ommers解释“ommer”到底是什么？ ommer就是一个区块的父区块与当前区块父区块的父区块是相同的。让我们快速了解一下ommers是用来干嘛的，并且为什么一个区块需要为ommers包含区块头。 由于以太坊的构造，它的区块生产时间（大概15秒左右）比其他的区块链例如Bitcoin（大概10分钟左右）要快很多。这使得交易的处理更快。但是，更短的区块生产时间的一个缺点就是：更多的竞争区块会被矿工发现。这些竞争区块同样也被称为“孤区块”（也就是被挖出来但是不会被添加到主链上的区块）。 Ommers的目的就是为了帮助奖励矿工纳入这些孤区块。矿工包含的ommers必须是有效的，也就是ommers必须是往上数6代之内或更小范围内父区块的子区块。 一个孤区块在第6个子区块之后，这种陈旧的孤区块将不会再被引用（因为包含老旧的交易会使事情变得复杂一点）。 Ommer区块会收到比全区块少一点的奖励。不管怎样，依然存在激励来让矿工们纳入孤区块并能从中获得一些报酬。 区块头让我们再回到区块的问题上。我们前面提到每个区块都有一个“区块头”，但这究竟是什么？ 区块头是区块的一部分，包含了： parentHash：父区块头的Hash值（这也是使得区块变成区块链的原因） ommerHash：当前区块ommers列表的Hash值 beneficiary：接收挖此区块费用的账户地址 stateRoot：状态树根节点的Hash值（回忆一下我们之前所说的保存在头中的状态树以及它使得轻客户端认证任何关于状态的事情都变得非常简单） transactionsRoot：包含此区块所有交易的Merkle树的根节点Hash值 receiptsRoot：包含此区块所有交易收据的Merkle树的根节点Hash值 logsBloom：由日志信息组成的一个Bloom过滤器 (一种数据结构) difficulty： 此区块的难度级别 number：当前区块的计数（创世纪块的区块序号为0，对于每个后续区块，区块序号都增加1） gasLimit：每个区块的当前gas limit gasUsed： 此区块中交易所用的总gas量 timestamp：此区块成立时的unix的时间戳 extraData：与此区块相关的附加数据 mixHash：一个Hash值，当与nonce组合时，证明此区块已经执行了足够的计算 nonce：一个Hash值，当与mixHash组合时，证明此区块已经执行了足够的计算 注意每个区块是如何包含三个树结构的，三个树结构分别对应： 状态（stateRoot） 交易（transactionsRoot） 收据（receiptsRoot）这三个树结构就是我们前面讨论的Merkle Patricia树。 另外，上面描述的有几个术语值得说明一下，下面来看一下。 日志以太坊允许日志可以跟踪各种交易和信息。一个合约可以通过定义“事件”来显示的生成日志。 一个日志的实体包含： 记录器的账户地址 代表本次交易执行的各种事件的一系列主题以及与这些事件相关的任何数据 日志被保存在bloom过滤器中，过滤器高效的保存了无尽的日志数据。（PS：这得保留多少？不是越来越大） 交易收据包含着日志信息的交易收据的根Hash值保存在头中。 就像你在商店买东西时收到的收据一样，以太坊为每笔交易都产生一个收据。像你期望的那样，每个收据包含关于交易的特定信息，这些信息为： 区块序号 区块Hash值 交易Hash值 当前交易使用了的gas 在当前交易执行完之后当前块使用的累计gas 执行当前交易时创建的日志 等等 区块难度区块的难度是被用来在验证区块时加强一致性。创世纪区块的难度是131,072，有一个特殊的公式用来计算之后的每个块的难度。如果某个区块比前一个区块验证的更快，以太坊协议就会增加区块的难度。 区块的难度影响nonce，它是在挖矿时必须要使用工作量证明算法计算出的一个Hash值。 区块难度和nonce之间的关系用数学形式表达就是：Hd代表的是难度。 找到符合难度阈值的nonce唯一方法就是使用工作量证明算法来列举所有的可能性。找到解决方案预期时间与难度成正比—难度越高，找到nonce就越困难，因此验证一个区块也就越难，这又相应地增加了验证新块所需的时间。所以，通过调整区块难度，协议可以调整验证区块所需的时间。 另一方面，如果验证时间变的越来越慢，协议就会降低难度。这样的话，验证时间自我调节以保持恒定的速率—平均每15s一个块。 交易执行我们已经到了以太坊协议最复杂的部分：交易的执行。假设你发送了一笔交易给以太坊网络处理，将以太坊状态转换成包含你的交易这个过程到底发生了什么？ 首先，为了可以被执行所有的交易必须都要符合最基础的一系列要求，包括： 交易必须是正确格式化的RLP。”RLP”代表Recursive Length Prefix，它是一种数据格式，用来编码二进制数据嵌套数组。以太坊就是使用RLP格式序列化对象。 有效的交易签名。 有效的交易序号。回忆一下账户中的nonce就是从此账户发送出去交易的计数。如果有效，那么交易序号一定等于发送账户中的nonce。 交易的gas limit 一定要等于或者大于交易使用的intrinsic gas，intrinsic gas包括：1.执行交易预订费用为21,000gas2.随交易发送的数据的gas费用（每字节数据或代码为0的费用为4gas，每个非零字节的数据或代码费用为68gas）3.如果是合约创建交易，还需要额外的32,000gas 发送账户余额必须有足够的Ether来支付”前期”gas费用。前期gas费用的计算比较简单：首先，交易的gas limit乘以交易的gas价格得到最大的gas费用。然后，这个最大的gas费用加上从发送方传送给接收方的总值。 如果交易符合上面的所有要求，那么我们进行下面的步骤。 第一步，我们从发送者的余额中扣除执行的前期费用，并为当前交易将发送者账户中的nonce增加1。此时，我们可以计算剩余的gas，将交易的总gas减去使用的intrinsic gas。 第二步，开始执行交易。在交易执行的整个过程中，以太坊保持跟踪“子状态”。子状态是记录在交易中生成的信息的一种方式，当交易完成时会立即需要这些信息。具体来说，它包含： 自毁集：在交易完成之后会被丢弃的账户集（如果存在的话） 日志系列：虚拟机的代码执行的归档和可检索的检查点 退款余额：交易完成之后需要退还给发送账户的总额。回忆一下我们之前提到的以太坊中的存储需要付费，发送者要是清理了内存就会有退款。以太坊使用退款计数进行跟踪退款余额。退款计数从0开始并且每当合约删除了一些存储中的东西都会进行增加。 第三步，交易所需的各种计算开始被处理。 当交易所需的步骤全部处理完成，并假设没有无效状态，通过确定退还给发送者的未使用的gas量，最终的状态也被确定。除了未使用的gas，发送者还会得到上面所说的“退款余额”中退还的一些津贴。 一旦发送者得到退款之后： gas的Ether就会给矿工 交易使用的gas会被添加到区块的gas计数中（计数一直记录当前区块中所有交易使用的gas总量，这对于验证区块时是非常有用的） 所有在自毁集中的账户（如果存在的话）都会被删除 最后，我们就有了一个新的状态以及交易创建的一系列日志。 现在我们已经介绍了交易执行的基本知识，让我们再看看合约创建交易和消息通信的一些区别。 合约创建(Contract creation) 回忆一下在以太坊中，有两种账户类型：合约账户和外部拥有账户。当我们说一个交易是“合约创建”，是指交易的目的是创建一个新的合约账户。 为了创建一个新的合约账户，我们使用一个特殊的公式来声明新账户的地址。然后我们使用下面的方法来初始化一个账户： 设置nonce为0 如果发送者通过交易发送了一定量的Ether作为value，那么设置账户的余额为value 将存储设置为0 设置合约的codeHash为一个空字符串的Hash值 一旦我们完成了账户的初始化，使用交易发送过来的init code（查看”交易和消息”章节来复习一下init code），实际上就创造了一个账户。init code的执行过程是各种各样的。取决于合约的构造器，可能是更新账户的存储，也可能是创建另一个合约账户，或者发起另一个消息通信等等。 当初始化合约的代码被执行之后，会使用gas。交易不允许使用的gas超过剩余gas。如果它使用的gas超过剩余gas，那么就会发生gas不足异常(OOG)并退出。如果一个交易由于gas不足异常而退出，那么状态会立刻恢复到交易前的一个点。发送者也不会获得在gas用完之前所花费的gas。 不过，如果发送者随着交易发送了Ether，即使合约创建失败Ether也会被退回来。 如果初始化代码成功的执行完成，最后合约创建的花费会被支付。这些是存储成本，与创建的合约代码大小成正比（再一次，没有免费的午餐）。如果没有足够的剩余gas来支付最后的花费，那么交易就会再次宣布gas不足异常并中断退出。 如果所有的都正常进行没有任何异常出现，那么任何剩余的未使用gas都会被退回给原始的交易发送者，现在改变的状态才被允许永久保存。 消息通信(Message calls)消息通信的执行与合约创建比较类似，只不过有一点点区别。 由于没有新账户被创建，所以消息通信的执行不包含任何的init code。不过，它可以包含输入数据，如果交易发送者提供了此数据的话。一旦执行，消息通信同样会有一个额外的组件来包含输出数据，如果后续执行需要此数据的话组件就会被使用。 就像合约创建一样，如果消息通信执行退出是因为gas不足或交易无效（例如栈溢出，无效跳转目的地或无效指令），那么已使用的gas是不会被退回给原始触发者的。相反，所有剩余的未使用gas也会被消耗掉，并且状态会被立刻重置为余额转移之前的那个点。 没有任何方法停止或恢复交易的执行而不让系统消耗你提供的所有gas，直到最新的以太坊更新。例如，假设你编写了一个合约，当调用者没有授权来执行这些交易的时候抛出一个错误。在以太坊的前一个版本中，剩余的gas也会被消耗掉，并且没有任何gas退回给发送者。但是拜占庭更新包括了一个新的“恢复”代码，允许合约停止执行并且恢复改变的状态而不消耗剩余的gas，此代码还拥有返回交易失败原因的能力。如果一个交易是由于恢复而退出，那么未使用的gas就会被退回给发送者。 执行模式到目前为止，我们了解了从开始到结束交易的执行必须经历的一系列步骤。现在，我们来看看交易究竟是如何在虚拟机(VM)中执行的。 协议实际操作交易处理的部分是以太坊自己的虚拟机，称之为以太坊虚拟机(EVM)。 像之前定义的那样，EVM是图灵完备虚拟机器。EVM存在而典型图灵完备机器不存在的唯一限制就是EVM本质上是被gas束缚。因此，可以完成的计算总量本质上是被提供的gas总量限制的。 此外，EVM具有基于堆栈的架构。堆栈机器就是使用后进先出来保存临时值的计算机。 EVM中每个堆栈项的大小为256位，堆栈有一个最大的大小，为1024位。 EVM有内存，各项按照可寻址字节数组来存储。内存是易失性的，也就是数据是不持久的。 EVM也有一个存储器。不像内存，存储器是非易失性的，并作为系统状态的一部分进行维护。EVM分开保存程序代码，在虚拟ROM 中只能通过特殊指令来访问。这样的话，EVM就与典型的冯·诺依曼架构不同，此架构将程序的代码存储在内存或存储器中。 EVM同样有属于它自己的语言：“EVM字节码”，当一个程序员比如你或我写一个在以太坊上运行的智能合约时，我们通常都是用高级语言例如Solidity来编写代码。然后我们可以将它编译成EVM可以理解的EVM字节码。 好了现在来说执行。 在执行特定的计算之前，处理器会确定下面所说的信息是否有效和是否可获取： 系统状态 用于计算的剩余gas 拥有执行代码的账户地址 原始触发此次执行的交易发送者的地址 触发代码执行的账户地址（可能与原始发送者不同） 触发此次执行的交易gas price 此次执行的输入数据 Value(单位为Wei)作为当前执行的一部分传递给该账户 待执行的机器码 当前区块的区块头 当前消息通信或合约创建堆栈的深度 执行刚开始时，内存和堆栈都是空的，程序计数器为0。 1 PC: 0 STACK: [] MEM: [], STORAGE: {} 然后EVM开始递归的执行交易，为每个循环计算系统状态和机器状态。系统状态也就是以太坊的全局状态(global state)。机器状态包含： 可获取的gas 程序计数器 内存的内容 内存中字的活跃数 堆栈的内容 堆栈中的项从系列的最左边被删除或者添加。 每个循环，剩余的gas都会被减少相应的量，程序计数器也会增加。在每个循环的结束，都有三种可能性： 机器到达异常状态（例如 gas不足，无效指令，堆栈项不足，堆栈项会溢出1024，无效的JUMP/JUMPI目的地等等）因此停止，并丢弃所有更改 进入后续处理下一个循环 机器到达了受控停止（到达执行过程的终点） 假设执行没有遇到异常状态，达到一个“可控的”或正常的停止，机器就会产生一个合成状态，执行之后的剩余gas、产生的子状态、以及组合输出。 呼。我们终于过了一遍以太坊最难的部分了。如果你不能完全理解这个部分，也没关系。除非你在研究非常深层次的东西，否则你真的没有必要去理解交易执行的每个细节。 一个块是如何完成的？最后，让我们看看一个包含许多交易的块是如何完成的。 当我们说“完成”，取决于此块是新的还是已存在的，可以指两个不同的事情。如果是个新块，就是指挖这个块所需的处理。如果是已存在的块，就是指验证此块的处理。不论哪种情况，一个块的“完成”都有4个要求：1）验证（或者，如果是挖矿的话，就是确定）ommers在区块头中的每个ommer都必须是有效的并且必须在当前块往上6代之内 2）验证（或者，如果是挖矿的话，就是确定）交易区块中的gasUsed数量必须与区块中所列交易使用的累积gas量相等。（回忆一下，当执行一个交易的时候，我们会跟踪区块的gas计数器，也就跟踪了区块中所有交易使用的gas总数量） 3）申请奖励（只有挖矿时）受益人的地址会因为挖矿而获得5Ether（在以太坊EIP-649 提案中，5ETH很快将会被减少为3ETH）。另外，对于每个ommer，当前块的受益人会获得额外的1/32当前块奖励金的奖励。最近，每个ommer区块的受益人能够得到一定量的奖励（有个特殊公式可以进行计算）。 4）校验（或者，如果是挖矿的话，就是计算一个有效的）状态和nonce确保所有的交易和改变的结果状态都被应用了，然后在区块奖励被应用于最终交易结果状态之后定义一个新块为状态。通过检查最终状态与存储在头中的状态树来进行验证。 工作量证明挖矿在“区块”这个章节简短的说明了一下区块难度这个概念。给予区块难度意义的算法叫做工作量证明（PoW）。 以太坊的工作量证明算法称之为“Ethash” （之前叫做Dagger-Hashimoto）。算法正式定义为：m代表的是mixHash，n代表的是nonce，Hn代表的是新区块的头（不包含需要计算的nonce和mixHash），Hn是区块头的nonce，d是DAG ，就是一个大数据集。 在”区块”章节，我们讨论了存在于区块头中的多项。其中两项叫做mixHash和nonce。也许你会回忆起： mixHash：一个Hash值，当与nonce组合时，证明此区块已经执行了足够的计算 nonce：一个Hash值，当与mixHash组合时，证明此区块已经执行了足够的计算 PoW函数就是用来估算这两项的。mixHash和nonce到底是如何使用PoW函数来计算出来的有点复杂，如果深入了解的话，我们可以另写一篇文章来讲解了。但是在一个高层面上，它大致就是这样计算的：会为每个区块计算一个”种子”。每个“时期”的种子都不一样，每个时期是30,000个区块长度。对于第一时期，种子就是32位0的Hash值。对于后续的每个时期，种子就是前一个种子Hash值的Hash值。使用这个种子，节点可以计算出一个伪随机“缓存”。 这个缓存是非常有用的，因为它可以使“轻节点”的概念变成现实，轻节点概念在这篇文章的前面讨论过。轻节点的目的就是让某个节点有能力高效的校验交易而用不着存储整个区块链的数据集。一个轻节点可以仅基于缓存来校验一个交易的有效性，因为缓存可以重新生成需要校验的特定块。 使用这个缓存，节点可以生成DAG“数据集”，数据集中的每项取决于缓存中少量伪随机选择项。为了成为矿工，你需要要生成全数据集，所有全客户端和矿工都保存这个数据集，并且这个数据集随着时间线性增长。 然后矿工可以随机抽取数据集中的部分并将它们放入一个数学函数中Hash出一个”mixHash”。矿工会重复生成mixHash直到输出的值小于想要的目标值nonce。当输出的值符合这个条件的时候，nonce就被认为是有效的，然后区块就被添加到链中。 挖矿作为安全机制总的来说，PoW的目的就是以加密安全的方式证明生成的一些输出（也就是nonce）是经过了一定量的计算的。因为除了列举所有的可能性，没有更好的其他方法来找到一个低于要求阈值的nonce。重复应用Hash函数的输出均匀分布，所以我们可以确保，在平均值上，找到满足要求的nonce所需时间取决于难度阈值。难度系数越大，所需时间越长。这样的话，PoW算法就给予难度这个概念意义了：用来加强区块链的安全。 我们所说的区块链的安全又是什么意思？这非常简单：我们想要创造一个每个人都信任的区块链。像我们之前在这篇文章中讨论的那样，如果存在超过1条以上的链，用户的信任就会消失，因为他们没有能力合理的确认哪条链才是“有效的”。为了让一群用户接受存储在区块链中的潜在状态，我们需要有一群人信任的一个权威区块链。 这完完全全就是Pow算法所做的事情：它确保特定的区块链直到未来都一直保持着权威性，让攻击者创造一个新区块来重写某个历史部分（例如清除一个交易或者创建一个假的交易）或者保持一个分叉变得非常困难。为了首先让他们的区块被验证，攻击者需要总是比网络上的其他人更快的解决掉nonce问题，这样网络就会相信他们的链是最重的链（基于我们之前提到的GHOST协议原则）。除非攻击者拥有超过一半的网络挖矿能力（这种场景也被称为大多数51%攻击），要不然这基本上是不可能的。 挖矿作为财富分配机制除了提供一个安全的区块链，PoW同样也是分配财富给那些为提供这个安全而花费自己计算力的人的一种方法。回忆一下，一个矿工挖出一个区块的时候会获得奖励，包括： 为“获胜”区块提供的5 ether静态区块奖励（马上就会变成3 ether） 区块中的交易在区块内所消耗的gas 纳入ommers作为区块的一部分的额外奖励 为了保证PoW共识算法机制对安全和财富分配的使用是长期可持续的，以太坊努力灌输这两个特性： 尽可能的让更多的人可访问。换句话说，人们不需要特殊的或者与众不同的硬件来运行这个算法。这样做的目的是为了让财富分配模式变的尽可能的开放，以便任何人都可以提供一些算力而获得Ether作为回报。 降低任何单个节点（或小组）能够创造与其不成比例的利润可能性。任何可以创造不成比例的利润的节点拥有比较大的影响力来决定权威区块链。这是件麻烦的事情，因为这降低了网络的安全性。 在区块链网络中，与上面两个特性有关的一个问题是PoW算法是SHA256哈希函数。这种函数的缺点就是它使用特殊的硬件（也被称之为ASCIs）可以更加快速高效的解决nonce问题。 为了减轻这个问题，以太坊选择让PoW算法(Ethhash) 提高内存级别难度。意思是此算法被设计为计算出要求的nonce需要大量的内存和带宽。大量内存的需求让电脑平行的使用内存同时计算多个nonce变得极其困难，高带宽的需求让即使是超级电脑同时计算多个nonce也变得十分艰难。这种方式降低了中心化的风险，并为正在进行验证的节点提供了更加公平的竞争环境。 有一件值得注意的事情是以太坊正在从PoW共识机制渐渐转换为一个叫做“权益证明(PoS)”的共识算法。这就是一个比较野心的话题了，我们希望可以在未来的文章中探索这个话题。 总结（PS：原文的总结）呼！ 你终于坚持到最后了。我希望如此？ 这篇文章中有很多的地方需要消化。如果需要你阅读好几遍才能理解怎么回事，这完全正常。我个人重复阅读了好几次以太坊黄皮书，白皮书，以及代码的不同部分才渐渐明白是怎么回事。 无论如何，我希望你觉得这篇文章对你有帮助。如果你发现了任何的错误或失误，我很乐意你给我写个私人消息或者直接在评论区评论（我保证我会查看所有评论）。 记住，我是个人类（对，这是真的），我会犯错误。为了社区的利益，我花时间免费写了这篇文章。所以请你在反馈时不要带着没必要的攻击性，尽量是建设性的反馈。 以太坊的黄皮书 总结（自己的总结）第二遍读完，又多消化了一些内容，也总结了一些问题，列在下面： storageRoot和codeHash存储的都是hash值，那么原本的内容被存在哪里？ 合约代码会发生改变吗？代码里面的状态值是怎样体现改变的？ 账户的唯一性是由nonce来确定的吗？ 账户的信息又是怎么保存在区块链上呢？ 什么时候需要重新执行交易呢？交易的结果状态没有被记录吗？ 日志是如何保留的？ 下一步，一个问题一个问题去研究。]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-checkout的用法总结（2）]]></title>
    <url>%2Fgit%2Fgit-checkout-2.html</url>
    <content type="text"><![CDATA[原帖收藏于IT老兵驿站，传递一个IT老兵在凋零前的光和氧。Git的git-checkout的用法总结。 前言结合前一篇文章，再认真总结一下git-checkout的用法，因为可能一次总结到不了位，那么就不怕啰嗦，不怕重复，多总结几次，这样可能会造成每篇文章内容之间的分布不是那么清晰，将来再做更好的整理吧，因为现在这种方式，对于当前的学习是有帮助的，是从浅入深的。 正文概要1git-checkout - Switch branches or restore working tree files 从上面可见，git checkout是用来切换分支或者回复工作目录的，看到这里，记住这一点，会有很大帮助。（备注：我发现，有的时候，学习的节奏放慢一些，反而其实是更有效率的，反而是更快的。多思考一些，记的更准确，因慢得快） 语法1234567git checkout [-q] [-f] [-m] [&lt;branch&gt;]git checkout [-q] [-f] [-m] --detach [&lt;branch&gt;]git checkout [-q] [-f] [-m] [--detach] &lt;commit&gt;git checkout [-q] [-f] [-m] [[-b|-B|--orphan] &lt;new_branch&gt;] [&lt;start_point&gt;]git checkout [-f|--ours|--theirs|-m|--conflict=&lt;style&gt;] [&lt;tree-ish&gt;] [--] &lt;paths&gt;…​git checkout [&lt;tree-ish&gt;] [--] &lt;pathspec&gt;…​git checkout (-p|--patch) [&lt;tree-ish&gt;] [--] [&lt;paths&gt;…​] 用法11git checkout &lt;branch&gt; 用来切换到一个分支上。切换index和工作目录，还有HEAD 指针到这个分支上。本地发生的修改也会被保留。如果本地不存在这个分支而远程存在同名分支的话，则这个命令相当于： 1$ git checkout -b &lt;branch&gt; --track &lt;remote&gt;/&lt;branch&gt; 1git checkout -b|-B &lt;new_branch&gt; [&lt;start point&gt;] -b表示创建新分支；如果分支存在的话，不进行任何处理。-B 在创建新分支的功能和-b 是一样的；但是，如果分支存在的话，它会重置&lt;start_point&gt;。 1234567Specifying -b causes a new branch to be created as if git-branch[1] were called and then checked out. In this case you can use the --track or --no-track options, which will be passed to git branch. As a convenience, --track without -b implies branch creation; see the description of --track below.If -B is given, &lt;new_branch&gt; is created if it doesn’t exist; otherwise, it is reset. This is the transactional equivalent of$ git branch -f &lt;branch&gt; [&lt;start point&gt;]$ git checkout &lt;branch&gt;that is to say, the branch is not reset/created unless &quot;git checkout&quot; is successful. 这里涉及到track 和&lt;start point&gt;的概念。track表示的是远程仓库与之对应的分支，这个信息被称为upstream，上游，远程仓库的，是上游。本地的，是下游，有一个对应的关系。track 本意是轨迹、跟踪的意思，使用了--track或者--no-track 来设置这个，这个信息会传递给git branch。 用法212git checkout --detach [&lt;branch&gt;]git checkout [--detach] &lt;commit&gt; 切换代码到某一个提交号或者分支上，并且分离了HEAD指针，指向了这个提交。这块有点复杂，还需要理解深度理解一下，这个可能要留到下一篇帖子来完成了，争取每天整理一些（2018-08-03）。 整理完成，可以参考这里（2018-08-04）。 用法312git checkout [&lt;tree-ish&gt;] [--] &lt;pathspec&gt;…​git checkout (-p|--patch) [&lt;tree-ish&gt;] [--] [&lt;pathspec&gt;…​] 从index或者&lt;tree-ish&gt; 检出代码来替换&lt;pathspec&gt; 处的代码。如果&lt;tree-ish&gt; 被指定了，那么index和工作空间的代码都会被更新。 The index may contain unmerged entries because of a previous failed merge. By default, if you try to check out such an entry from the index, the checkout operation will fail and nothing will be checked out. Using -f will ignore these unmerged entries. The contents from a specific side of the merge can be checked out of the index by using –ours or –theirs. With -m, changes made to the working tree file can be discarded to re-create the original conflicted merge result. index区域可能还有一些没有merge的条目，因为之前有失败的merge。 后一种用法使用互动的方式来完成这个功能。 到此，这个命令的用法基本整理完，下一步，要实践一些实例。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>用法</tag>
        <tag>checkout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：DETACHED HEAD的概念]]></title>
    <url>%2Fgit%2Fgit-DETACHED-HEAD.html</url>
    <content type="text"><![CDATA[Git：DETACHED HEAD的概念。 博客原帖收藏于IT老兵驿站，传递一个IT老兵在凋零前的光和氧。 前言在git使用的时候，经常会碰到DETACHED HEAD，在此总结一下。 正文 HEAD normally refers to a named branch (e.g. master). Meanwhile, each branch refers to a specific commit. Let’s look at a repo with three commits, one of them tagged, and with branch master checked out: HEAD是一个指针，指向一个branch。 1234567 HEAD (refers to branch &apos;master&apos;) | va---b---c branch &apos;master&apos; (refers to commit &apos;c&apos;) ^ | tag &apos;v2.0&apos; (refers to commit &apos;b&apos;) 上面是一个常见的例子，三个提交，HEAD指针指向c，往往是这个分支上最后的提交。然后又进行了一次修改和提交，生成了d。 123456789$ edit; git add; git commit HEAD (refers to branch &apos;master&apos;) | va---b---c---d branch &apos;master&apos; (refers to commit &apos;d&apos;) ^ | tag &apos;v2.0&apos; (refers to commit &apos;b&apos;) HEAD指向了d。这个时候我们需要重新检出v2.0版本（这种可能性是很大，经常容易出现的），如下： 12$ git checkout v2.0 # or$ git checkout master^^ 这个“^^”符号需要记忆一下。1234567 HEAD (refers to commit &apos;b&apos;) | va---b---c---d branch &apos;master&apos; (refers to commit &apos;d&apos;) ^ | tag &apos;v2.0&apos; (refers to commit &apos;b&apos;) 这个时候HEAD指针就指向了b，这就是detached HEAD状态，这意味着HEAD指向了某一个提交了，而不再指向当前分支的最后一个提交了。 然后我们又进行了一次提交，就会变成这样： 1234567891011$ edit; git add; git commit HEAD (refers to commit &apos;e&apos;) | v e /a---b---c---d branch &apos;master&apos; (refers to commit &apos;d&apos;) ^ | tag &apos;v2.0&apos; (refers to commit &apos;b&apos;) one more time，再来一次：1$ edit; git add; git commit 123456789 HEAD (refers to commit &apos;f&apos;) | v e---f /a---b---c---d branch &apos;master&apos; (refers to commit &apos;d&apos;) ^ | tag &apos;v2.0&apos; (refers to commit &apos;b&apos;) HEAD指向了f。我们可以做任何正常的git操作，如果你想回到master分支，那么。 123456789$ git checkout master HEAD (refers to branch &apos;master&apos;) e---f | / va---b---c------d branch &apos;master&apos; (refers to commit &apos;d&apos;) ^ | tag &apos;v2.0&apos; (refers to commit &apos;b&apos;) HEAD重新指向b。这个时候要意识到没有指针指向f提交，最后e和f 都会被常规的Git垃圾回收所删除掉，除非我们创建一个指针，例如: 123$ git checkout -b foo (1)$ git branch foo (2)$ git tag foo (3) 创建了一个新的分支指向f，并且更新了HEAD指针，这样HEAD指针就不再是detached状态了 简单创建了一个新的分支指向f，这个时候HEAD指针仍然是detached状态。3.创建了一个新tag，指向f，这个时候HEAD指针仍然是detached状态。 If we have moved away from commit f, then we must first recover its object name (typically by using git reflog), and then we can create a reference to it. For example, to see the last two commits to which HEAD referred, we can use either of these commands: 如果f已经被移除了，我们首先需要恢复它的对象名，使用git reflog，然后我们创建一个指针指向它。例如，想看到HEAD之前的最后两个提交，我们可以使用下面的命令（二选一）： 12$ git reflog -2 HEAD # or$ git log -g -2 HEAD 总结这篇总结基本上还是以翻译为主，留了一个问题，就是reflog。这些箭头用MD画起来很痛苦，所见不是所得。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>DETACHED HEAD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-tag的用法总结]]></title>
    <url>%2Fgit%2Fgit-tag.html</url>
    <content type="text"><![CDATA[原帖收藏于IT老兵驿站，传递一个IT老兵在凋零前的光和氧。Git：git-tag的用法总结。 前言Git的tag和SVN不一样，SVN的tag其实还是一个分支，Git的tag则真的是一个标签，是给某一个commit打上一个标签，这个说明了Linus的巧妙设计，那么，对这个tag做一个笔记。 正文记录几个主要的命令，用于速查：实例：列显已有的标签 1$ git tag 实例：只看某一个tag 1$ git tag -l &apos;v1.4.2.*&apos; 实例：新建含附注的标签 1$ git tag -a v1.4 -m &apos;my version 1.4&apos; -a相当于给标签起个名字，-m是写一些注释。 实例：查看相应标签的版本信息，并连同显示打标签时的提交对象 1$ git show v1.4 实例：签署标签1$ git tag -s v1.5 -m &apos;my signed 1.5 tag&apos; 实例：轻量级标签1$ git tag v1.4-lw 实例：验证标签1$ git tag -v v1.4.2.1 这个功能的具体意义还没有搞明白。 实例：后期加注标签 1$ git tag -a v1.2 9fceb02 实例：推送某一标签到服务器1$ git push origin v1.5 实例：推送所有标签到服务器1$ git push origin --tags 参考https://git-scm.com/book/zh/v1/Git-%E5%9F%BA%E7%A1%80-%E6%89%93%E6%A0%87%E7%AD%BE]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git tag</tag>
        <tag>标签</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-reset的用法总结]]></title>
    <url>%2Fgit%2Fgit-reset.html</url>
    <content type="text"><![CDATA[概要Git：git-reset的用法总结。 博客博客地址：IT老兵驿站。 前言Git reset感觉是相当复杂的一个指令，用了快一年了，总感觉还没有用明白，所以，需要好好总结一下。 语法123git reset [-q] [&lt;tree-ish&gt;] [--] &lt;paths&gt;…​git reset (--patch | -p) [&lt;tree-ish&gt;] [--] [&lt;paths&gt;…​]git reset [--soft | --mixed [-N] | --hard | --merge | --keep] [-q] [&lt;commit&gt;] 这个命令有三种用法，前两种用来从&lt;tree-ish&gt;所指定的地方拷贝条目到index区域。其实这个是说从仓库的某一个版本获取到index区域，感觉这个用法更多是用在了对于git add等操作的逆向操作。 第三种格式，将当前分支的HEAD指针设定为&lt;commit&gt; 这个提交号，同时可以选择性地修改index和工作区域。 tree-ish是什么意思呢？要参考这里，是Git所使用的指明路径的语法。类似以下这样的格式： 123&lt;rev&gt;:&lt;path&gt;, e.g. HEAD:README, :README, master:./README A suffix : followed by a path names the blob or tree at the given path in the tree-ish object named by the part before the colon. 是指某个版本的某个文件，或者某个分支的某个文件。 用法1git reset [-q] [&lt;tree-ish&gt;] [--] &lt;paths&gt;…​ This form resets the index entries for all to their state at . (It does not affect the working tree or the current branch.) This means that git reset is the opposite of git add . After running git reset to update the index entry, you can use git-checkout[1] to check the contents out of the index to the working tree. Alternatively, using git-checkout[1] and specifying a commit, you can copy the contents of a path out of a commit to the index and to the working tree in one go. 这个用法将index区域所有符合的条目修改为&lt;tree-ish&gt; 的状态。（这并不影响工作目录或者当前分支。）这个reset是更新index条目，更新后，可以从index中通过checkout指令获取内容到工作目录。如果git checkout指定了一个提交号，那么就可以根据这个提交号更新内容到index和工作目录。 关于ORIG_HEAD的介绍：需要参考这里。 HEAD is (direct or indirect, i.e. symbolic) reference to the current commit. It is a commit that you have checked in the working directory (unless you made some changes, or equivalent), and it is a commit on top of which “git commit” would make a new one. Usually HEAD is symbolic reference to some other named branch; this branch is currently checked out branch, or current branch. HEAD can also point directly to a commit; this state is called “detached HEAD”, and can be understood as being on unnamed, anonymous branch. And @ alone is a shortcut for HEAD, since Git 1.8.5 ORIG_HEAD is previous state of HEAD, set by commands that have possibly dangerous behavior, to be easy to revert them. It is less useful now that Git has reflog: HEAD@{1} is roughly equivalent to ORIG_HEAD (HEAD@{1} is always last value of HEAD, ORIG_HEAD is last value of HEAD before dangerous operation). For more information read git(1) manpage, Git User’s Manual, the Git Community Book and Git Glossary HEAD是指当前分支上当前的提交号。ORIG_HEAD 是指上一个HEAD所指向的提交号。这其实是两个指针，第二个指针的设计其实是为了做保护，一旦第一个指针被误操作了，还有机会去挽回。 实例：将发生改变的文件 _config.yml加入index ：1$ git add _config.yml 这样_config.yml 文件就被加入到index区域中，显示成：1234Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: _config.yml 撤销上面的操作，恢复_config.yml为版本库中的状态：1$ git reset _config.yml 这种用法相当于&lt;tree-ish&gt;的参数设置为HEAD。 用法2git reset (--patch | -p) [&lt;tree-ish&gt;] [--] [&lt;paths&gt;…​] Interactively select hunks in the difference between the index and (defaults to HEAD). The chosen hunks are applied in reverse to the index. This means that git reset -p is the opposite of git add -p, i.e. you can use it to selectively reset hunks. See the “Interactive Mode” section of git-add[1] to learn how to operate the –patch mode. 这个用法是以patch 的方式展示出来需要reset的代码， git reset -p 和git add -p 就是一对互为反向的操作，后者是把工作目录下变更的代码以patch 的方式展示出来，以互动的方式应用到index上，前者则是一个反向操作。 实例：将发生改变的文件 _config.yml加入index ：1$ git add -p _config.yml 撤销上面的操作：1$ git reset -p _config.yml 可以看到，这种用法相对上面那一种用法其实是增加了互动的提醒。 用法3git reset [&lt;mode&gt;] [&lt;commit&gt;] This form resets the current branch head to and possibly updates the index (resetting it to the tree of ) and the working tree depending on . If is omitted, defaults to “–mixed”. The must be one of the following: 这个用法是用来设置当前的分支的HEAD指针，或者index 的指向当前版本的指针，或者工作空间指向当前版本的指针。 –softDoes not touch the index file or the working tree at all (but resets the head to , just like all modes do). This leaves all your changed files “Changes to be committed”, as git status would put it. soft参数用来设置HEAD指针。 –mixedResets the index but not the working tree (i.e., the changed files are preserved but not marked for commit) and reports what has not been updated. This is the default action. mixed参数用来设置index指针，文件的修改仍然会被保留，但是没有纳入到index中。 If -N is specified, removed paths are marked as intent-to-add (see git-add[1]). –hardResets the index and working tree. Any changes to tracked files in the working tree since are discarded. hard重置index和工作区域，所有在这个&lt;commit&gt;之后的修改将被丢弃。 –mergeResets the index and updates the files in the working tree that are different between and HEAD, but keeps those which are different between the index and working tree (i.e. which have changes which have not been added). If a file that is different between and the index has unstaged changes, reset is aborted. In other words, –merge does something like a git read-tree -u -m , but carries forward unmerged index entries. merge重置index ，并且更新那些工作区的文件（在&lt;commit&gt;和HEAD中不同的）。这个还需要进一步理解一下。 –keepResets index entries and updates files in the working tree that are different between and HEAD. If a file that is different between and HEAD has local changes, reset is aborted. If you want to undo a commit other than the latest on a branch, git-revert[1] is your friend. 实例 版本库中的提交如下：1234567891011121314151617181920212223commit cac453cf6501c3ea3b626636bc4399ed48704543 (HEAD -&gt; master, origin/master, origin/HEAD)Author: xxx &lt;xxx@xxx.xxx&gt;Date: Fri Jul 27 18:08:46 2018 +0800 从版本库中移除项目配置文件和日志配置文件commit cd36b7297106a871ae331f487179fd5584fb38cdAuthor: xxx &lt;xxx@xxx.xxx&gt;Date: Fri Jul 27 18:06:37 2018 +0800 暂时参考原来的逻辑，使用硬编码的方式，新增了权限commit 24f19e80b5b8e2c05faf04706d95b5ac538ddbdd (f_1486)Author: xxx &lt;xxx@xxx.xxx&gt;Date: Wed Jul 11 22:00:43 2018 +0800 修改了login的登录按钮的宽度commit e948bb044676ff917be862d9fae8391ba1b82351Author: xxx &lt;xxx@xxx.xxx&gt;Date: Tue Jul 10 23:39:28 2018 +0800 完成初步的修改 现在发现最后三次提交是存在问题的，不应该直接提交到master上，这个时候需要把HEAD指针恢复到倒数第四次提交上。1git reset --soft e948bb044676ff917be862d9fae8391ba1b82351 这样后面三次提交的改变从版本库还原出来，变成尚未提交的状态，这样我们就可以新开一个临时的dev分支，继续我们之前的工作（参考Git Flow一篇文章）。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>用法</tag>
        <tag>git reset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链：比特币难度的概念]]></title>
    <url>%2Fblockchain%2Fbitcoin-difficulty.html</url>
    <content type="text"><![CDATA[原帖收藏于IT老兵驿站，传递一个IT老兵在凋零前的光和氧。区块链：比特币难度的概念。 前言这篇笔记记录一下对于比特币难度的学习，参考这里，记录下来自己的理解，哩哩啦啦地写了两天，边看边查，还是没有理解到位，等待之后再补充吧。 正文 Difficulty is a measure of how difficult it is to find a hash below a given target. Difficulty 是用来衡量找到一个低于给定目标的hash的困难程度。 难度公式： difficulty = difficulty_1_target / current_target(target is a 256 bit number) 有许多不同测量难度的方法，得到的difficulty_1_target可能不同。传统地，它表示一个HASH值，前32位为0，后面都为1（也就是被称为“矿池难度”或“pdiff”的值），比特币协议把目标HASH表示成一个有限精度的自定义浮点类型。因而，比特币客户端用该值来估计难度(称之为：“bdiff”)。 难度如何保存在区块上呢？每一个区块会用一种压缩的格式（被称为“Bits”）来表示实际的16进制的目标值。目标值通过一个预先定义好的公式，从这个压缩值中得出。举一个例子，压缩值为0x1b0404cb，16进制的目标值则是： 10x0404cb * 2**(8*(0x1b - 3)) = 0x00000000000404CB000000000000000000000000000000000000000000000000 Note that the 0x0404cb value is a signed value in this format. The largest legal value for this field is 0x7fffff. To make a larger value you must shift it down one full byte. Also 0x008000 is the smallest positive valid value. 注意0x0404cb 是一个符号数，最大值是0x7fffff，这个明白，下面两句就不明白了：如果想生成一个更大的值，你需要向下移动一整个字节。同时，0x008000 是最小的正值（为什么不是0x000001呢？）。 难度如何计算？最大目标难度（1）被定义成0x1d00ffff， 那么 10x00ffff * 2**(8*(0x1d - 3)) = 0x00000000FFFF0000000000000000000000000000000000000000000000000000 这个是截断后的目标值，就是上文所说的比特币协议定义的格式，如果没有截断，那么就是： 10x00000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF 所以0x1b0404cb 位置的难度是： 1230x00000000FFFF0000000000000000000000000000000000000000000000000000 /0x00000000000404CB000000000000000000000000000000000000000000000000 = 16307.420938523983 (bdiff) 或者： 1230x00000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF /0x00000000000404CB000000000000000000000000000000000000000000000000 = 16307.669773817162 (pdiff) 当前难度是什么Current difficulty。这个是比特币浏览器提供的接口，给出了当前难度。 什么是最大难度 There is no minimum target. The maximum difficulty is roughly: maximum_target / 1 (since 0 would result in infinity), which is a ridiculously huge number (about 2^224). The actual maximum difficulty is when current_target=0, but we would not be able to calculate the difficulty if that happened. (fortunately it never will, so we’re ok.) 不存在最小的目标。最大难度可以粗暴地认为是：maximum_target / 1（因为0会产生无限值），这个数很大，大约是2的224次方。 网络难度是否可以降低可以，参考上面的内容。 最小难度是什么 The minimum difficulty, when the target is at the maximum allowed value, is 1. 当targe是最大允许值的时候，最小难度，也就是1。这里说的target应该是分母，分子的最大值是确定的，上文已经说过了。 在给定难度的情况下，网络hash率是如何得出的根据10分钟一块的平均速度，产生2016个块应该需要两周。每产生2016个块，会调整一下难度，根据之前产生这2016个块花费的时间，和理论上应该花费的时间–两周做一下对比。 难度为1时的目标值（上文提到的）： 10xffff * 2**208 难度为D时的目标值应该是： 1(0xffff * 2**208) / D 所需要的hash计算数（2**256是最大hash计算数，除以当前的目标值，也是一个256位的数，例如上面的0x00000000000404CB000000000000000000000000000000000000000000000000 ，这个地方不是太理解，这样除，就可以算出总共需要的计算数？翻了一些帖子，还是没有找到答案）1D * 2**256 / (0xffff * 2**208) 简化为： 1D * 2**48 / 0xffff 以上是10分钟的hash数，hash率是以秒为单位的，所以：1D * 2**48 / 0xffff / 600 最后简化为： 1D * 2**32 / 600 如果难度为1的话，每秒钟7 Mhashes。原文写作时，难度是 22012.4941572，那么过去2016个块的hash率是： 122012.4941572 * 2**32 / 600 = around 157 Ghashes per second. 参考https://en.bitcoin.it/wiki/Difficulty。]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>比特币</tag>
        <tag>难度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-branch的用法总结]]></title>
    <url>%2Fgit%2Fgit-branch.html</url>
    <content type="text"><![CDATA[概要Git：git-branch的用法总结 博客原帖收藏于IT老兵博客 前言git-branch的用法。这个命令使用频度很高，还有一些没有搞明白，在这里总结梳理一下。 PS：之前的文章题目命名都用空格，以前一直不理解git的官网为什么多加一个“-”，现在明白了，为了用作文章名和题目比较方便，解了一个惑。 感觉之前的内容总结得还不是很到位，每次在这个地方遇到问题时，还是应该进来再看看，再看看官网，参考参考别的帖子，看看哪里没有整理总结明白，再补充补充。 正文用法12345678910111213git branch [--color[=&lt;when&gt;] | --no-color] [-r | -a] [--list] [-v [--abbrev=&lt;length&gt; | --no-abbrev]] [--column[=&lt;options&gt;] | --no-column] [--sort=&lt;key&gt;] [(--merged | --no-merged) [&lt;commit&gt;]] [--contains [&lt;commit]] [--no-contains [&lt;commit&gt;]] [--points-at &lt;object&gt;] [--format=&lt;format&gt;] [&lt;pattern&gt;…​] // 列出分支（这个用法有点复杂） git branch [--track | --no-track] [-l] [-f] &lt;branchname&gt; [&lt;start-point&gt;] // 设置分支本地和远程的关系（上流）git branch (--set-upstream-to=&lt;upstream&gt; | -u &lt;upstream&gt;) [&lt;branchname&gt;] // 设置分支上流git branch --unset-upstream [&lt;branchname&gt;] // 取消分支上流的设置git branch (-m | -M) [&lt;oldbranch&gt;] &lt;newbranch&gt; // 重命名分支git branch (-c | -C) [&lt;oldbranch&gt;] &lt;newbranch&gt; // 拷贝分支git branch (-d | -D) [-r] &lt;branchname&gt;…​ // 删除分支git branch --edit-description [&lt;branchname&gt;] //修改分支描述 git branch有以上这么多种用法，原本我看了几遍，也感觉云山雾绕，所以在上面加了一些备注。 常用实例实例： 展示分支最简单的形式：12$ git branch* master 较为详细的形式：12$ git branch -v* master cac453c 从版本库中移除项目配置文件和日志配置文件 可以看到，加了-v，显示出了提交号和提交的信息。 更为详细的形式：12$ git branch -vv* master cac453c [origin/master] 从版本库中移除项目配置文件和日志配置文件 可以看到，加两个v，除了显示出了提交号，还显示出了上流分支（upstream）的名称。。 实例： 查看所有分支（包括远程的） 12345678910111213$ git branch -a* master remotes/origin/HEAD -&gt; origin/master remotes/origin/dev remotes/origin/f_1123 remotes/origin/f_1268 remotes/origin/f_1316 remotes/origin/f_1317 remotes/origin/f_1346 remotes/origin/f_1347 remotes/origin/f_1490 remotes/origin/f_english remotes/origin/master -a就是-all的意思，显示所有。 实例： 查看远程分支 123456789101112$ git branch -r origin/HEAD -&gt; origin/master origin/dev origin/f_1123 origin/f_1268 origin/f_1316 origin/f_1317 origin/f_1346 origin/f_1347 origin/f_1490 origin/f_english origin/master 比上面那个指令少了一项master。-r的意思是remote，显示远程的分支情况。 实例： 修改分支名把master分支名称修改成dev，这里仅仅是举一个例子，正常工作中一般是不应该发生这样的修改，这里只是修改了本地的分支名。1git branch -m master dev 实例： 删除分支删除dev分支，-d是删除，-D是强制删除，这里只是删除本地的分支，并不是删除远程的分支。12git branch -d devgit branch -D dev 删除远程分支的命令如下，需要push到远程，这里涉及到了git-push这个指令：12git push &lt;remote_name&gt; --delete &lt;branch_name&gt;$ git push &lt;remote_name&gt; :&lt;branch_name&gt; 实例： 仅显示分支列表，区别于-v参数1234chaiyudeMacBook-Pro:wzu_sports_android_full chaiyu$ git branch --list hotfix master* t_1546 参考https://git-scm.com/docs/git-branch。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git branch</tag>
        <tag>用法</tag>
        <tag>分支</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java：“目标服务器没有返回一个X-Frame-Options头”的解决方案]]></title>
    <url>%2Fjava%2Fjava-springmvc-X-Frame-Options.html</url>
    <content type="text"><![CDATA[原帖位于IT老兵博客，沉淀着一个IT老兵对于这个行业的认知。 Java：“目标服务器没有返回一个X-Frame-Options头”的解决方案。 前言在涉及网站安全时遇到一个问题（360网站安全测试也会报告），“目标服务器没有返回一个X-Frame-Options头”，找了网上的帖子，说的都不是太清楚，所以研究总结一下，方便后人。 正文问题描述以下摘录一下对于安全网站这个问题的描述和建议解决方案： 概要目标服务器没有返回一个X-Frame-Options头。攻击者可以使用一个透明的、不可见的iframe，覆盖在目标网页上，然后诱使用户在该网页上进行操作，此时用户将在不知情的情况下点击透明的iframe页面。通过调整iframe页面的位置，可以诱使用户恰好点击iframe页面的一些功能性按钮上，导致被劫持。解决方案修改web服务器配置，添加X-frame-options响应头。赋值有如下三种：（1）DENY：不能被嵌入到任何iframe或frame中。（2）SAMEORIGIN：页面只能被本站页面嵌入到iframe或者frame中。（3）ALLOW-FROM uri：只能被嵌入到指定域名的框架中。也可在代码中加入，在PHP中加入：header(‘X-Frame-Options: deny’); 具体实例但是我们的环境是Java的Springmvc，这个应该怎么解决呢？其实Spring框架中的security本身有对这个问题的解决方案，但是这是之前的Spring框架中的（SSH那会的），现在用了SpringMVC了，应该怎么去解决这个问题呢？ 参考这里，这里介绍说配置项目的web.xml文件如下，即可解决问题： 1234567891011121314&lt;filter&gt; &lt;filter-name&gt;httpHeaderSecurity&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.catalina.filters.HttpHeaderSecurityFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;antiClickJackingOption&lt;/param-name&gt; &lt;param-value&gt;SAMEORIGIN&lt;/param-value&gt; &lt;/init-param&gt; &lt;async-supported&gt;true&lt;/async-supported&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;httpHeaderSecurity&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 但是我这里又引出一个新的问题： cvc-complex-type.2.4.a: Invalid content was found starting with element ‘async-supported’. One of ‘{“http://java.sun.com/xml/ns/javaee&quot;:run-as, “http://java.sun.com/xml/ns/javaee&quot;:security-role-ref}&#39; is expected. 意思是说async-supported这个元素不被识别。继续探索，找到这里说的： xmlns中再加两行：http://www.springmodules.org/schema/cache/springmodules-cache.xsdhttp://www.springmodules.org/schema/cache/springmodules-ehcache.xsd 要在web.xml顶部的xmlns里面再加两行，问题才真正得到了解决。 分析问题是解决了，但是问题产生的原因和解决的方法又是什么呢？ ###首先，这样设置web.xml的目的是什么？ 找到Tomcat官网的讲解： org.apache.catalina.filters.HttpHeaderSecurityFilter .//过滤器的类名……antiClickJackingOption //参数配置，可以设置成DENY（拒绝），SAMEORIGIN（同源），ALLOW-FROM（允许从哪里来的）What value should be used for the anticlick-jacking header? Must be one of DENY, SAMEORIGIN, ALLOW-FROM (case-insensitive). If not specified, the default value of DENY will be used. 意思是说HttpHeaderSecurityFilter 这个过滤器是用来做anticlick-jacking（防止点击劫持，Java做Web服务的优越性就在这里，很多功能都已经做成了工具类，只需要配置一下即可）。三个配置选项，我们上文中配置成了SAMEORIGIN（同源），安全性就大大提高了。那么，上面配置这个过滤器就搞明白了，那么后面出现的那个问题又是怎么回事呢？ 其次，web.xml的这个设置错误又是怎么回事？这里要研究一下这段语句的意思，在web.xml的头部，可能很多人总会看到它，但不会去思考它：1234567&lt;web-app version=&quot;3.0&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee http://www.springmodules.org/schema/cache/springmodules-cache.xsd http://www.springmodules.org/schema/cache/springmodules-ehcache.xsd&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot;&gt; xmlns：xml的namespace，这个是为了解决多个开发者对于xml的命名会产生冲突的问题。xmlns:xsi：定义了xml的标准前缀。xsi:schemaLocation：xml的schema定义的位置。 简言之，Java对于xml的名值设置了一套定义规则，发布在上面的地方，我们上面使用的这个元素名async-supported ，在之前的web.xml中所定义的位置是没有找到的，加了那两行的命名空间的定义，才可以找到这个元素定义的位置。 至此，问题基本搞明白了。 参考https://www.w3schools.com/xml/schema_intro.asp。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>tomcat</tag>
        <tag>linux</tag>
        <tag>X-Frame-Options</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git diff发现windows下会出现"^M"符号]]></title>
    <url>%2Fgit%2Fgit-diff-m-symbol.html</url>
    <content type="text"><![CDATA[原帖收藏于IT老兵驿站，传递一个IT老兵凋零前的光和氧。 Git：git diff发现windows下会出现”^M”符号。 前言在不同操作系统上编译Git仓库的文件，经常在git diff 时发现很多文件的变化是尾部多了一个^M 的符号。这给工作带来很多困扰，研究一下这个问题。 正题翻到这个帖子： GitHub suggests that you should make sure to only use \n as a newline character in git-handled repos. There’s an option to auto-convert: 1$ git config --global core.autocrlf true 大体翻译：GitHub建议你应该只用\n 来做为新行的开始，用上面那样的设置就可以做到自动的转换，这样也就解决了问题，Git不会再报告差异。 那这是为什么呢？ 阅读一下这里所介绍的这个帖子。 If you’re using Git to collaborate with others on GitHub, ensure that Git is properly configured to handle line endings. Every time you press return on your keyboard you’re actually inserting an invisible character called a line ending. Historically, different operating systems have handled line endings differently. When you view changes in a file, Git handles line endings in its own way. Since you’re collaborating on projects with Git and GitHub, Git might produce unexpected results if, for example, you’re working on a Windows machine, and your collaborator has made a change in OS X. 这里大概是说每个操作系统有自己的换行符（就是当你按下”回车”后，系统会自动插入一些不可见的符号来表示一行的结束），Linux和Mac都是使用LF ，Windows 则是CRLF ，这样就造成了差异。 Git会对此进行一些处理，但是做什么处理呢？这里没有说清楚，只是说要用 1git config core.autocrlf 来控制，和上面说的是一样的，但是原理还是没有搞明白。 只好来看官网。 core.autocrlfSetting this variable to “true” is the same as setting the text attribute to “auto” on all files and core.eol to “crlf”. Set to true if you want to have CRLF line endings in your working directory and the repository has LF line endings. This variable can be set to input, in which case no output conversion is performed. 这个变量设置为true 等同于在所有文件上设置text attribute 为auto 并且把core.eol 设置为crlf。设成true ， 如果你的工作空间用的是CRLF 作为行结束符，同时仓库用的是LF 行结束符。这个变量也可以设置成input，这样在输出时就不做转换了。 对上面说的core.eol 又不明白了，继续查看： core.eolSets the line ending type to use in the working directory for files that have the text property set when core.autocrlf is false. Alternatives are lf, crlf and native, which uses the platform’s native line ending. The default value is native. See gitattributes[5] for more information on end-of-line conversion. 这个变量是用来设置行结束符的，在core.autocrlf 是false的时候。可以设置成lf，crlf和native ， native是说使用当前平台自己的行结束符。 到这里，大体就明白了，还留有一个问题，就是attribute的问题，留在下一次来研究。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>diff</tag>
        <tag>windows</tag>
        <tag>^M</tag>
        <tag>符号</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链：Base64编码和Base58编码]]></title>
    <url>%2Fblockchain%2Fbase64-base58.html</url>
    <content type="text"><![CDATA[区块链：Base64编码和Base58编码。 前言接触base64有很久了，其实一直没有理解base64这种编码的根本原理，或者说是设计意图，今天又读了一遍，终于开窍了，请看下图。 Base64 table 原始值 显示值 原始值 显示值 原始值 显示值 原始值 显示值 0 A 16 Q 32 g 48 w 1 B 17 R 33 h 49 x 2 C 18 S 34 i 50 y 3 D 19 T 35 j 51 z 4 E 20 U 36 k 52 0 5 F 21 V 37 l 53 1 6 G 22 W 38 m 54 2 7 H 23 X 39 n 55 3 8 I 24 Y 40 o 56 4 9 J 25 Z 41 p 57 5 10 K 26 a 42 q 58 6 11 L 27 b 43 r 59 7 12 M 28 c 44 s 60 8 13 N 29 d 45 t 61 9 14 O 30 e 46 u 62 + 15 P 31 f 47 v 63 / 什么意思呢？我这样理解，在计算机里面，所有信息都是以二进制的方式承载的，即0和1，这二进制如何去表达我们想要显示（printable）的信息呢？这个时候ASCII码就出现了，以及后来的UniCode等等，它们是以字节为单位来处理，一个字节是8位，这也是计算机系统里面的一个标准处理单位。 而Base64的目的不是这个，它是要把一段信息显示成另外的一个形式，变成不可直接读，这样尤其是在邮件传输时，可以增加一点安全性。所以，它是以6位为一个单位来处理，对应上面的这个码表，来显示的。Base58是Base64的子集，它的码表元素就更少了，少了8个（ 0 , O (大写O)，I (大写i) ，l (小写L) ， + (加号) ，/ (后倒线)），现在被用于区块链领域。 6位的处理会造成有的时候，不够一个字节，后面需要多补一个到两个字节的0来补齐长度，统一用=显示。 以下这个例子取自维基百科，一个表用MD不好展示，分成了两个表： Source text (ASCII) M a Source octets 77 (0x4d) 97 (0x61) Bit pattern 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 Source text (ASCII) M a Bit pattern 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 Index 19 22 4 (padding) Base64-encoded T W E = Encoded octets 84 (0x54) 87 (0x57) 69 (0x45) 61 (0x3D) 这样MA就变成了TWE=，这样就清楚了。]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Base48</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法及常见用例：which]]></title>
    <url>%2Flinux%2Fshell-command-which.html</url>
    <content type="text"><![CDATA[which命令用于查找并显示给定命令的绝对路径，环境变量$PATH中保存了查找命令时需要遍历的目录。which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 备注今天有点累，也已经很晚了，总结一个较为简单的命令吧，20英里法则，每天尽量坚持往前走一点。 命令功能which命令是查找某个命令的完整路径。它是用来在当前登录用户的$PATH环境变量记录的路径中查找可执行文件（即二进制文件）的路径。默认情况下，只返回第一个搜索结果。 1234WHICH(1) General Commands Manual WHICH(1)NAME which - shows the full path of (shell) commands. 命令格式which [选项] 命令 实例实例：查看ls命令的位置 123[root@iZwz90drrwkerfi7bc8mqiZ ~]# which lsalias ls=&apos;ls --color=auto&apos; /usr/bin/ls 第一行输出暂时没有搞明白是哪里来的，第二行就是ls命令的位置了。]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：修改远程仓库地址]]></title>
    <url>%2Fgit%2Fgit-remote-set-url.html</url>
    <content type="text"><![CDATA[Git：修改远程仓库地址。 前言有的时候，我们会遇到Git远程仓库IP发生改变，这样的改变可能是： 远程服务器挂了：远程服务器上的Git仓库被一个爱折腾的同事给删掉了，这个时候把他骂死也没用了，这要是SVN就没办法了，还好是Git，可以从本机的仓库去恢复这个远程仓库。 远程服务器迁移了，IP变了。这个时候就要用到这个命令了。 命令1git-remote - Manage set of tracked repositories（管理被追踪的仓库集合） 概要123git remote set-url [--push] &lt;name&gt; &lt;newurl&gt; [&lt;oldurl&gt;]git remote set-url --add [--push] &lt;name&gt; &lt;newurl&gt;git remote set-url --delete [--push] &lt;name&gt; &lt;url&gt; 修改远程仓库的url只是这个命令的一个功能。 记得很久以前（刚毕业的时候）从一本书中看到，中括号表示是可选项，尖括号表示为必选项，现在找不到了，上网查了查，可以参考这里。关于这个问题，接触过的很多同事都是糊里糊涂的，我觉得这样总是不好，搞计算机，应该严谨一些，做事情，还是应该寻根究底。 所以，要修改远程仓库，只需要进入代码目录： 12345678git remote -v# 查看当前的远程仓库git remote set-url origin https://where you want to put your repository to.git# 修改为想要设置的远程仓库git remote -v#验证一下 大功告成，亲个嘴（韦小宝语）。 参考https://git-scm.com/docs/git-remote。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>修改</tag>
        <tag>远程仓库地址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：git-revert 的用法总结]]></title>
    <url>%2Fgit%2Fgit-revert.html</url>
    <content type="text"><![CDATA[概述1git-revert - Revert some existing commits // 撤销一些已经存在的提交 博客原文收藏于IT老兵驿站。 语法1234git revert [--[no-]edit] [-n] [-m parent-number] [-s] [-S[&lt;keyid&gt;]] &lt;commit&gt;…​git revert --continuegit revert --quitgit revert --abort 理解 Given one or more existing commits, revert the changes that the related patches introduce, and record some new commits that record them. This requires your working tree to be clean (no modifications from the HEAD commit).Note: git revert is used to record some new commits to reverse the effect of some earlier commits (often only a faulty one). If you want to throw away all uncommitted changes in your working directory, you should see git-reset[1], particularly the –hard option. If you want to extract specific files as they were in another commit, you should see git-checkout[1], specifically the git checkout – syntax. Take care with these alternatives as both will discard uncommitted changes in your working directory. 这个工具的使用场景有一点复杂，所以把原本的介绍贴在这里，下面附上翻译：给定一个或多个现有提交，还原由这些提交引入的更改，并用新的提交去记录。 这需要您的工作树是干净的（没有对于HEAD的修改）。注意：git revert用于记录一些新的提交以还原某些早期提交的效果（通常是一个错误的提交）。 如果你想丢弃工作目录中所有未提交的更改，你应该看到git-reset [1]，特别是–hard选项。 如果你想在另一个提交中提取特定文件，你应该看到git-checkout [1]，特别是git checkout &lt;commit&gt; - &lt;filename&gt;语法。 请注意这些替代方案，因为它们都会丢弃工作目录中未提交的更改。 意思是，如果你想撤销之前的一个或几个提交带来的修改，那么使用这个工具；如果想放弃工作目录的修改（并没有提交），那么你应该使用git reset；或者你只是想检出一个文件的某一个版本，那么使用git checkout。 实例摘录了官网的两个例子：实例： 撤销 HEAD 指针之前的第3个提交，并且生成一个新的提交。这里补充解释一下，HEAD 是指当前的提交指针，除了 HEAD，再往前找3个，所以下面的英文里面说是第4个。 1git revert HEAD~3 Revert the changes specified by the fourth last commit in HEAD and create a new commit with the reverted changes. 实例： 撤销从 master 之前第5个提交到之前第3个提交的变化（这么看来，前面是开区间，第6个没有被包含；后面是闭区间，包含了第3个，这个语法厉害了）。 1git revert -n master~5..master~2 Revert the changes done by commits from the fifth last commit in master (included) to the third last commit in master (included), but do not create any commit with the reverted changes. The revert only modifies the working tree and the index. 实例： 撤销某个提交带来的修改1git revert &lt;commit&gt; 参考https://git-scm.com/docs/git-revert]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>用法</tag>
        <tag>git revert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链：教程 | 以太坊智能合约编程之菜鸟教程]]></title>
    <url>%2Fblockchain%2Fblock-chain-ethereum-contract-program.html</url>
    <content type="text"><![CDATA[区块链：教程 | 以太坊智能合约编程之菜鸟教程。 这篇介绍以太坊合约的文章写得很好，在查找了这么多资料，进行对比之后，感觉阅读这一篇就可以大体理解以太坊编程的原理，如果对个别的知识点还有点含糊，可以相应地去查一查，就是以这篇为主干，别的资料为辅。稍微整理了一下格式，以及修改了一些半角符号。 译注：原文首发于ConsenSys开发者博客，原作者为Eva以及ConsenSys的开发团队。如果您想要获取更多及时信息，可以访问ConsenSys首页点击左下角Newsletter订阅邮件。本文的翻译获得了ConsenSys创始人Lubin先生的授权。 有些人说以太坊太难对付，于是我们(译注：指Consensys, 下同)写了这篇文章来帮助大家学习如何利用以太坊编写智能合约和应用。这里所用到的工具，钱包，应用程序以及整个生态系统仍处于开发状态，它们将来会更好用！ 第一部分概述，讨论了关键概念，几大以太坊客户端以及写智能合约用到的编程语言。 第二部分讨论了总体的工作流程，以及目前流行的一些DApp框架和工具。 第三部分主要关于编程，我们将学习如何使用Truffle来为智能合约编写测试和构建DApp。 第一部分 概述如果你对诸如比特币以及其工作原理等密码学货币的概念完全陌生，我们建议你先看看Andreas Antonopoulos所著的Bitcoin Book的头几章，然后读一下以太坊白皮书。(译注：以太坊白皮书中文版请看 http://ethfans.org/posts/ethereum-whitepaper) 如果你觉得白皮书中的章节太晦涩，也可以直接动手来熟悉以太坊。在以太坊上做开发并不要求你理解所有那些“密码经济计算机科学”(crypto economic computer science)，而白皮书的大部分是关于以太坊想对于比特币架构上的改进。 新手教程ethereum.org提供了官方的新手入门教程，以及一个代币合约和众筹合约的教程。合约语言Solidity也有官方文档。学习智能合约的另一份不错的资料（也是我的入门资料）是dappsForBeginners，不过现在可能有些过时了。 这篇文章的目的是成为上述资料的补充，同时介绍一些基本的开发者工具，使入门以太坊，智能合约以及构建DApps(decentralized apps, 分布式应用)更加容易。我会试图按照我自己(依然是新手)的理解来解释工作流程中的每一步是在做什么，我也得到了ConsenSys酷酷的开发者们的许多帮助。 基本概念了解这些名词是一个不错的开始： 公钥加密系统。 Alice有一把公钥和一把私钥。她可以用她的私钥创建数字签名，而Bob可以用她的公钥来验证这个签名确实是用Alice的私钥创建的，也就是说，确实是Alice的签名。当你创建一个以太坊或者比特币钱包的时候，那长长的0xdf...5f地址实质上是个公钥，对应的私钥保存某处。类似于Coinbase的在线钱包可以帮你保管私钥，你也可以自己保管。如果你弄丢了存有资金的钱包的私钥，你就等于永远失去了那笔资金，因此你最好对私钥做好备份。过来人表示：通过踩坑学习到这一点是非常痛苦的… 点对点网络。 就像BitTorrent, 以太坊分布式网络中的所有节点都地位平等，没有中心服务器。(未来会有半中心化的混合型服务出现为用户和开发者提供方便，这我们后面会讲到。) 区块链。 区块链就像是一个全球唯一的帐簿，或者说是数据库，记录了网络中所有交易历史。 以太坊虚拟机(EVM)。 它让你能在以太坊上写出更强大的程序（比特币上也可以写脚本程序）。它有时也用来指以太坊区块链，负责执行智能合约以及一切。 节点。 你可以运行节点，通过它读写以太坊区块链，也即使用以太坊虚拟机。完全节点需要下载整个区块链。轻节点仍在开发中。 矿工。 挖矿，也就是处理区块链上的区块的节点。这个网页可以看到当前活跃的一部分以太坊矿工：stats.ethdev.com。 工作量证明。 矿工们总是在竞争解决一些数学问题。第一个解出答案的(算出下一个区块)将获得以太币作为奖励。然后所有节点都更新自己的区块链。所有想要算出下一个区块的矿工都有与其他节点保持同步，并且维护同一个区块链的动力，因此整个网络总是能达成共识。(注意：以太坊正计划转向没有矿工的权益证明系统(POS)，不过那不在本文讨论范围之内。) 以太币。 缩写ETH。一种你可以购买和使用的真正的数字货币。这里是可以交易以太币的其中一家交易所的走势图。在写这篇文章的时候，1个以太币价值65美分。 Gas。(汽油) 在以太坊上执行程序以及保存数据都要消耗一定量的以太币，Gas是以太币转换而成。这个机制用来保证效率。 DApp。 以太坊社区把基于智能合约的应用称为去中心化的应用程序(Decentralized App)。DApp的目标是(或者应该是)让你的智能合约有一个友好的界面，外加一些额外的东西，例如IPFS（可以存储和读取数据的去中心化网络，不是出自以太坊团队但有类似的精神)。DApp可以跑在一台能与以太坊节点交互的中心化服务器上，也可以跑在任意一个以太坊平等节点上。(花一分钟思考一下：与一般的网站不同，DApp不能跑在普通的服务器上。他们需要提交交易到区块链并且从区块链而不是中心化数据库读取重要数据。相对于典型的用户登录系统，用户有可能被表示成一个钱包地址而其它用户数据保存在本地。许多事情都会与目前的web应用有不同架构。) 如果想看看从另一个新手视角怎么理解这些概念，请读Just Enough Bitcoin for Ethereum。 以太坊客户端，智能合约语言编写和部署智能合约并不要求你运行一个以太坊节点。下面有列出基于浏览器的IDE和API。但如果是为了学习的话，还是应该运行一个以太坊节点，以便理解其中的基本组件，何况运行节点也不难。 运行以太坊节点可用的客户端以太坊有许多不同语言的客户端实现（即多种与以太坊网络交互的方法），包括C++, Go, Python, Java, Haskell等等。为什么需要这么多实现？不同的实现能满足不同的需求（例如Haskell实现的目标是可以被数学验证），能使以太坊更加安全，能丰富整个生态系统。 在写作本文时，我使用的是Go语言实现的客户端geth (go-ethereum)，其他时候还会使用一个叫testrpc的工具, 它使用了Python客户端pyethereum。后面的例子会用到这些工具。 注: 我曾经使用过C++的客户端，现在仍然在用其中的ethminer组件和geth配合挖矿，因此这些不同的组件是可以一起工作的。关于挖矿：挖矿很有趣，有点像精心照料你的室内盆栽，同时又是一种了解整个系统的方法。虽然以太币现在的价格可能连电费都补不齐，但以后谁知道呢。人们正在创造许多酷酷的DApp, 可能会让以太坊越来越流行。 交互式控制台。 客户端运行起来后，你就可以同步区块链，建立钱包，收发以太币了。使用geth的一种方式是通过Javascript控制台（JavaScript console, 类似你在chrome浏览器里面按F12出来的那个，只不过是跑在终端里）。此外还可以使用类似cURL的命令通过JSON RPC来与客户端交互。本文的目标是带大家过一边DApp开发的流程，因此这块就不多说了。但是我们应该记住这些命令行工具是调试，配置节点，以及使用钱包的利器。 在测试网络运行节点。 如果你在正式网络运行geth客户端，下载整个区块链与网络同步会需要相当时间。（你可以通过比较节点日志中打印的最后一个块号和stats.ethdev.com上列出的最新块来确定是否已经同步。) 另一个问题是在正式网络上跑智能合约需要实实在在的以太币。在测试网络上运行节点的话就没有这个问题。此时也不需要同步整个区块链，创建一个自己的私有链就勾了，对于开发来说更省时间。 testrpc。 用geth可以创建一个测试网络，另一种更快的创建测试网络的方法是使用testrpc。Testrpc可以在启动时帮你创建一堆存有资金的测试账户。它的运行速度也更快因此更适合开发和测试。你可以从testrpc起步，然后随着合约慢慢成型，转移到geth创建的测试网络上 - 启动方法很简单，只需要指定一个networkid：geth --networkid &quot;12345&quot;。这里是testrpc的代码仓库，下文我们还会再讲到它。 接下来我们来谈谈可用的编程语言，之后就可以开始真正的编程了。 写智能合约用的编程语言用Solidity就好。 要写智能合约有好几种语言可选：有点类似Javascript的Solidity, 文件扩展名是.sol和Python接近的Serpent, 文件名以.se结尾。还有类似Lisp的LLL。Serpent曾经流行过一段时间，但现在最流行而且最稳定的要算是Solidity了，因此用Solidity就好。听说你喜欢Python? 用Solidity。 solc编译器。 用Solidity写好智能合约之后，需要用solc来编译。它是一个来自C++客户端实现的组件（又一次，不同的实现产生互补），这里是安装方法。如果你不想安装solc也可以直接使用基于浏览器的编译器，例如Solidity real-time compiler或者Cosmo。后文有关编程的部分会假设你安装了solc。 注意：以太坊正处于积极的开发中，有时候新的版本之间会有不同步。确认你使用的是最新的dev版本，或者稳定版本。如果遇到问题可以去以太坊项目对应的Gitter聊天室或者forums.ethereum.org上问问其他人在用什么版本。 web3.js API。 当Solidity合约编译好并且发送到网络上之后，你可以使用以太坊的web3.js JavaScript API来调用它，构建能与之交互的web应用。 以上就是在以太坊上编写智能合约和构建与之交互的DApp所需的基本工具。 第二部分 DApp框架，工具以及工作流程DApp开发框架虽然有上文提到的工具就可以进行开发了，但是使用社区大神们创造的框架会让开发更容易。 Truffle and Embark。 是Truffle把我领进了门。在Truffle出现之前的那个夏天，我目睹了一帮有天分的学生是如何不眠不休的参加一个hackathon（编程马拉松）活动的，虽然结果相当不错，但我还是吓到了。然后Truffle出现了，帮你处理掉大量无关紧要的小事情，让你可以迅速进入写代码-编译-部署-测试-打包DApp这个流程。另外一个相似的DApp构建与测试框架是Embark。我只用过Truffle, 但是两个阵营都拥有不少DApp大神。 Meteor。 许多DApp开发者使用的另一套开发栈由web3.js和Meteor组成，Meteor是一套通用webapp开发框架（ethereum-meteor-wallet项目提供了一个很棒的入门实例，而SilentCiero正在构建大量Meteor与web3.js和DApp集成的模板）。我下载并运行过一些不错的DApp是以这种方式构造的。在11月9日至13日的以太坊开发者大会ÐΞVCON1上将有一些有趣的讨论，是关于使用这些工具构建DApp以及相关最佳实践的（会议将会在YouTube上直播）。 APIs。 BlockApps.net打算提供一套RESTful API给DApp使用以免去开发者运行本地节点的麻烦，这个中心化服务是基于以太坊Haskell实现的。这与DApp的去中心化模型背道而驰，但是在本地无法运行以太坊节点的场合非常有用，比如在你希望只有浏览器或者使用移动设备的用户也能使用你的DApp的时候。BlockApps提供了一个命令行工具bloc，注册一个开发者帐号之后就可以使用。 许多人担心需要运行以太坊节点才能使用DApp的话会把用户吓跑，其实包括BlockApps在内的许多工具都能解决这个问题。Metamask允许你在浏览器里面使用以太坊的功能而无需节点，以太坊官方提供的AlethZero或者AlethOne是正在开发中有易用界面的客户端，ConsenSys正在打造一个轻钱包LightWallet，这些工具都会让DApp的使用变得更容易。轻客户端和水平分片(sharding)也在计划和开发之中。这是一个能进化出混合架构的P2P生态系统。 智能合约集成开发环境 (IDE)IDE。 以太坊官方出品了用来编写智能合约的Mix IDE，我还没用过但会尽快一试。 基于浏览器的IDE。 Solidity real-time compiler和Cosmo都可以让你快速开始在浏览器中编写智能合约。你甚至可以让这些工具使用你的本地节点，只要让本地节点开一个端口（注意安全！这些工具站点必须可信，而且千万不要把你的全部身家放在这样一个本地节点里面！Cosmo UI上有如何使用geth做到这一点的指引）。在你的智能合约调试通过之后，可以用开发框架来给它添加用户界面和打包成DApp，这正是Truffle的工作，后面的编程章节会有详细讲解。 Ether.Camp正在开发另一个强大的企业级浏览器IDE。他们的IDE将支持沙盒测试网络，自动生成用于测试的用户界面（取代后文将展示的手动编写测试），以及一个测试交易浏览器test.ether.camp。当你的合约准备正式上线之前，使用他们的测试网络会是确保你的智能合约在一个接近真实的环境工作正常的好方法。他们也为正式网络提供了一个交易浏览器frontier.ether.camp，上面可以看到每一笔交易的细节。在本文写作时Ether.Camp的IDE还只能通过邀请注册，预计很快会正式发布。 合约和Dapp示例。 在Github上搜索DApp仓库和.sol文件可以看到进行中的有趣东西。这里有一个DApp大列表：dapps.ethercasts.com，不过其中一些项目已经过时。Ether.fund/contracts上有一些Solidity和Serpent写的合约示例，但是不清楚这些例子有没有经过测试或者正确性验证。11月12日的开发者大会ÐΞVCON1将会有一整天的DApp主题演讲。 部署智能合约的流程流程如下： 启动一个以太坊节点 (例如geth或者testrpc)。 使用solc_编译_智能合约。 =&gt; 获得二进制代码。 将编译好的合约部署到网络。（这一步会消耗以太币，还需要使用你的节点的默认地址或者指定地址来给合约签名。） =&gt; 获得合约的区块链地址和ABI（合约接口的JSON表示，包括变量，事件和可以调用的方法）。(译注：作者在这里把ABI与合约接口弄混了。ABI是合约接口的二进制表示。) 用web3.js提供的JavaScript API来调用合约。（根据调用的类型有可能会消耗以太币。） 下图详细描绘了这个流程： 你的DApp可以给用户提供一个界面先部署所需合约再使用之（如图1到4步），也可以假设合约已经部署了（常见方法），直接从使用合约（如图第6步）的界面开始。 第三部分 编程在Truffle中进行测试Truffle用来做智能合约的测试驱动开发(TDD)非常棒，我强烈推荐你在学习中使用它。它也是学习使用JavaScript Promise的一个好途径，例如deferred和异步调用。Promise机制有点像是说“做这件事，如果结果是这样，做甲，如果结果是那样，做乙… 与此同时不要在那儿干等着结果返回，行不？”。Truffle使用了包装web3.js的一个JS Promise框架Pudding（因此它为为你安装web3.js）。(译注：Promise是流行于JavaScript社区中的一种异步调用模式。它很好的封装了异步调用，使其能够灵活组合，而不会陷入callback hell.) Transaction times。 Promise对于DApp非常有用，因为交易写入以太坊区块链需要大约12-15秒的时间。即使在测试网络上看起来没有那么慢，在正式网络上却可能会要更长的时间（例如你的交易可能用光了Gas，或者被写入了一个孤儿块）。 下面让我们给一个简单的智能合约写测试用例吧。 使用Truffle首先确保你 1.安装好了solc以及 2.testrpc。（testrpc需要Python和pip。如果你是Python新手，你可能需要用virtualenv来安装，这可以将Python程序库安装在一个独立的环境中。） 接下来安装 3.Truffle（你可以使用NodeJS’s npm来安装：npm install -g truffle, -g开关可能会需要sudo）。安装好之后，在命令行中输入truffle list来验证安装成功。然后创建一个新的项目目录（我把它命名为’conference’），进入这个目录，运行truffle init。该命令会建立如下的目录结构： 现在让我们在另一个终端里通过执行testrpc来启动一个节点（你也可以用geth）： 回到之前的终端中，输入truffle deploy。这条命令会部署之前truffle init产生的模板合约到网络上。任何你可能遇到的错误信息都会在testrpc的终端或者执行truffle的终端中输出。 在开发过程中你随时可以使用truffle compile命令来确认你的合约可以正常编译（或者使用solc YourContract.sol），truffle deploy来编译和部署合约，最后是truffle test来运行智能合约的测试用例。 第一个合约下面是一个针对会议的智能合约，通过它参会者可以买票，组织者可以设置参会人数上限，以及退款策略。本文涉及的所有代码都可以在这个代码仓库找到。 contract Conference { address public organizer; mapping (address =&gt; uint) public registrantsPaid; uint public numRegistrants; uint public quota; event Deposit(address _from, uint _amount); // so you can log these events event Refund(address _to, uint _amount); function Conference() { // Constructor organizer = msg.sender; quota = 500; numRegistrants = 0; } function buyTicket() public returns (bool success) { if (numRegistrants &gt;= quota) { return false; } registrantsPaid[msg.sender] = msg.value; numRegistrants++; Deposit(msg.sender, msg.value); return true; } function changeQuota(uint newquota) public { if (msg.sender != organizer) { return; } quota = newquota; } function refundTicket(address recipient, uint amount) public { if (msg.sender != organizer) { return; } if (registrantsPaid[recipient] == amount) { address myAddress = this; if (myAddress.balance &gt;= amount) { recipient.send(amount); registrantsPaid[recipient] = 0; numRegistrants--; Refund(recipient, amount); } } } function destroy() { // so funds not locked in contract forever if (msg.sender == organizer) { suicide(organizer); // send funds to organizer } } } 接下来让我们部署这个合约。（注意：本文写作时我使用的是Mac OS X 10.10.5, solc 0.1.3+ (通过brew安装)，Truffle v0.2.3, testrpc v0.1.18 (使用venv)） 部署合约 (译注：图中步骤翻译如下：） 使用truffle部署智能合约的步骤：1. truffle init (在新目录中) =&gt; 创建truffle项目目录结构2. 编写合约代码，保存到contracts/YourContractName.sol文件。3. 把合约名字加到config/app.json的’contracts’部分。4. 启动以太坊节点（例如在另一个终端里面运行testrpc）。5. truffle deploy（在truffle项目目录中) 添加一个智能合约。 在truffle init执行后或是一个现有的项目目录中，复制粘帖上面的会议合约到contracts/Conference.sol文件中。然后打开config/app.json文件，把’Conference’加入’deploy’数组中。 启动testrpc。 在另一个终端中启动testrpc。 编译或部署。 执行truffle compile看一下合约是否能成功编译，或者直接truffle deploy一步完成编译和部署。这条命令会把部署好的合约的地址和ABI（应用接口）加入到配置文件中，这样之后的truffle test和truffle build步骤可以使用这些信息。 出错了？ 编译是否成功了？记住，错误信息即可能出现在testrpc终端也可能出现在truffle终端。 重启节点后记得重新部署！ 如果你停止了testrpc节点，下一次使用任何合约之前切记使用truffle deploy重新部署。testrpc在每一次重启之后都会回到完全空白的状态。 合约代码解读让我们从智能合约头部的变量声明开始： address public organizer; mapping (address =&gt; uint) public registrantsPaid; uint public numRegistrants; uint public quota; address。 地址类型。第一个变量是会议组织者的钱包地址。这个地址会在合约的构造函数function Conference()中被赋值。很多时候也称呼这种地址为’owner’（所有人）。 uint。 无符号整型。区块链上的存储空间很紧张，保持数据尽可能的小。 public。 这个关键字表明变量可以被合约之外的对象使用。private修饰符则表示变量只能被本合约(或者衍生合约)内的对象使用。如果你想要在测试中通过web3.js使用合约中的某个变量，记得把它声明为public。 Mapping或数组。（译注：Mapping类似Hash, Directory等数据类型，不做翻译。）在Solidity加入数组类型之前，大家都使用类似mapping (address =&gt; uint)的Mapping类型。这个声明也可以写作address registrantsPaid[]，不过Mapping的存储占用更小(smaller footprint)。这个Mapping变量会用来保存参加者（用他们的钱包地址表示）的付款数量以便在退款时使用。 关于地址。 你的客户端（比如testrpc或者geth）可以生成一个或多个账户/地址。testrpc启动时会显示10个可用地址： 第一个地址, accounts[0]，是发起调用的默认地址，如果没有特别指定的话。 组织者地址 vs 合约地址。 部署好的合约会在区块链上拥有自己的地址（与组织者拥有的是不同的地址）。在Solidity合约中可以使用this来访问这个合约地址，正如refundTicket函数所展示的：address myAddress = this; Suicide, Solidity的好东西。（译注：suicide意为’自杀’, 为Solidity提供的关键字，不做翻译。）转给合约的资金会保存于合约（地址）中。最终这些资金通过destroy函数被释放给了构造函数中设置的组织者地址。这是通过suicide(orgnizer);这行代码实现的。没有这个，资金可能被永远锁定在合约之中（reddit上有些人就遇到过），因此如果你的合约会接受资金一定要记得在合约中使用这个方法！ 如果想要模拟另一个用户或者对手方（例如你是卖家想要模拟一个买家），你可以使用可用地址数组中另外的地址。假设你要以另一个用户，accounts[1], 的身份来买票，可以通过from参数设置： conference.buyTicket({ from: accounts[1], value: some_ticket_price_integer }); 函数调用可以是交易。 改变合约状态（修改变量值，添加记录，等等）的函数调用本身也是转账交易，隐式的包含了发送人和交易价值。因此web3.js的函数调用可以通过指定{ from: __, value: __ }参数来发送以太币。在Solidity合约中，你可以通过msg.sender和msg.value来获取这些信息： function buyTicket() public { ... registrantsPaid[msg.sender] = msg.value; ... } 事件(Event)。 可选的功能。合约中的Deposit（充值）和Send（发送）事件是会被记录在以太坊虚拟机日志中的数据。它们实际上没有任何作用，但是用事件(Event)把交易记录进日志是好的做法。 好了，现在让我们给这个智能合约写一个测试，来确保它能工作。 写测试把项目目录test/中的example.js文件重命名为conference.js，文件中所有的’Example’替换为’Conference’。 contract(&apos;Conference&apos;, function(accounts) { it(&quot;should assert true&quot;, function(done) { var conference = Conference.at(Conference.deployed_address); assert.isTrue(true); done(); // stops tests at this point }); }); 在项目根目录下运行truffle test，你应该看到测试通过。在上面的测试中truffle通过Conference.deployed_address获得合约部署在区块链上的地址。 让我们写一个测试来初始化一个新的Conference，然后检查变量都正确赋值了。将conference.js中的测试代码替换为： contract(&apos;Conference&apos;, function(accounts) { it(&quot;Initial conference settings should match&quot;, function(done) { var conference = Conference.at(Conference.deployed_address); // same as previous example up to here Conference.new({ from: accounts[0] }) .then(function(conference) { conference.quota.call().then( function(quota) { assert.equal(quota, 500, &quot;Quota doesn&apos;t match!&quot;); }).then( function() { return conference.numRegistrants.call(); }).then( function(num) { assert.equal(num, 0, &quot;Registrants should be zero!&quot;); return conference.organizer.call(); }).then( function(organizer) { assert.equal(organizer, accounts[0], &quot;Owner doesn&apos;t match!&quot;); done(); // to stop these tests earlier, move this up }).catch(done); }).catch(done); }); }); 构造函数。 Conference.new({ from: accounts[0] })通过调用合约构造函数创造了一个新的Conference实例。由于不指定from时会默认使用accounts[0]，它其实可以被省略掉： Conference.new({ from: accounts[0] }); // 和Conference.new()效果相同 Promise。 代码中的那些then和return就是Promise。它们的作用写成一个深深的嵌套调用链的话会是这样： conference.numRegistrants.call().then( function(num) { assert.equal(num, 0, &quot;Registrants should be zero!&quot;); conference.organizer.call().then( function(organizer) { assert.equal(organizer, accounts[0], &quot;Owner doesn&apos;t match!&quot;); }).then( function(...)) }).then( function(...)) // Because this would get hairy... Promise减少嵌套，使代码变得扁平，允许调用异步返回，并且简化了表达“成功时做这个”和“失败时做那个”的语法。Web3.js通过回调函数实现异步调用，因此你不需要等到交易完成就可以继续执行前端代码。Truffle借助了用Promise封装web3.js的一个框架，叫做Pudding，这个框架本身又是基于Bluebird的，它支持Promise的高级特性。 call。 我们使用call来检查变量的值，例如conference.quota.call().then(...，还可以通过传参数，例如call(0), 来获取mapping在index 0处的元素。Solidity的文档说这是一种特殊的“消息调用”因为 1.不会为矿工记录和 2.不需要从钱包账户/地址发起（因此它没有被账户持有者私钥做签名）。另一方面，交易/事务(Transaction)会被矿工记录，必须来自于一个账户（也就是有签名），会被记录到区块链上。对合约中数据做的任何修改都是交易。仅仅是检查一个变量的值则不是。因此在读取变量时不要忘记加上call()！否则会发生奇怪的事情。（此外如果在读取变量是遇到问题别忘记检查它是否是public。）call()也能用于调用不是交易的函数。如果一个函数本来是交易，但你却用call()来调用，则不会在区块链上产生交易。 断言。 标准JS测试中的断言（如果你不小心拼成了复数形式’asserts’，truffle会报错，让你一头雾水），assert.equal是最常用的，其他类型的断言可以在Chai的文档中找到。 再一次运行truffle test确保一切工作正常。 测试合约函数调用现在我们测试一下改变quote变量的函数能工作。在tests/conference.js文件的contract(&#39;Conference&#39;, function(accounts) {...};)的函数体中添加如下测试用例： it(&quot;Should update quota&quot;, function(done) { var c = Conference.at(Conference.deployed_address); Conference.new({from: accounts[0] }).then( function(conference) { conference.quota.call().then( function(quota) { assert.equal(quota, 500, &quot;Quota doesn&apos;t match!&quot;); }).then( function() { return conference.changeQuota(300); }).then( function(result) { // result here is a transaction hash console.log(result); // if you were to print this out it’d be long hex - the transaction hash return conference.quota.call() }).then( function(quota) { assert.equal(quota, 300, &quot;New quota is not correct!&quot;); done(); }).catch(done); }).catch(done); }); 这里的新东西是调用changeQuota函数的那一行。console.log对于调试很有用，用它能在运行truffle的终端中输出信息。在关键点插入console.log可以查看执行到了哪一步。记得把Solidity合约中changeQuota函数被声明为public，否则你不能调用它： function changeQuota(uint newquota) public { } 测试交易现在让我们调用一个需要发起人发送资金的函数。 Wei。 以太币有很多种单位（这里有个很有用的转换器）,在合约中通常用的是Wei，最小的单位。Web3.js提供了在各单位与Wei之间互相转换的便利方法，形如web3.toWei(.05, &#39;ether&#39;)。JavaScript在处理很大的数字时有问题，因此web3.js使用了程序库BigNumber，并建议在代码各处都以Wei做单位，直到要给用户看的时候（文档。 账户余额。 Web3.js提供了许多提供方便的方法，其中另一个会在下面测试用到的是web3.eth.getBalance(some_address)。记住发送给合约的资金会由合约自己持有直到调用suicide。 在contract(Conference, function(accounts) {...};)的函数体中插入下面的测试用例。在高亮显示的方法中，测试用例让另一个用户(accounts[1])以ticketPrice的价格买了一张门票。然后它检查合约的账户余额增加了ticketPrice，以及购票用户被加入了参会者列表。 这个测试中的buyTicket是一个交易函数： it(&quot;Should let you buy a ticket&quot;, function(done) { var c = Conference.at(Conference.deployed_address); Conference.new({ from: accounts[0] }).then( function(conference) { var ticketPrice = web3.toWei(.05, &apos;ether&apos;); var initialBalance = web3.eth.getBalance(conference.address).toNumber(); conference.buyTicket({ from: accounts[1], value: ticketPrice }).then( function() { var newBalance = web3.eth.getBalance(conference.address).toNumber(); var difference = newBalance - initialBalance; assert.equal(difference, ticketPrice, &quot;Difference should be what was sent&quot;); return conference.numRegistrants.call(); }).then(function(num) { assert.equal(num, 1, &quot;there should be 1 registrant&quot;); return conference.registrantsPaid.call(accounts[1]); }).then(function(amount) { assert.equal(amount.toNumber(), ticketPrice, &quot;Sender&apos;s paid but is not listed&quot;); done(); }).catch(done); }).catch(done); }); 交易需要签名。 和之前的函数调用不同，这个调用是一个会发送资金的交易，在这种情况下购票用户(accounts[1])会用他的私钥对buyTicket()调用做签名。（在geth中用户需要在发送资金之前通过输入密码来批准这个交易或是解锁钱包的账户。） toNumber()。 有时我们需要把Solidity返回的十六进制结果转码。如果结果可能是个很大的数字可以用web3.toBigNumber(numberOrHexString)来处理因为JavaScript直接对付大数要糟。 测试包含转账的合约最后，为了完整性，我们确认一下refundTicket方法能正常工作，而且只有会议组织者能调用。下面是测试用例： it(&quot;Should issue a refund by owner only&quot;, function(done) { var c = Conference.at(Conference.deployed_address); Conference.new({ from: accounts[0] }).then( function(conference) { var ticketPrice = web3.toWei(.05, &apos;ether&apos;); var initialBalance = web3.eth.getBalance(conference.address).toNumber(); conference.buyTicket({ from: accounts[1], value: ticketPrice }).then( function() { var newBalance = web3.eth.getBalance(conference.address).toNumber(); var difference = newBalance - initialBalance; assert.equal(difference, ticketPrice, &quot;Difference should be what was sent&quot;); // same as before up to here // Now try to issue refund as second user - should fail return conference.refundTicket(accounts[1], ticketPrice, {from: accounts[1]}); }).then( function() { var balance = web3.eth.getBalance(conference.address).toNumber(); assert.equal(web3.toBigNumber(balance), ticketPrice, &quot;Balance should be unchanged&quot;); // Now try to issue refund as organizer/owner - should work return conference.refundTicket(accounts[1], ticketPrice, {from: accounts[0]}); }).then( function() { var postRefundBalance = web3.eth.getBalance(conference.address).toNumber(); assert.equal(postRefundBalance, initialBalance, &quot;Balance should be initial balance&quot;); done(); }).catch(done); }).catch(done); }); 这个测试用例覆盖的Solidity函数如下： function refundTicket(address recipient, uint amount) public returns (bool success) { if (msg.sender != organizer) { return false; } if (registrantsPaid[recipient] == amount) { address myAddress = this; if (myAddress.balance &gt;= amount) { recipient.send(amount); Refund(recipient, amount); registrantsPaid[recipient] = 0; numRegistrants--; return true; } } return false; } 合约中发送以太币。 address myAddress = this展示了如何获取该会议合约实例的地址，以变接下来检查这个地址的余额（或者直接使用this.balance）。合约通过recipient.send(amount)方法把资金发回了购票人。 交易无法返回结果给web3.js。 注意这一点！refundTicket函数会返回一个布尔值，但是这在测试中无法检查。因为这个方法是一个交易函数（会改变合约内数据或是发送以太币的调用），而web3.js得到的交易运行结果是一个交易哈希（如果打印出来是一个长长的十六进制/怪怪的字符串）。既然如此为什么还要让refundTicket返回一个值？因为在Solidity合约内可以读到这个返回值，例如当另一个合约调用refundTicket()的时候。也就是说Solidity合约可以读取交易运行的返回值，而web3.js不行。另一方面，在web3.js中你可以用事件机制（Event, 下文会解释）来监控交易运行，而合约不行。合约也无法通过call()来检查交易是否修改了合约内变量的值。 关于sendTransaction()。 当你通过web3.js调用类似buyTicket()或者refundTicket()的交易函数时（使用web3.eth.sendTransaction），交易并不会立即执行。事实上交易会被提交到矿工网络中，交易代码直到其中一位矿工产生一个新区块把交易记录进区块链之后才执行。因此你必须等交易进入区块链并且同步回本地节点之后才能验证交易执行的结果。用testrpc的时候可能看上去是实时的，因为测试环境很快，但是正式网络会比较慢。 事件/Event。 在web3.js中你应该监听事件而不是返回值。我们的智能合约示例定义了这些事件： event Deposit(address _from, uint _amount); event Refund(address _to, uint _amount); 它们在buyTicket()和refundTicket()中被触发。触发时你可以在testrpc的输出中看到日志。要监听事件，你可以使用web.js监听器(listener)。在写本文时我还不能在truffle测试中记录事件，但是在应用中没问题： Conference.new({ from: accounts[0] }).then( function(conference) { var event = conference.allEvents().watch({}, &apos;&apos;); // or use conference.Deposit() or .Refund() event.watch(function (error, result) { if (error) { console.log(&quot;Error: &quot; + error); } else { console.log(&quot;Event: &quot; + result.event); } }); // ... 过滤器/Filter。 监听所有事件可能会产生大量的轮询，作为替代可以使用过滤器。它们可以更灵活的开始或是停止对事件的监听。更多过滤器的信息可查看Solidity文档。 总的来说，使用事件和过滤器的组合比检查变量消耗的Gas更少，因而在验证正式网络的交易运行结果时非常有用。 Gas。 （译注：以太坊上的燃料，因为代码的执行必须消耗Gas。直译为汽油比较突兀，故保留原文做专有名词。）直到现在我们都没有涉及Gas的概念，因为在使用testrpc时通常不需要显式的设置。当你转向geth和正式网络时会需要。在交易函数调用中可以在{from: __, value: __, gas: __}对象内设置Gas参数。Web3.js提供了web3.eth.gasPrice调用来获取当前Gas的价格，Solidity编译器也提供了一个参数让你可以从命令行获取合约的Gas开销概要：solc --gas YouContract.sol。下面是Conference.sol的结果： 为合约创建DApp界面下面的段落会假设你没有网页开发经验。 上面编写的测试用例用到的都是在前端界面中也可以用的方法。你可以把前端代码放到app/目录中，运行truffle build之后它们会和合约配置信息一起编译输出到build/目录。在开发时可以使用truffle watch命令在app/有任何变动时自动编译输出到build/目录。然后在浏览器中刷新页面即可看到build/目录中的最新内容。（truffle serve可以启动一个基于build/目录的网页服务器。） app/目录中有一些样板文件帮助你开始： index.html会加载app.js： 因此我们只需要添加代码到app.js就可以了。 默认的app.js会在浏览器的console(控制台)中输出一条”Hello from Truffle!”的日志。在项目根目录中运行truffle watch，然后在浏览器中打开build/index.html文件，再打开浏览器的console就可以看到。（大部分浏览器例如Chrome中，单击右键 -&gt; 选择Inspect Element然后切换到Console即可。） 在app.js中，添加一个在页面加载时会运行的window.onload调用。下面的代码会确认web3.js已经正常载入并显示所有可用的账户。（注意：你的testrpc节点应该保持运行。） window.onload = function() { var accounts = web3.eth.accounts; console.log(accounts); } 看看你的浏览器console中看看是否打印出了一组账户地址。 现在你可以从tests/conference.js中复制一些代码过来（去掉只和测试有关的断言），将调用返回的结果输出到console中以确认代码能工作。下面是个例子： window.onload = function() { var accounts = web3.eth.accounts; var c = Conference.at(Conference.deployed_address); Conference.new({ from: accounts[0] }).then( function(conference) { var ticketPrice = web3.toWei(.05, &apos;ether&apos;); var initialBalance = web3.eth.getBalance(conference.address).toNumber(); console.log(&quot;The conference&apos;s initial balance is: &quot; + initialBalance); conference.buyTicket({ from: accounts[1], value: ticketPrice }).then( function() { var newBalance = web3.eth.getBalance(conference.address).toNumber(); console.log(&quot;After someone bought a ticket it&apos;s: &quot; + newBalance); return conference.refundTicket(accounts[1], ticketPrice, {from: accounts[0]}); }).then( function() { var balance = web3.eth.getBalance(conference.address).toNumber(); console.log(&quot;After a refund it&apos;s: &quot; + balance); }); }); }; 上面的代码应该输出如下： (console输出的warning信息可忽略。) 现在起你就可以使用你喜欢的任何前端工具，jQuery, ReactJS, Meteor, Ember, AngularJS，等等等等，在app/目录中构建可以与以太坊智能合约互动的DApp界面了！接下来我们给出一个极其简单基于jQuery的界面作为示例。 这里是index.html的代码，这里是app.js的代码。 通过界面测试了智能合约之后我意识到最好加入检查以保证相同的用户不能注册两次。另外由于现在是运行在testrpc节点上，速度很快，最好是切换到geth节点并确认交易过程依然能及时响应。否则的话界面上就应该显示提示信息并且在处理交易时禁用相关的按钮。 尝试geth。 如果你使用geth, 可以尝试以下面的命令启动 - 在我这儿(geth v1.2.3)工作的很好： build/bin/geth --rpc --rpcaddr=&quot;0.0.0.0&quot; --rpccorsdomain=&quot;*&quot; --mine --unlock=&apos;0 1&apos; --verbosity=5 --maxpeers=0 --minerthreads=&apos;4&apos; --networkid &apos;12345&apos; --genesis test-genesis.json 这条命令解锁了两个账户, 0和1。1. 在geth控制台启动后你可能需要输入这两个账户的密码。2. 你需要在test-genesis.json文件里面的’alloc’配置中加入你的这两个账户，并且给它们充足的资金。3. 最后，在创建合约实例时加上gas参数： Conference.new({from: accounts[0], gas: 3141592}) 然后把整个truffle deploy, truffle build流程重来一遍。 教程中的代码。 在这篇基础教程中用到的所有代码都可以在这个代码仓库中找到。 自动为合约生成界面。 SilentCicero制作了一个叫做DApp Builder的工具，可以用Solidity合约自动生成HTML, jQuery和web.js的代码。这种模式也正在被越来越多的正在开发中的开发者工具采用。 教程到此结束！ 最后一章我们仅仅学习了一套工具集，主要是Truffle和testrpc. 要知道即使在ConsenSys内部，不同的开发者使用的工具和框架也不尽相同。你可能会发现更适合你的工具，这里所说的工具可能很快也会有改进。但是本文介绍的工作流程帮助我走上了DApp开发之路。 (⊙ω⊙) wonk wonk 感谢Joseph Chow的校阅和建议，Christian Lundkvist, Daniel Novy, Jim Berry, Peter Borah和Tim Coulter帮我修改文字和debug，以及Tim Coulter, Nchinda Nchinda和Mike Goldin对DApp前端步骤图提供的帮助。]]></content>
      <categories>
        <category>Blockchain</category>
        <category>以太坊</category>
        <category>智能合约</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB的基本操作：删除记录（删）]]></title>
    <url>%2Fmongodb%2Fmongodb-collection-delete.html</url>
    <content type="text"><![CDATA[MongoDB的基本操作：删除记录（删）。 方法删除记录有两个方法： 123.2版本之前db.collection.remove() // 1233.2版本之后 - db.collection.deleteMany() //删除匹配条件的多条记录 - db.collection.deleteOne() //删除匹配条件的单条记录 括号里面的参数是查询过滤器。 查询过滤器：查询过滤器用来设定查询条件。 格式&lt;field&gt;:&lt;value&gt;。 12345&#123; &lt;field1&gt;: &lt;value1&gt;, &lt;field2&gt;: &#123; &lt;operator&gt;: &lt;value&gt; &#125;, ...&#125; 实例实例：删除前文test数据库中所有记录。 1db.test.deleteMany(&#123;&#125;); {}表示没有约束条件。 实例：删除前文test数据库中_id为5abb3b5bce69c048be080199的记录。 1db.test.deleteMany(&#123;_id: ObjectId(&quot;5abb3b5bce69c048be080199&quot;)&#125;); 笔记整理到这里，发现之前的记录有点问题，因为对MongoDB的官网的结构没有完全搞清楚，所以，之前的基本操作更多偏向于参考手册的层面，可能还需要修改和调整一下。 参考：https://docs.mongodb.com/manual/tutorial/remove-documents/。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS：逻辑操作符“||”、“&&”和“!”]]></title>
    <url>%2Fjs%2Fjs-logical-operator.html</url>
    <content type="text"><![CDATA[JS：逻辑操作符“||”、“&amp;&amp;”和“!”。 Operator Usage Description Logical AND (&amp;&amp;) expr1 &amp;&amp; expr2 Returns expr1 if it can be converted to false; otherwise, returns expr2. Thus, when used with Boolean values, &amp;&amp; returns true if both operands are true; otherwise, returns false. Logical OR (&#124;&#124;) expr1 &#124;&#124; expr2 Returns expr1 if it can be converted to true; otherwise, returns expr2. Thus, when used with Boolean values, &#124;&#124; returns true if either operand is true. Logical NOT (!) !expr Returns false if its single operand can be converted to true; otherwise, returns true. 翻译一下： 操作符 用法 描述 逻辑和 (&amp;&amp;) expr1 &amp;&amp; expr2 如果expr1可以被转换为false，那么返回expr1，否则，返回expr2。 如果使用的是布尔值，那么仅当两个操作数都为真时，返回true；否则，返回false。 逻辑或 (&#124;&#124;) expr1 &#124;&#124; expr2 如果expr1可以被转换为true，返回expr1；否则，返回expr2。如果是布尔值，则两个操作数中有一个位true就返回true。 逻辑非 (!) !expr 如果这个操作数可以转换为true，返回false，否则，返回true 以下这些表达式都可以转换为false： null; NaN; 0; empty string (“” or ‘’); undefined. 这样就比较清楚了。 需要注意的是：操作符有一个优先级的规定，可以参考：https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Operator_Precedence。 参考：https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Logical_Operators。]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JS</tag>
        <tag>逻辑操作符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员晋升之路：生存意识、服务意识--IT老兵的心得]]></title>
    <url>%2Fthinking_in_programmer_life%2Ffull-stack-programmer.html</url>
    <content type="text"><![CDATA[程序员晋升之路：生存意识、服务意识–IT老兵的心得 前言 这篇文章原载于新浪博客，写于2017-06-0623:15:33，现在因为建立了自己的博客，所以迁过来，也转载在CSDN上，同时又加上“时过境迁”，又会有一些新的思考，所以修改一下，修改的地方以备注的形式展示出来，可以看出一些心态的不同来。 有一个程序员，学过前端、学过iOS，或者这么说，他喜欢研究技术，而且能把所研究的技术都搞得明明白白，但是他做项目，从来不排期，不汇报，也从来不怎么理会产品设计，结果他什么好的产品都做不出来。 这些是在厦门遇到了一个程序员所发出的感想，到了今时今日，据我了解，他还是什么都没有做出来。 做不出好的产品来，是一个好的程序员吗？ 技术都会过时的，最新的技术也不见得是最好的技术，那么技术人员的使命是什么呢？ 掌握了那么复杂的C++就算是好的程序员了吗？ 实际上，很多年了，C++程序员都缺乏用武之地了—-直到今天的区块链的火热，才又唤起市场对C++程序员的需求。 或者说，现在所鼓吹的全栈，你成为全栈了，就是好的程序员了吗？ 我觉得都不是。 你用你的能力，掌握了技术，能够很好服务于你的公司，服务于社会，这才是好的程序员。 技术是为人类服务的，脱离了服务，再尖端的技术又有什么用呢？脱离了服务，掌握了再尖端的技术的程序员，又有什么用呢？ 要使用你的技术，去提供服务，换取你的合理报酬，这就是生存意识。 掌握社会服务所需要的，或者是将要需要的技术，去提供服务，换取更好的报酬，这就是生存意识。 固守于一门很复杂的语言，为自己掌握了它而别人没有掌握而沾沾自喜，却不思考这门语言对于提供服务的价值和意义，这就已经完全走偏了，惑于技巧的层面，而忽略了根本的初衷，我们不是为了学技术而学技术的，技术也从来不是为了让你学而产生的。如果一门技术，已经不能很好地服务于社会，那怕它再难，学习起来再有挑战性，对你来说只能是满足征服的快乐，而不是满足你谋生、立业的人生目标。 放下心中自己围起来的那道技术的篱笆，不拘一格地去掌握那些需要你掌握的技术，做出好的产品来提供你的服务。 不要太在意这个技术是你新学的，也许掌握的还没有那么扎实，也许写出来的代码还没有那么漂亮，这些都会慢慢变好的，因为你写的代码，做出的产品有人在使用，这就要比那些写的很漂亮，但是没人用，只能束之高阁的代码要强太多了。代码不被使用，再漂亮，也缺乏生命力。 代码也是有生命的，这是我的感觉，所以，我们需要好好去维护她，不断去调整，让她更好地提供服务。]]></content>
      <categories>
        <category>程序人生</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git：git-checkout的用法总结（1）]]></title>
    <url>%2Fgit%2Fgit-checkout-1.html</url>
    <content type="text"><![CDATA[原帖收藏于IT老兵驿站，传递一个IT老兵在凋零前的光和氧。Git的git-checkout的用法总结。 初衷git-checkout是Git最常用的命令之一，但又是有些复杂的命令，总会感觉有些用不明白，用不明白的原因应该是没有深度地、全面地理解一下，所以要对它好好整理一下。 介绍checkout在CVS和SVN中都是检出的意思，从版本库检出一个版本，在Git中就不是这么简单了。手册上是这样介绍的： 1git-checkout - Switch branches or restore working tree files 在Git里面，checkout用于切换分支或者恢复工作树的文件。 实例问题：线上分支出现了一个问题，急需要修复（可以参看Git Flow一章）。步骤： 需要创建一个hotfix分支，参考语法：1git checkout -b|-B &lt;new_branch&gt; [&lt;start point&gt;] 实际语句：1git checkout -b hotfix-1.2.1 master 这个时候分支是本地分支，并没有提交到服务器上去，如果这个分支已经被创建，这个命令会失败，这个时候，如果想要重置这个分支，需要使用-B参数。 查看分支：git branch -av 进行修改工作 …… 问题：本地发生了一些修改，但是想放弃这些修改，回退到获取这个版本初始时的状态。参考语法：1git checkout [&lt;tree-ish&gt;] [--] &lt;pathspec&gt;…​ 实际语句：123git checkout 26a2e80 # 26a2e80 是一个commit号，这个命令会把index区域和工作区域的内容都更新git checkout -- README # README是想恢复的文件名，恢复成index区域里面的内容，为什么要加“--”呢，这个是为了告诉Git，这是一个文件而不是一个分支Git checkout . # 从index区域恢复所有文件 这个命令很灵活，既可以带一个commit号，又可以带着一个路径，tree-ish 可以理解成一个commit号，就是恢复到某一个commit号，index就是暂存区，这里要理解Git的三个区域，如果这个还不明白，那需要单开一篇文章去讲了。 以上是checkout比较常用的两个用法，逐步整理其他的用法。 参考：https://git-scm.com/docs/git-checkout。https://stackoverflow.com/questions/14460595/git-checkout-with-dot。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>用法</tag>
        <tag>checkout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法及常见用例：top]]></title>
    <url>%2Flinux%2Fshell-command-top.html</url>
    <content type="text"><![CDATA[top命令可以实时动态地查看系统的整体运行情况，是一个综合了多方信息监测系统性能和运行信息的使用工具。top命令提供了互动式的界面，用热键管理。这个命令是一个非常重要和常用的命令，但是同时也有点复杂，参数较多，怎么能够掌握好呢？用了这么多年，也都一直没有用好。老老实实读一遍手册，总体了解一下都有什么才好去做整理，有的时候可能连它都有什么功能都不知道。 命令格式top [选项] 命令功能top命令用来显示Linux的处理器活动和内核实时管理的任务。它会显示正在使用的处理器和内存以及运行进程等其他信息。 命令选项 -b：以批处理模式操作。 -c：显示完整的命令。 -d：屏幕刷新间隔时间。 -I：忽略失效过程。 -s：保密模式。 -S：累积模式。 -i&lt;时间&gt;：设置间隔时间。 -u&lt;用户名&gt;：指定用户名。 -p&lt;进程号&gt;：指定进程。 -n&lt;次数&gt;：循环显示的次数。 top交互命令在top命令执行过程中可以使用的一些交互命令。这些命令都是单字母的，如果在命令行中使用了-s选项， 其中一些命令可能会被屏蔽。 h：显示帮助画面，给出一些简短的命令总结说明。 k：终止一个进程。 i：忽略闲置和僵死进程，这是一个开关式命令。 q：退出程序。 r：重新安排一个进程的优先级别。 S：切换到累计模式。 s：改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成ms。输入0值则系统将不断刷新，默认值是5s。 f或者F：从当前显示中添加或者删除项目。 o或者O：改变显示项目的顺序。 l：切换显示平均负载和启动时间信息。 m：切换显示内存信息。 t：切换显示进程和CPU状态信息。 c：显示进程启动时的完整路径和程序名。 M：根据驻留内存大小进行排序。 P：根据CPU使用百分比大小进行排序。 T：根据时间/累计时间进行排序。 w：将当前设置写入~/.toprc文件中。 界面解释12345top - 21:52:52 up 247 days, 6:23, 2 users, load average: 0.09, 0.12, 0.13Tasks: 126 total, 1 running, 125 sleeping, 0 stopped, 0 zombie%Cpu(s): 4.0 us, 2.3 sy, 0.0 ni, 93.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 3881808 total, 153396 free, 3577588 used, 150824 buff/cacheKiB Swap: 4063228 total, 1206484 free, 2856744 used. 86344 avail Mem 统计信息区前五行是系统整体的统计信息。系统运行时间和平均负载第一行是任务队列信息，同uptime命令的执行结果，可以使用l命令切换uptime的显示。其内容如下： 21:52:52：当前时间。 up 247 days, 6:23：系统运行时间。 2 users：当前登录用户数。 load average: 0.09, 0.12, 0.13：系统负载，即任务队列平均长度。分别为1、5、15min前到现在平均值。 进程第二行为进程信息。内容如下： 126 total：进程总数[键入H可查看线程数]。 1 running：正在运行的进程。 125 sleeping：睡眠进程。 0 stopped：停止的进程。 0 zombie：僵尸进程数。 CPU状态第三行为CPU状态信息，当有多个CPU时，这些内容可能会超过两行。内容如下： us, user：运行(未调整优先级的) 用户进程的CPU百分比。 sy，system：运行内核进程的CPU百分比。 ni，niced：运行已调整优先级的用户进程的CPU百分比。 wa，IO wait：用于等待IO完成的CPU百分比。 hi：处理硬件中断的CPU百分比。 si：处理软件中断的CPU百分比。 st：这个虚拟机被hypervisor偷去的CPU百分比。（译注：如果当前处于一个hypervisor下的vm，实际上hypervisor也是要消耗一部分CPU处理时间的）。 内存使用倒数第2、3行为内存相关信息，内存显示可以用m命令切换： KiB Mem: 3881808 total, 153396 free：分别是物理内存总量、空闲内存总量。 3577588 used, 150824 buff/cache：使用物理内存总量、用作内核缓存内存量。 KiB Swap: 4063228 total, 1206484 free：分别是交换分区总量、使用交换分区剩余量。 2856744 used. 86344 avail Mem：可用来启动应用的内存（有些复杂，以后解释，恶意参考这里）。 字段/列最后一行则是进程相关的资源占用信息： PID：进程的ID，进程的唯一标识符。 USER：进程所有者的实际用户名。 PR：进程的优先级别，范围0-39，越小越优先被执行。 NI：nice值。范围-20-19，负值表示高优先级，正值表示低优先级。在top里，PR-NI=20，默认启动一个进程，nice是0。 VIRT：进程占用的虚拟内存。 RES：进程占用的物理内存。 SHR：进程使用的共享内存。 S：进程的状态。 D：表示不可终端的睡眠状态。 R：表示正在运行。 S：表示休眠。 T：表示作业控制信号下已停止。 t：表示在调试状态的停止。 Z：表示僵死状态。 %CPU：自从上一次更新到现在任务所使用的CPU使用率。 %MEM：进程使用的物理内存和总内存的百分比。 TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值，精确到百分之一秒。 COMMAND：进程启动命令名称。 交互命令实例实例：h：帮助描述：在top状态下，按h键或者?键显示交互命令的帮助菜单。输出： 123456789101112131415161718192021222324Help for Interactive Commands - procps-ng version 3.3.10Window 1:Def: Cumulative mode Off. System: Delay 3.0 secs; Secure mode Off. Z,B,E,e Global: &apos;Z&apos; colors; &apos;B&apos; bold; &apos;E&apos;/&apos;e&apos; summary/task memory scale l,t,m Toggle Summary: &apos;l&apos; load avg; &apos;t&apos; task/cpu stats; &apos;m&apos; memory info 0,1,2,3,I Toggle: &apos;0&apos; zeros; &apos;1/2/3&apos; cpus or numa node views; &apos;I&apos; Irix mode f,F,X Fields: &apos;f&apos;/&apos;F&apos; add/remove/order/sort; &apos;X&apos; increase fixed-width L,&amp;,&lt;,&gt; . Locate: &apos;L&apos;/&apos;&amp;&apos; find/again; Move sort column: &apos;&lt;&apos;/&apos;&gt;&apos; left/right R,H,V,J . Toggle: &apos;R&apos; Sort; &apos;H&apos; Threads; &apos;V&apos; Forest view; &apos;J&apos; Num justify c,i,S,j . Toggle: &apos;c&apos; Cmd name/line; &apos;i&apos; Idle; &apos;S&apos; Time; &apos;j&apos; Str justify x,y . Toggle highlights: &apos;x&apos; sort field; &apos;y&apos; running tasks z,b . Toggle: &apos;z&apos; color/mono; &apos;b&apos; bold/reverse (only if &apos;x&apos; or &apos;y&apos;) u,U,o,O . Filter by: &apos;u&apos;/&apos;U&apos; effective/any user; &apos;o&apos;/&apos;O&apos; other criteria n,#,^O . Set: &apos;n&apos;/&apos;#&apos; max tasks displayed; Show: Ctrl+&apos;O&apos; other filter(s) C,... . Toggle scroll coordinates msg for: up,down,left,right,home,end k,r Manipulate tasks: &apos;k&apos; kill; &apos;r&apos; renice d or s Set update interval W,Y Write configuration file &apos;W&apos;; Inspect other output &apos;Y&apos; q Quit ( commands shown with &apos;.&apos; require a visible task display window ) Press &apos;h&apos; or &apos;?&apos; for help with Windows,Type &apos;q&apos; or &lt;Esc&gt; to continue 实例：显示各个CPU负载描述：在top状态下，按下“1”，可以显示每个CPU的负载情况。 12345678top - 22:30:09 up 247 days, 7:00, 2 users, load average: 0.16, 0.14, 0.14Tasks: 126 total, 1 running, 125 sleeping, 0 stopped, 0 zombie%Cpu0 : 0.3 us, 0.3 sy, 0.0 ni, 99.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 : 0.3 us, 0.0 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 : 0.3 us, 0.0 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 0.3 us, 0.3 sy, 0.0 ni, 99.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 3881808 total, 141164 free, 3578540 used, 162104 buff/cacheKiB Swap: 4063228 total, 1206756 free, 2856472 used. 79768 avail Mem 实例：手动刷新描述：在top状态下，按空格或者回车进行手动刷新。top命令默认在一个特定间隔（3秒）后刷新显示。 实例：A：切换交替显示模式 描述：在top状态下，按A键，可以在全屏和交替模式间切换。在交替模式下会显示4个窗口。 Def（默认字段组） Job（任务字段组） Mem（内存字段组） Usr（用户字段组） 这四组字段共有一个独立的可配置的概括区域和它自己的可配置任务区域。4个窗口中只有一个窗口是当前窗口。当前窗口的名称显示在左上方。只有当前窗口才会接受你键盘交互命令。可以用a和w在4个窗口间切换，a移到后一个窗口，w移到前一个窗口。用g命令可以输入一个数字来选择当前窗口。 实例：B：粗体显示描述：在top状态下，按B键，会将一些重要信息会以加粗字体显示。输出： 实例：d或s：设置显示的刷新间隔描述：在top状态下，按d键或者s键，设置显示的刷新间隔为1秒。输出： 实例：f：字段管理描述：在top状态下，按f键进入字段管理界面。d键选择要显示的字段，用*标记的是已选择的。上下光标键在字段内导航，左光标键可以选择字段，右光标键进入排序状态，此时按上下光标键可以进行上下移动，回车确认。s键设置当前排序的字段，q或Esc键退出。输出： 实例：R：反向排序描述：在top状态下，按R键切换反向/常规排序。 实例：c：切换显示命令名称和完整命令行描述：在top状态下，按c键，切换是否显示进程启动时的完整路径和程序名。也可以使用如下命令行。命令：top -c输出： 实例：i：空闲任务描述：在top状态下，按i键，切换显示空闲任务。输出：不显示空闲任务： 实例：V：树视图描述：在top状态下，按V键，切换树视图。输出： 实例：z：切换彩色显示描述：在top状态下，按z键，切换彩色，即打开或关闭彩色显示。输出： 实例：Z：改变配色描述：在top状态下，按Z键，显示一个改变top命令的输出颜色的屏幕。可以为8个任务区域选择8种颜色。输出：设置修改：显示效果： 实例：按照内存使用大小排序描述：在top状态下，按shift+m，可以按照内存使用大小排序进程。输出： 实例：x、y：切换高亮信息描述：在top状态下，按x键将排序字段高亮显示（纵列）；按y键将运行进程高亮显示（横行）。输出： 实例：u：特定用户的进程描述：在top状态下，按u键将会提示输入用户名，输入首显示特定用户的进程。空白将会显示全部用户。输出： 实例：n或#：任务的数量描述：在top状态下，按n键或者#键可以设置最大显示的任务数量。输出： 实例：k：结束任务描述：在top状态下，按k键输入PID后，发送信号给任务（通常是结束任务）。输出： 实例：r：重新设置优先级描述：在top状态下，按r键输入-20~19范围中的数字后，重新设置一个任务的调度优先级（nice值）。输出： 命令行实例实例：-p：监控特定的PID描述：-p选项监控指定的PID。PID的值为0将被作为top命令自身的PID。命令：top -p 0 实例：-u或-U: 用户名或者UID描述：可以用这些选项浏览特定用户的进程。用户名或者UID可以在选项中指定。-p、-u和-U选项是互斥的，同时只可以使用这其中一个选项。试图组合使用这些选项时，会得到一个错误:命令：top -p 1248 -u root输出： 实例：-b：批处理模式描述：-b选项以批处理模式启动top命令，在文件中保存输出时是很有用的。 实例：-c：命令/程序名 触发:描述：显示进程启动时的完整路径和程序名。 实例：-d：设置延迟间隔描述：设置top的显示间隔(以秒计)。命令：top -d 1 实例：-i：切换显示空闲进程命令：top -i 实例：-n：特定重复次数后退出描述：top输出保持刷新，直到按q键或者到达指定次数。下面的命令将在10次重复之后自动退出。命令：top -n 10]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：Git merge的--ff和--no-ff]]></title>
    <url>%2Fgit%2Fgit-git-merge-ff-no-ff.html</url>
    <content type="text"><![CDATA[Git用法总结系列收藏于IT老兵驿站。Git：Git-merge的–ff和–no-ff。 前言Git merge最容易糊涂的地方就是这个--ff参数和--no-ff 参数，通过本文，把这个整理清楚。 其实官网讲的非常清楚，不过可能因为是英文的，所以大家阅读起来会有一些障碍。（PS：其实还是应该逐步逐步提高自己阅读英文文档的能力，想达到一个更高的高度，是需要客服自己本身很多的弱点的） 实例假设合并前的分支是这样，这个一个非常常见的场景，如果不明白，可以参考另外一篇文章Git Flow工作流：这是一个很常见的用例，功能开发分支是iss53，在开发新功能，master分支是线上分支，出现了问题，开辟了hotfix分支进行修复，修复完成，进行合并，需要把hotfix合并回master。 123456$ git checkout master$ git merge hotfixUpdating f42c576..3a0874cFast-forward index.html | 2 ++ 1 file changed, 2 insertions(+) 步骤如下： 切换回master分支。 将hotfix分支合并会master分支。然后看到了Fast-forward 的字样，这个词组的意思就是快进，播放电影的时候，可以注意一下，快进按钮上面就是这个词组。那么实际变成了什么样呢？仅仅是master指针指向了这个提交C4。这样是一种比较快的合并方式，轻量级，简单。这个时候，我们往往会删掉hotfix分支，因为它的历史作用已经结束，这个时候，我们的iss53这个功能又向前开发，进行了一次提交，到了C5，那么变成了这样：然后，我们要把iss53 这个分支合并回master，就变成了这样：这个时候生成了一个新的commit号，这种提交就不是fast-forward（这个时候也无法生成fast-forward提交，因为要将两个版本的内容进行合并，只有在没有需要合并内容的时候，会有这个fast-forward 方式的提交）。如果我们对第一次合并，使用了--no-ff参数，那么也会产生这样的结果，生成一个新的提交，实际上等于是对C4 进行一次复制，创建一个新的commit，这就是--no-ff的作用。 参考：https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging，这里讲了原理。参考：https://git-scm.com/docs/git-merge，这里是参考。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>git merge</tag>
        <tag>ff</tag>
        <tag>no-ff</tag>
        <tag>fast forward</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu：新增和删除用户]]></title>
    <url>%2Flinux%2Fubuntu-user-add-delete.html</url>
    <content type="text"><![CDATA[Ubuntu：新增和删除用户，修改用户组信息。参考：https://www.digitalocean.com/community/tutorials/how-to-add-and-delete-users-on-ubuntu-16-04#how-to-delete-a-user。 Linux上root用户是权力最大的用户，但是也非常危险，处于安全考虑，增加个人用户是必要的方法，下文讲了讲在Ubuntu上如何新增和删除用户。 创建用户实例： root用户新增用户chenming1234567891011121314151617root@iZhp3fz3iqsadyes2s8ay8Z:~# adduser chenmingAdding user `chenming&apos; ...Adding new group `chenming&apos; (1000) ...Adding new user `chenming&apos; (1000) with group `chenming&apos; ...Creating home directory `/home/chenming&apos; ...Copying files from `/etc/skel&apos; ...Enter new UNIX password: Retype new UNIX password: passwd: password updated successfullyChanging the user information for chenmingEnter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] y 首先创建了一个新的用户组chenming。在这个组内新建了用户chenming。要求你输入密码。要求输入一些其他信息，可以按回车略过。最后按下y对以上信息进行确认。 实例：非root用户新增用户 1$sudo adduser chenming 给用户授权实例：把chenming加到sudo组里面 12root@iZhp3fz3iqsadyes2s8ay8Z:~# groups chenmingchenming : chenming 可以看到，chenming只在chenming的组里面（前面是用户名，冒号后面是组名）。在这个组里面，可能很多命令你都不能执行。 1root@iZhp3fz3iqsadyes2s8ay8Z:~# usermod -aG sudo chenming 再来看一下： 12root@iZhp3fz3iqsadyes2s8ay8Z:~# groups chenmingchenming : chenming sudo look，进入了sudo组了，这下你可以臭屁了。 还有一种方法可以加入sodu组。如果是root用户。1root@iZhp3fz3iqsadyes2s8ay8Z:~# visudo 这个时候会打开一个文本编辑器，去编辑/etc/sudoer这个文件，可能是vim，也可能是nano。找到： 12# User privilege specificationroot ALL=(ALL:ALL) ALL 在下面加入： 1chenming ALL=(ALL:ALL) ALL 保存（vim下是:x，nano下是ctrl+x），退出，这样chenming这个用户就加入了sudo组。 删除用户仅仅删除用户：1234root@iZhp3fz3iqsadyes2s8ay8Z:~# deluser chenmingRemoving user `chenming&apos; ...Warning: group `chenming&apos; has no more members.Done. 将用户的目录也删除：1root@iZhp3fz3iqsadyes2s8ay8Z:~# deluser --remove-home chenming 但这个时候，这个已经被删除的用户还是在sudo组里面。参照上面的过程，使用visudo命令，删掉增加的那一行即可。]]></content>
      <categories>
        <category>Linux</category>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS：NPM依赖包版本号波浪字符"~"]]></title>
    <url>%2Fjs%2Fjs-npm-symbol-tilde.html</url>
    <content type="text"><![CDATA[提醒原帖完整收藏于IT老兵驿站，并会不断更新。 JS：NPM依赖包版本号波浪字符”~”。 参考官网：https://github.com/npm/node-semver#functions。 Tilde Ranges ~1.2.3 ~1.2 ~1Allows patch-level changes if a minor version is specified on the comparator. Allows minor-level changes if not. 如果minor被指定，则允许patch被改变；如果没有，允许minor被改变。（个别知识需要参考前面的帖子） ~1.2.3 := &gt;=1.2.3 &lt;1.(2+1).0 := &gt;=1.2.3 &lt;1.3.0~1.2 := &gt;=1.2.0 &lt;1.(2+1).0 := &gt;=1.2.0 &lt;1.3.0 (Same as 1.2.x)~1 := &gt;=1.0.0 &lt;(1+1).0.0 := &gt;=1.0.0 &lt;2.0.0 (Same as 1.x)~0.2.3 := &gt;=0.2.3 &lt;0.(2+1).0 := &gt;=0.2.3 &lt;0.3.0~0.2 := &gt;=0.2.0 &lt;0.(2+1).0 := &gt;=0.2.0 &lt;0.3.0 (Same as 0.2.x)~0 := &gt;=0.0.0 &lt;(0+1).0.0 := &gt;=0.0.0 &lt;1.0.0 (Same as 0.x)~1.2.3-beta.2 := &gt;=1.2.3-beta.2 &lt;1.3.0 Note that prereleases in the 1.2.3 version will be allowed, if they are greater than or equal to beta.2. So, 1.2.3-beta.4 would be allowed, but 1.2.4-beta.2 would not, because it is a prerelease of a different [major, minor, patch] tuple.]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>波浪字符</tag>
        <tag>依赖包</tag>
        <tag>Javascript</tag>
        <tag>NPM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS：NPM依赖包版本号波浪字符"~"]]></title>
    <url>%2Fjs%2Fjs-npm-symbol-caret.html</url>
    <content type="text"><![CDATA[提醒原帖完整收藏于IT老兵驿站，并会不断更新。 JS：NPM依赖包版本号波浪字符”~”。 正文官网摘录如下： Tilde Ranges ~1.2.3 ~1.2 ~1Allows patch-level changes if a minor version is specified on the comparator. Allows minor-level changes if not. 简单翻译：如果minor被指定，则允许patch被改变；如果没有，允许minor被改变。这里需要理解NPM的版本三元组，NPM采用的是3元组的版本控制，[major，minor，patch]。（详细知识需要参考这里） 实例： ~1.2.3 := &gt;=1.2.3 &lt;1.(2+1).0 := &gt;=1.2.3 &lt;1.3.0 minor被指定为2，所以能够获取的版本是大于等于1.2.3，小于1.3之间。 ~1.2 := &gt;=1.2.0 &lt;1.(2+1).0 := &gt;=1.2.0 &lt;1.3.0 (Same as 1.2.x) minor被指定为2，所以能够获取的版本是大于等于1.2，小于1.3之间。 ~1 := &gt;=1.0.0 &lt;(1+1).0.0 := &gt;=1.0.0 &lt;2.0.0 (Same as 1.x) minor没有被指定，所以能够获取的版本是大于等于1，小于2之间，就是minor可以改变。 下面留几道思考题： ~0.2.3 := &gt;=0.2.3 &lt;0.(2+1).0 := &gt;=0.2.3 &lt;0.3.0~0.2 := &gt;=0.2.0 &lt;0.(2+1).0 := &gt;=0.2.0 &lt;0.3.0 (Same as 0.2.x)~0 := &gt;=0.0.0 &lt;(0+1).0.0 := &gt;=0.0.0 &lt;1.0.0 (Same as 0.x)~1.2.3-beta.2 := &gt;=1.2.3-beta.2 &lt;1.3.0 Note that prereleases in the 1.2.3 version will be allowed, if they are greater than or equal to beta.2. So, 1.2.3-beta.4 would be allowed, but 1.2.4-beta.2 would not, because it is a prerelease of a different [major, minor, patch] tuple. 参考https://github.com/npm/node-semver#functions。]]></content>
      <categories>
        <category>JavaScript</category>
        <category>NPM</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>波浪字符</tag>
        <tag>依赖包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法及常见用例总结：tar]]></title>
    <url>%2Flinux%2Fshell-command-tar.html</url>
    <content type="text"><![CDATA[概要Linux下shell命令用法及常见用例总结：tar。 博客博客地址：IT老兵驿站 前言tar命令用来归档多个文件或目录到单个归档文件中，并且归档文件可以进一步使用 gzip 或者 bzip2 等技术进行压缩。 上文是摘抄自网上。 这系列总结第一版的思路不够清楚，当时更多是摘抄了一些内容，没有输出自己的理解，对这篇文档逐渐进行整理，更多加入自己的一些理解。 正文命令格式tar [OPTION...] [FILE]... 命令功能 Tar（Tape ARchive，磁带归档的缩写，最初设计用于将文件打包到磁带上，现在大都使用它来实现备份某个分区或者某些重要的目录）是类Unix系统中使用最广泛的命令，用于归档多个文件或目录到单个归档文件中，并且归档文件可以进一步使用gzip或者bzip2等技术进行压缩，还能保留其文件权限。换言之，tar命令也可以用于备份：先是归档多个文件和目录到一个单独的tar文件或归档文件，然后在需要之时将tar文件中的文件和目录释放出来。 命令选项 选项 含义 -A或–catenate 新增文件到以存在的备份文件 -B 设置区块大小 -c或–create 建立新的备份文件 -C&lt;目录&gt; 这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项 -d 记录文件的差别 -x或–extract或–get 从备份文件中还原文件 -t或–list 列出备份文件的内容 -z或–gzip或–ungzip 通过gzip指令处理备份文件 -Z或–compress或–uncompress 通过compress指令处理备份文件 -f&lt;备份文件&gt;或–file=&lt;备份文件&gt; 指定备份文件 -v或–verbose 显示指令执行过程 -r 添加文件到已经压缩的文件 -u 添加改变了和现有的文件到已经存在的压缩文件 -j 支持bzip2解压文件 -v 显示操作过程 -l 文件系统边界设置 -k 保留原有文件不覆盖 -m 保留文件不被覆盖 -w 确认压缩文件的正确性 -p或–same-permissions 用原来的文件权限还原文件 -P或–absolute-names 文件名使用绝对名称，不移除文件名称前的“/”号 -N &lt;日期格式&gt;或–newer=&lt;日期时间&gt;只将较指定日期更新的文件保存到备份文件里 –exclude=&lt;范本样式&gt; 排除符合范本样式的文件 什么是“文件压缩”？ 我们知道，在计算机系统中文件的内容是信息，信息实际上就是一个由值0和值1组成的位（又称为比特）序列，8个位被组织成一组，称为字节。一般来说，一个字节的8位是没有被全部利用起来的，这些没有被利用的位占据了一个文件的大部分空间，而“文件压缩”就是利用复杂的计算方式，将这些没有利用的空间腾出来，以让文件占用的空间变小。简单来说，「压缩」就是把文件中没有完全填满的空间填满。压缩过的文件不能直接被操作系统所使用，因此，「解压缩」就是指把文件「还原」为未压缩之前的模样。压缩前与压缩后的文件所占用的磁盘空间大小之比就是「压缩比」。 常见的压缩格式Linux 中常见的压缩格式有：123456*.Z：compress 程序压缩的文件。*.gz：gzip 程序压缩的文件。*.bz2：bzip2 程序压缩的文件。*.tar：tar 程序打包的数据，没有被压缩过。*.tar.gz（简写为 .tgz）：tar 程序打包的数据，经过 gzip 的压缩。*.tar.bz2（简写为 .tbz2）：tar 程序打包的数据，经过 bzip2 的压缩。 上面的压缩格式中，主要是gzip和bzip2两个压缩命令，它们是GNU计划的中的一部分，在此之前是compress命令，但它已经不再流行了。bzip2比gzip的压缩比很好，不过bzip2通常只能针对一个文件来压缩和解压缩。如果是这样的话，压缩整个开发环境目录就太繁琐了。 因此tar命令就出现了，tar不是一个 “压缩命令”，而是一个“打包命令”。也就是说，tar可以把很多文件「打包」成一个文件，甚至连目录也可以进行打包。一开始tar命令的确是不支持压缩的功能，后来GNU计划为了提供给使用者更方便并且更加强大的压缩与打包功能，就把整个tar与压缩的功能结合在一起了。 仅仅打包起来的tar文件俗称tarfile文件，经过压缩的tar文件叫做tarball文件。 全能的 tar 命令概要tar可以将多个目录或文件打成一个大文件，同时支持gzip/bzip2 归档：tar {-c} [option…] -f destination source追加归档：tar {-r | -u} -f source [option…] destination解压：tar {-t | -x} -f source [option…] -C destination 最简单的使用 tar 只要记住下面的方式： 压缩：tar -jcv -f filename.tar.bz2 被压缩的文件或目录名称 查看文件：tar -jtv -f filename.tar.bz2 解压缩：tar -jxv -f filename.tar.gz -C 解压到哪里 filename.tar.bz2 既然tar不是一个压缩命令，是个打包命令，那么是如何做到打包并压缩的呢？我们先来看一下tar命令的常用参数： 模式参数 -c（–create）：创建新的归档文件。 -r（–append）：与-c一样创建新的归档文件，但这是以追加的模式，只能往未压缩过的归档文件中追加，要求指定-f参数。 -t：查看归档文件的内容含有哪些文件，可以看到包括文件名在内的详细信息。 -u：与-r一样，但是只往归档文件添加更新的文件。 -x：解压缩归档文件。如果一个归档文件里有相同文件名的多个文件，那么会先将每个文件解压，最新的文件将覆盖旧的文件。 tar分为三种模式，-c，-r，-u三个一类，为归档/压缩模式，在该模式下，tar会递归遍历指定目录下的所有目录和文件，并创建归档文件。-x表示为去归档/解压模式，-t表示为打印列表模式。 通用参数 -j：使用bzip2的支持进行压缩和解压缩，文件名最好为*.tar.bz2。 -z：使用gzip的支持进行压缩和解压缩，文件名最好为*.tar.gz。 -v：在压缩/解压缩的过程中，将正在处理的文件名显示出来。 -f：后面接被处理的文件名，最好把-f单独出来写一个参数。 -C：指定解压的目录。 -p：保留文件的原始信息，权限等等 -P：解压时保留绝对路径。 –exclude=FILE：在打包压缩的时候，不要将 FILE 打包。 示例示例：打包一个目录。描述：将/home/test这个目录打包，生成文件名为command-18-06-02.tar的归档文件，保存在当前目录下。123456tar -cv -f command-18-06-02.tar /home/test/home/test/.bash_logout/home/test/.bashrc/home/test/apache-tomcat-9.0.7.tar.gz/home/test/.bash_profile/home/test/nginx-1.10.1.tar.gz -c（–create的简写）参数，这表示为指定的文件或者目录创建新的归档文件。使用-f指定读取或者写入的归档文件，可以用-表示标准输入或者标准输出，-f可以与其他参数连起来写，必须保证f参数后面跟的是文件名。但不推荐这样写，因为参数调换顺序是允许的，如果写成-cfv就会导致压缩后的文件名变成了v。 使用-v表示生成详细的输出，在压缩或者解压的模式中，会列出正在向归档文件读或者写的文件名字。 示例：打包并且使用 gzip 压缩。描述：将/home/test/images目录下的所有文件以及目录中的文件打包，并用gzip进行压缩，生成名为MyImages-18-06-02.tar.gz的归档文件，放在当前目录下。 12345678tar -zcv -f MyImages-18-06-02.tar.gz /home/test/imagesOR# tar -zcv -f MyImages-18-06-02.tar.tgz /home/test/images/home/test/images/alejandro-gonzalez-17189.jpg/home/test/images/brooke-lark-275181.jpg/home/test/images/brenda-godinez-228181.jpg/home/test/images/artur-rutkowski-97622.jpg/home/test/images/ben-white-138743.jpg -z表示要使用gzip支持来压缩或者解压文件，注意gzip的压缩的文件格式最好写成tar.gz。（注：tar.gz 和 tgz 是同一个意思） 示例：打包压缩并排除某些文件。描述：将/home/test/images目录下，排除brooke-lark-275181.jpg和ben-white-138743.jpg之外的所有文件打包，并用gzip进行压缩，生成名为MyImages-18-06-02.tar.gz的归档文件，放在当前目录下。这个--exclude参数需要放在源文件前面。 1234tar -czv -f MyImages-18-06-02.tar.gz --exclude=./brooke-lark-275181.jpg --exclude=./ben-white-138743.jpg /home/test/images/home/test/images/alejandro-gonzalez-17189.jpg/home/test/images/brenda-godinez-228181.jpg/home/test/images/artur-rutkowski-97622.jpg 示例：解压，默认解压。描述：将名为MyImages-18-06-02.tar的归档文件解压至当前目录下。 1234# tar -xvf MyImages-18-06-02.tarhome/test/images/alejandro-gonzalez-17189.jpghome/test/images/brenda-godinez-228181.jpghome/test/images/artur-rutkowski-97622.jpg 其中，-x参数表示去解压一个归档文件，如果归档文件中有两个相同名字的文件，那么每一个文件都会被解压出来，然后最新的会覆盖旧的文件。注意这里没有指定-j参数，因为tar看到指定了-x参数，就知道这是解压操作，会自动判断该解压包的压缩类型。 示例：解压到一个指定目录。描述：将名为MyImages-18-06-02.tar.gz的归档文件解压至一个指定的目录。 1234tar -xv -f MyImages-18-06-02.tar -C /home/test/public_imageshome/test/public_images/alejandro-gonzalez-17189.jpghome/test/public_images/brenda-godinez-228181.jpghome/test/public_images/artur-rutkowski-97622.jpg 示例：查看压缩包文件信息。描述：列出MyImages-18-06-02.tar.bz2中的文件信息，-v参数，会生成与ls(1)命令相近的输出。 123456tar -tv -f MyImages-18-06-02.tar.gzORtar -tv -f MyImages-18-06-02.tar.bz2-rw-r--r-- root/root 2176861 2018-06-02 21:26 home/test/images/alejandro-gonzalez-17189.jpg-rw-r--r-- root/root 8452524 2018-06-02 21:26 home/test/images/brenda-godinez-228181.jpg-rw-r--r-- root/root 1131986 2018-06-02 21:26 home/test/images/artur-rutkowski-97622.jpg 示例：解压单个文件。描述：将home/test/.bashrc这一个文件从归档文件中提取出来。12tar -xv -f command-18-06-02.tar home/test/.bashrchome/test/.bashrc 示例：解压多个指定的文件。描述：将file1、file2等多个文件从归档文件中提取出来，可以用空格隔开多个文件，也可以用通配符的形式。1234567tar -zxv -f MyImages-18-06-02.tar.gz "file 1" "file 2"OR# tar -zxv -f MyImages-18-06-02.tar.gz --wildcards '*b*.jpg'home/test/images/brooke-lark-275181.jpghome/test/images/brenda-godinez-228181.jpghome/test/images/ben-white-138743.jpghome/test/images/aleks-dahlberg-274646.jpg 示例：更换打包路径切换到 home 目录下，进行打包，这样打包里面就没有home这级目录。1tar -zxv -f MyImages-18-06-02.tar.gz -C home .]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS：文档流]]></title>
    <url>%2Fcss%2Fcss-normal-flow.html</url>
    <content type="text"><![CDATA[CSS的文档流介绍。 官网：https://www.w3.org/TR/2016/WD-CSS22-20160412/visuren.html#normal-flow。 文档流文档流其实应该叫正常流，英文是Normal flow，我的理解呢，就是接收到的文档的内容，因为这些内容一直从服务端传输过来，边传输边需要处理，就像水流一样，所以称为流。 在文档流中的盒子是需要归属于一个上下文的，块级盒子参与到块格式化上下文中，内联级盒子参与到内联格式化上下文中，还有表格格式化上下文。 块格式化上下文（Block formatting contexts）块格式化上下文，简称BFC，是按照从上到下，一个一个垂直排列的，块之间的间距是靠margin来控制的。 In a block formatting context, boxes are laid out one after the other, vertically, beginning at the top of a containing block. The vertical distance between two sibling boxes is determined by the ‘margin’ properties. Vertical margins between adjacent block-level boxes in a block formatting context collapse. 翻译：在块格式化上下文中，框从一个包含块的顶部开始一个接一个地垂直排列。 两个兄弟盒子之间的垂直距离由“margin”属性决定。 块格式化上下文中相邻块级盒子之间的垂直margin会折叠。 ##内联格式化上下文（Inline formatting contexts）内联格式化上下文，简称IFC，主要是水平排列的，水平对齐是由一些参数来控制的。 An inline formatting context is established by a block container box that contains no block-level boxes. In an inline formatting context, boxes are laid out horizontally, one after the other, beginning at the top of a containing block. Horizontal margins, borders, and padding are respected between these boxes. The boxes may be aligned vertically in different ways: their bottoms or tops may be aligned, or the baselines of text within them may be aligned. The rectangular area that contains the boxes that form a line is called a line box. 翻译：内联格式化上下文由不包含块级框的块容器盒子建立。 在内联格式化上下文中，盒子从一个接一个地开始，从一个包含块的顶部开始。 这些框之间会考虑水平边距，边框和填充。 盒子可以以不同的方式垂直对齐：它们的底部或顶部可以对齐，或者它们内的文本的基线可以对齐。 包含形成一条线的框的矩形区域称为线盒子line box。 这里面有一些父容器和子布局的一些关系，需要梳理。 相对定位相对定位是根据这个盒子原本在文档流中的位置或者floated进行一些偏移。 未完，待续……]]></content>
      <categories>
        <category>CSS</category>
      </categories>
      <tags>
        <tag>CSS</tag>
        <tag>文档流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java：Tomcat的部署实例之资源目录]]></title>
    <url>%2Fjava%2Fjava-deploy-resource-folder.html</url>
    <content type="text"><![CDATA[原帖收藏于IT老兵博客。 Tomcat上部署应用后，原本目录是否会被移除。 实例：一个项目的资源放在了WebContent下面，这样每次打包，都会将这些文件打包进去，这样在打包时，导致打出来的war包有好几百兆，这样上传Git也非常不方便。 方案1：分析：如果删除掉本地WebContent下的资源文件，再部署到服务器上，war包其实是一个压缩包，加压后覆盖原本目录下的相同内容，因为新上传的war包没有相同的资源文件，这样就不会覆盖原本的资源文件。结果：加压后的项目目录也不存在资源文件了，看来这个部署过程，是会删掉原本的项目目录的。 方案2：分析：因为webapps是web服务根目录，那么把资源文件从项目目录移到webapps下面，这样应该也可以被访问到。结果：成功，可以被访问到。 为了验证这个，上网查了很多帖子，众说纷纭，最后还是在官网找到这么一段话： The following deployment sequence will occur on Tomcat startup in that case: Any Context Descriptors will be deployed first. Exploded web applications not referenced by any Context Descriptor will then be deployed. If they have an associated .WAR file in the appBase and it is newer than the exploded web application, the exploded directory will be removed and the webapp will be redeployed from the .WAR .WAR files will be deployed 注意这里the exploded directory will be removed and the webapp will be redeployed from the .WAR，原本的解压目录会被移除，应用会被重新从war文件中部署。 寻根究底，而不人云亦云，这样才是端正的学习的态度。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>tomcat</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git：工作流程Git Flow]]></title>
    <url>%2Fgit%2Fgit-git-flow.html</url>
    <content type="text"><![CDATA[概要Git：工作流程Git Flow。 博客博客地址：IT老兵驿站。 前言参考：https://nvie.com/posts/a-successful-git-branching-model/， 这篇帖子是10年发表的，而我大概是08、09年接触的Git，当时因为刚刚花了好大气力研究明白SVN的流程，所以对Git很排斥，这也是我工作中一直以来的一个问题，因为在一项老技术上花了太多气力，而导致对新技术的出现本能地产生很大的排斥。如果当时仔细去研究一下Git，应该会发现Git不是来革我们这些SVN拥趸的命，而是提供完善和丰富了SVN的功能。 概述从CVS到SVN，再从SVN到Git。从中心化到去中心化的中心化（Decentralized but centralized），这句话挺有挺有深意。 分支长期分支项目存在两个长期分支： 主分支master。 开发分支develop或者dev。 We consider origin/master to be the main branch where the source code of HEAD always reflects a production-ready state.We consider origin/develop to be the main branch where the source code of HEAD always reflects a state with the latest delivered development changes for the next release. Some would call this the “integration branch”. This is where any automatic nightly builds are built from. 这里的HEAD是Git的一个指针，指向当前的分支上。上面的话的意思大概是master分支总是指向“等待上生产”状态的代码。develop分支往往是最近交付的开发修改。这个过程是和原本的SVN工作流是很接近的，一个开发分支，一个线上分支。开发完，测试后，发布到线上。SVN流程推荐在测试时分叉一个branch出来进行测试，这个时候不影响trunk上业务的继续开发，这个工作流没有这么明说，但是因为Git的灵活性，建立一个临时的测试分支也是没有问题的。Git好就好在非常灵活，不过也正是因为如此，导致了一些问题，之前有一个小朋友，把所有的功能分支都保存了下来，还说这样会更加方便，我很难理解，这样怎么会方便呢？每个人分支都需要不断同步。灵活也应该是相对的，在一个相对固定的流程下，适当的灵活，是可以提高效率的。 支持分支原文叫做supporting branches。这里面的每一个分支都有指定的目的和约束的规则，如何产生和如何合并。 Feature branches Release branches Hotfix branches 功能分支可以产生于:develop必须合并到:develop分支命名约定:除了master, develop, release-, or hotfix- 都可以，前面几个作为保留。 功能分支用于开发未来的一项功能，目标的发布此时可能还不确定。这个分支最终会被合并回develop（采用了）或者被抛弃掉（不采用）。功能分支更多存在于用户仓库，而不是origin仓库。 创建：12$ git checkout -b myfeature developSwitched to a new branch &quot;myfeature&quot; 合并回develop： 12345678$ git checkout developSwitched to branch &apos;develop&apos;$ git merge --no-ff myfeatureUpdating ea1b82a..05e9557(Summary of changes)$ git branch -d myfeatureDeleted branch myfeature (was 05e9557).$ git push origin develop 对于–no-ff，参考：https://git-scm.com/docs/git-merge，有待更进一步的解释。 发布分支可以产生于:develop必须合并到:develop和master分支分支命名约定:release-* 我理解的，这里主要用于准备一个发布版的功能已经开发完成，等待一些信息最后的确认，为了不影响下一个开发版的正常进行，打出一个发布分支。 创建一个发布分支1234567$ git checkout -b release-1.2 developSwitched to a new branch &quot;release-1.2&quot;$ ./bump-version.sh 1.2Files modified successfully, version bumped to 1.2.$ git commit -a -m &quot;Bumped version number to 1.2&quot;[release-1.2 74d9424] Bumped version number to 1.21 files changed, 1 insertions(+), 1 deletions(-) 结束一个发布分支合并回master分支123456$ git checkout masterSwitched to branch &apos;master&apos;$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes)$ git tag -a 1.2 合并回develop分支12345$ git checkout developSwitched to branch &apos;develop&apos;$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes) 删除原分支12$ git branch -d release-1.2Deleted branch release-1.2 (was ff452fe). 热修复分支可以产生于:master必须合并到:develop和master分支分支命名约定:hotfix-* 主要用于对线上代码进行热修复用，线上代码出现了问题，开出一个分支进行修复，等修复完成，合并回master和develop分支。 创建 1234567$ git checkout -b hotfix-1.2.1 masterSwitched to a new branch &quot;hotfix-1.2.1&quot;$ ./bump-version.sh 1.2.1Files modified successfully, version bumped to 1.2.1.$ git commit -a -m &quot;Bumped version number to 1.2.1&quot;[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.11 files changed, 1 insertions(+), 1 deletions(-) 提交 123$ git commit -m &quot;Fixed severe production problem&quot;[hotfix-1.2.1 abbe5d6] Fixed severe production problem5 files changed, 32 insertions(+), 17 deletions(-) 结束合并回master123456$ git checkout masterSwitched to branch &apos;master&apos;$ git merge --no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes)$ git tag -a 1.2.1 合并回develop12345$ git checkout developSwitched to branch &apos;develop&apos;$ git merge --no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes) 删除 12$ git branch -d hotfix-1.2.1Deleted branch hotfix-1.2.1 (was abbe5d6).]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Git Flow</tag>
        <tag>工作流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法及常见用例：netstat]]></title>
    <url>%2Flinux%2Fshell-command-netstat.html</url>
    <content type="text"><![CDATA[netstat命令用来查看系统中所有的网络套接字连接情况。 命令格式netstat [选项] 参考：https://linux.die.net/man/8/netstat。 netstat命令用来查看系统中所有的网络套接字连接情况。 命令格式netstat [选项] 参考：https://linux.die.net/man/8/netstat。 netstat命令用来查看系统中所有的网络套接字连接情况。 命令格式netstat [选项] 命令功能netstat命令用来查看系统中所有的网络套接字连接情况，包括TCP、UDP和Unix套接字。也可以显示路由表，接口状态，masquerade 连接，多播成员（Multicast Memberships）等等。另外，它还可以列出处于监听状态（等待接入请求）的套接字，比如想确认系统中的web服务是否起来，就可以查看80端口有没有打开。 命令参数 -a或–all：显示所有选项，默认不显示LISTEN相关。 -t或–tcp：(TCP)仅显示TCP相关选项。 -u或–udp：(UDP)仅显示UDP相关选项。 -x或–unix：此参数的效果和指定”-A unix”参数相同。 -n或–numeric：拒绝显示别名，能显示数字的全部转化成数字。 -l或–listening：仅列出有在Listen(监听)的服务状态。 -g或–groups：显示多重广播功能群组组员名单。 -p或–programs：显示建立相关链接的程序名和PID。 -r或–route：显示路由信息，路由表。 -e或–extend：显示扩展信息，例如UID等。 -s或–statistics：按各个协议进行统计。 -c或–continuous：每隔一个固定时间，执行该netstat命令。 -g或–groups：显示多重广播功能群组组员名单。 提示：LISTEN和LISTENING的状态只有用-a或者-l才能看到。 输出信息含义netstat的输出结构可以分为两个部分：一个是Active Internet connections，称为有源TCP连接。其中”Recv-Q”和”Send-Q”指的是接收队列和发送队列。123456Active Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 VM01.root:ssh 61.149.11.230:21859 ESTABLISHEDtcp 0 0 localhost:51476 localhost:27017 ESTABLISHEDtcp 0 0 VM01.root:ssh 61.149.11.230:50883 ESTABLISHEDtcp 0 0 VM01.root:58300 47.89.193.173:3666 ESTABLISHED 另一个是Active UNIX domain sockets，称为有源Unix域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。Proto显示连接使用的协议，RefCnt表示连接到本套接口上的进程号，Types显示套接口的类型，State显示套接口当前的状态，Path表示连接到套接口的其它进程使用的路径名。 1234567Active UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 15049 /run/user/0/systemd/notifyunix 3 [ ] DGRAM 13640 /run/systemd/notifyunix 2 [ ] DGRAM 13645 /run/systemd/journal/syslogunix 8 [ ] DGRAM 13660 /run/systemd/journal/socketunix 25 [ ] DGRAM 10467 /run/systemd/journal/dev-log 实例实例：列出当前所有的连接（-a）命令：netstat -a输出： 12345678root@iZhp3fz3iqsadyes2s8ayeZ:~# netstat -aActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 *:8838 *:* LISTEN tcp 0 0 localhost:27017 *:* LISTEN tcp 0 0 *:8330 *:* LISTEN tcp 0 0 localhost:submission *:* LISTEN ...... 实例：列出所有TCP端口（-t）命令：netstat -at输出： 1234567root@iZhp3fz3iqsadyes2s8ayeZ:~# netstat -atActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 *:8838 *:* LISTEN tcp 0 0 localhost:27017 *:* LISTEN tcp 0 0 *:8330 *:* LISTEN tcp 0 0 localhost:submission *:* LISTEN 示例：列出所有监听TCP的端口，数字显示描述：查看本机监听的（-l）TCP连接（-t）的IP地址的数字显示（-n）。不适用-n的话，就会用端口的约定名称来显示，例如80端口，会显示成http。命令：netstat -tnl输出： 123456root@iZhp3fz3iqsadyes2s8ayeZ:~# netstat -tnlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:8838 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:27017 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8330 0.0.0.0:* LISTEN 示例：获取本机的所有的TCP连接的进程名、进程号以及用户ID描述：使用-p选项查看进程信息，-ep选项可以同时查看进程名和用户名。另外，-n和-e选项一起使用，User列的属性就是用户ID，而不是用户名。查看本机所有的（al）TCP连接的（t）进程名（p）和用户名ID（ne）。命令：netstat -altpen 1234567root@iZhp3fz3iqsadyes2s8ayeZ:~# netstat -altpenActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State User Inode PID/Program nametcp 0 0 0.0.0.0:8838 0.0.0.0:* LISTEN 0 11863750 31212/bnewd tcp 0 0 127.0.0.1:27017 0.0.0.0:* LISTEN 110 2945745 18546/mongod tcp 0 0 0.0.0.0:8330 0.0.0.0:* LISTEN 0 22250263 13550/btnd tcp 0 0 127.0.0.1:587 0.0.0.0:* LISTEN 0 12285119 11792/sendmail: MTA 这个可能是最屌的命令了，也可能是最常用的命令了。 还有一些实例，暂时不常用，有待完善。]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB：聚合之累加操作符]]></title>
    <url>%2Fmongodb%2Fmongodb-collection-aggregator-accumulate-operator.html</url>
    <content type="text"><![CDATA[MongoDB的聚合之累加操作符。 官网：https://docs.mongodb.com/manual/reference/operator/aggregation/group/#considerations。 累加操作符感觉这个没有太多可说的，简单翻译一下。 名字 描述 $avg Returns an average of numerical values. Ignores non-numeric values.（返回平均值） $first Returns a value from the first document for each group. Order is only defined if the documents are in a defined order.（返回第一个） $last Returns a value from the last document for each group. Order is only defined if the documents are in a defined order.（返回最后一个） $max Returns the highest expression value for each group.（返回最大值） $min Returns the lowest expression value for each group.（返回最小值） $push Returns an array of expression values for each group. $addToSet Returns an array of unique expression values for each group. Order of the array elements is undefined.（） $stdDevPop Returns the population standard deviation of the input values. $stdDevSamp Returns the sample standard deviation of the input values. $sum Returns a sum of numerical values. Ignores non-numeric values.（返回总和）]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记：聚合之介绍]]></title>
    <url>%2Fmongodb%2Fmongodb-collection-aggregation-introduction.html</url>
    <content type="text"><![CDATA[博客IT老兵博客。 概述MongoDB学习笔记：聚合之介绍。 前言初衷：MongoDB 的使用还是有一定难度的，官网的讲解一环牵扯一环，不容易一下子把握住重点，也不利于快速查询操作，所以整理一下。 （本文第一版是2018年7月4完成，8月30日感觉这篇文章的结构和后来写的结构不太统一，思路也不统一，所以对其进行一定的修改。） 正文概念聚合函数是对记录集（data records）进行操作，是把多条记录集合（group）在一起，进行一些统一的操作，返回一个结果，与此相对应的是SQL 的 group by 等操作，这是数据处理所涉及的一个方面，对很多具有一定相同属性的数据整体进行处理。 MongoDB 提供三种聚合方法： 聚合管道。 map-reduce 函数。 单一功能的聚合方法。 聚合管道接触过linux shell的人应该对管道不会陌生，管道就是对输入的数据进行一系列的处理、转换，变成新的数据。 这里的聚合管道是对记录集进行多阶段（multi-stage）的转换，转换文档为一个新的聚合结果，例如： 1234db.orders.aggregate([ &#123; $match: &#123; status: "A" &#125; &#125;, &#123; $group: &#123; _id: "$cust_id", total: &#123; $sum: "$amount" &#125; &#125; &#125;]) 解释一下： 数据集合：orders，共有4条记录，这里省略了_id 这个域。 需求：查找所有status=&quot;A&quot; 的记录，根据cust_id进行分组，计算每个组的amount的和。 分析：{$match: {status: &quot;A&quot;}}，第一个阶段，匹配阶段，查找所有status=&quot;A&quot; 的记录。{$group: {_id: &quot;$cust_id&quot;, total: {$sum: &quot;$amount&quot;}}}，第二个阶段，分组计算，根据cust_id进行分组，对每个组的amount进行求和。这里涉及$group 的语法，如下： 1&#123; $group: &#123; _id: &lt;expression&gt;, &lt;field1&gt;: &#123; &lt;accumulator1&gt; : &lt;expression1&gt; &#125;, ... &#125; &#125; 其中，_id是强制的，后面是可选的。&lt;accumulator1&gt;是累加操作符，参考这里，例如这里的$sum，注意，这里必须要加$。&lt;expression1&gt;是表达式，有待补充， &quot;$amount&quot; , 表示是去取上一个结果中的amount 这个域，对其进行累加，并把结果存入新的域total中。 这个例子看明白了，聚合就基本明白了。 Map-Reducemap-reduce操作分为两个阶段：map 阶段，处理每一条记录，产出一个或多个对象；reduce 阶段，合并 map 阶段的输出。作为可选，map-reduce可以有一个最终阶段来对结果进行最终的操作。map-reduce 也可以进行查询、排序和限制输出结果。 图片取自 MongoDB 官网。 这个例子的顺序有点不太好理解，先做 query，然后是 map，reduce， 但是在指令中，则是先有 map。 In this map-reduce operation, MongoDB applies the map phase to each input document (i.e. the documents in the collection that match the query condition). The map function emits key-value pairs. For those keys that have multiple values, MongoDB applies the reduce phase, which collects and condenses the aggregated data. MongoDB then stores the results in a collection. Optionally, the output of the reduce function may pass through a finalize function to further condense or process the results of the aggregation. 在这个过程中，MongoDB 应用 map 去针对每一条输入文档（这些文档是满足了 query 查询条件的）。map 函数 emit （发射出）key-value 对，对于那些有多值的 key，MongoDB 使用 reduce，来收集和浓缩这些聚集数据。 单一功能的聚合方法MongoDB也提供db.collection.count()（求和）和db.collection.distinct()（去重）函数。 图片取自 MongoDB 官网。 总结关于聚合，总体的概念总结到这里，下一步，需要细化了。 （2019-12-02）又复习整理了一遍聚合，感觉理解的更加清楚了。 之前的图片都失效了，把这些图片都补上了。 参考https://docs.mongodb.com/manual/aggregation/#single-purpose-agg-operationshttps://docs.mongodb.com/manual/core/map-reduce/]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>aggregate</tag>
        <tag>聚合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB：查询和投影操作符]]></title>
    <url>%2Fmongodb%2Fmongodb-collection-find-projection-operator.html</url>
    <content type="text"><![CDATA[MongoDB查询和投影操作符。 官网：https://docs.mongodb.com/manual/reference/operator/query/。 这一章节都是很简单的英语，就做一个很简单的备注，如果连这个英语都看不懂，那就需要提高了，程序员看不懂基本的英语是很难提高的。这一章节还需要完善一些样例，这个有待补充。 查询选择器比较 名字 描述 $eq Matches values that are equal to a specified value.（判断相等） $gt Matches values that are greater than a specified value.（判断大于） $gte Matches values that are greater than or equal to a specified value.（判断大于等于） $in Matches any of the values specified in an array.（判断在其中） $lt Matches values that are less than a specified value.（判断小于） $lte Matches values that are less than or equal to a specified value.（判断小于等于） $ne Matches all values that are not equal to a specified value.（判断所有值都不等于指定值） $nin Matches none of the values specified in an array.（判断不在其中） 逻辑 名字 描述 $and Joins query clauses with a logical AND returns all documents that match the conditions of both clauses.（与） $not Inverts the effect of a query expression and returns documents that do not match the query expression.（非） $nor Joins query clauses with a logical NOR returns all documents that fail to match both clauses.（异或） $or Joins query clauses with a logical OR returns all documents that match the conditions of either clause.（或） 元素 名字 描述 $exists Matches documents that have the specified field. $type Selects documents if a field is of the specified type. 评估 名字 描述 $expr Allows use of aggregation expressions within the query language. $jsonSchema Validate documents against the given JSON Schema. $mod Performs a modulo operation on the value of a field and selects documents with a specified result. $regex Selects documents where values match a specified regular expression. $text Performs text search. $where Matches documents that satisfy a JavaScript expression. 地理空间 名字 描述 $geoIntersects Selects geometries that intersect with a GeoJSON geometry. The 2dsphere index supports $geoIntersects. $geoWithin Selects geometries within a bounding GeoJSON geometry. The 2dsphere and 2d indexes support $geoWithin. $near Returns geospatial objects in proximity to a point. Requires a geospatial index. The 2dsphere and 2d indexes support $near. $nearSphere Returns geospatial objects in proximity to a point on a sphere. Requires a geospatial index. The 2dsphere and 2d indexes support $nearSphere. 数组 名字 描述 $all Matches arrays that contain all elements specified in the query. $elemMatch Selects documents if element in the array field matches all the specified $elemMatch conditions. $size Selects documents if the array field is a specified size. 位操作 名字 描述 $bitsAllClear Matches numeric or binary values in which a set of bit positions all have a value of 0. $bitsAllSet Matches numeric or binary values in which a set of bit positions all have a value of 1. $bitsAnyClear Matches numeric or binary values in which any bit from a set of bit positions has a value of 0. $bitsAnySet Matches numeric or binary values in which any bit from a set of bit positions has a value of 1. 注释 名字 描述 $comment Adds a comment to a query predicate. 投影操作 名字 描述 $ Projects the first element in an array that matches the query condition. $elemMatch Projects the first element in an array that matches the specified $elemMatch condition. $meta Projects the document’s score assigned during $text operation. $slice Limits the number of elements projected from an array. Supports skip and limit slices.]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MongoDB集合的基本操作：查找记录（查）]]></title>
    <url>%2Fmongodb%2Fmongodb-collection-find-1.html</url>
    <content type="text"><![CDATA[MongoDB集合的基本操作：查找记录（查）。 博客原文收藏于IT老兵驿站。 语法1db.collection.find(query, projection) 在集合或者视图的文档中进行选择，并且返回一个指向被选中的文档的游标。（原文是：Selects documents in a collection or view and returns a cursor to the selected documents.）参数|类型|描述-|-|-query|文档型|可选。使用查询操作符(参考这里)，指定了查询过滤器。 想要返回集合中所有的文档，忽略这个参数，或者传一个空的文档({})。projection|文档型|可选。制定了匹配查询过滤器，要返回的文档的域。想要返回匹配的文档中的所有域，忽略这个参数。 projection参数决定了哪些域需要被返回。 1&#123; field1: &lt;value&gt;, field2: &lt;value&gt; ... &#125; &lt;value&gt;可以是: 1 或 true 表示要在返回文档中包含这个域。 0 或 false 表示不包含这个域。 表达式使用了投影操作符（有待解释）。 分析基本的查找参考上面的语法即可，下面也有实例，其实较为难以掌握的是组合查找，例如逻辑关系是AND的，或者是OR的，还有IN的，这几个需要梳理一下。 实例实例 查找上文test集合中的所有文档。12345678910111213141516171819&gt; db.test.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;5abb3b5bce69c048be080199&quot;), &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;1&quot;&#125;&#123; &quot;_id&quot; : ObjectId(&quot;5abb3b5bce69c048be080120&quot;), &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;3&quot;&#125; pretty()是用来让展示更加舒适。 实例 查找test集合中的b=&quot;3&quot;的记录，这里要注意“3”和3是不一样的，这里是要符合js的语法，字符串和数字表示方式是不同的。做一个好的程序员，一定要严谨，而做到了严谨，可以帮你更快地提高，更快地产出，更好地规避错误，其实加快了你的职场发展节奏。12345678910&gt; db.test.find(&#123;b: &quot;3&quot;&#125;).pretty()&#123; &quot;_id&quot; : ObjectId(&quot;5abb3b5bce69c048be080120&quot;), &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;3&quot;&#125; 可以看到，这次只查出了一条符合条件的记录。 实例 查找test集合中的b=&quot;3&quot;的记录a和b两个域，不要其它域。 12&gt; db.test.find(&#123;b: &quot;3&quot;&#125;, &#123;a: 1, b: 1&#125;).pretty()&#123; &quot;_id&quot; : ObjectId(&quot;5abb3b5bce69c048be080120&quot;), &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;3&quot; &#125; 可以看到，没有涉及的域就没有再获取出来，这样在一些情况下是可以节省网络开销和分析成本的，在《高性能MySQL》也是讲过类似的原理，不要大而全地去把所有内容获取回来，对于资源的使用，应该是有规划的、经济地去使用。 实例 查找test集合中的b=&quot;3&quot; 并且a=&quot;4&quot;的记录。1&gt; db.test.find(&#123;b: &quot;3&quot;, a: &quot;4&quot;&#125;&#125;).pretty() 可以看到，在第一个{} 中逗号分隔开的是AND的查询关系。 实例 查找test集合中的b=&quot;3&quot; 或者b=&quot;4&quot;的记录。1&gt; db.test.find(&#123;b: &#123;$in: [&quot;3&quot;, &quot;4&quot;]&#125;&#125;).pretty() 这个语法的原则是操作符$in在前，作为JSON名值对的名，[&quot;3&quot;, &quot;4&quot;]是它的值，然后整个{$in: [&quot;3&quot;, &quot;4&quot;]}作为b的值，从JSON语法的角度去思考和记忆这个语法，就容易一些了。 实例 修改一下上面的例子，查找test集合中的b=&quot;3&quot; 或者a=&quot;4&quot;的记录。1&gt; db.test.find(&#123;$or: [&#123;b: &quot;3&quot;&#125;, &#123;a: &quot;4&quot;&#125;]&#125;).pretty() 这个语法和IN 的道理是一样的，其实AND也可以这么用，上面那种是隐式的用法，显式的用法是这样：1&gt; db.test.find(&#123;$and: [&#123;b: &quot;3&quot;&#125;, &#123;a: &quot;4&quot;&#125;]&#125;).pretty() 查询操作符还有一些大于、小于等操作，具体参考查询操作符一节。 总结本文只是总结了MongoDB查询的一部分的功能，MongoDB是NoSQL的，没有统一的模式，以一个json对象来保存记录，而查询主要是根据这个json对象的健值来做判断。 参考https://docs.mongodb.com/manual/reference/method/db.collection.find/]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记：插入记录（增）]]></title>
    <url>%2Fmongodb%2Fmongodb-collection-insert-1.html</url>
    <content type="text"><![CDATA[完整的MongoDB学习笔记位于IT老兵博客。 MongoDB学习笔记：插入记录（增）。 初衷MongoDB的使用还是有一定难度的，官网讲解的一环牵扯一环，不容易一下子把握住重点，也不利于快速查询操作，所以整理一下，便于快速查找。另外，做一做笔记，对于记忆和理解也是有好处的，同时可以方便一些英语暂时不好的同学用来参考。 语法1233.2版本之后db.collection.insertOne()db.collection.insertMany() 123.2版本之前db.collection.insert() 12345678910111213141516171819202122db.collection.insert( &lt;document or array of documents&gt;, &#123; writeConcern: &lt;document&gt;, ordered: &lt;boolean&gt; &#125;)db.collection.insertOne( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;)db.collection.insertMany( [ &lt;document 1&gt; , &lt;document 2&gt;, ... ], &#123; writeConcern: &lt;document&gt;, ordered: &lt;boolean&gt; &#125;) 参数 类型 描述 document 文档或者数组 将要插入集合的文档或者文档数组。 writeConcern 文档 可选。待解释和细化。 ordered 布尔型 可选。插入数组时是否要按照顺序，默认为true。 实例：数据库：my_test，之前文章创建的数据库，创建数据库，参看这里。集合：test。插入记录如下： 123456789&#123; &quot;_id&quot; : ObjectId(&quot;5abb3b5bce69c048be080199&quot;), &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;1&quot;,&#125; 语句： 123456789db.test.insert(&#123; &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;1&quot;,&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;) 看到最后这句，表示插入一条记录成功。馈赠一条，为之后的例子做一个铺垫：123456789db.test.insert(&#123; &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;3&quot;,&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;) 查看一下： 12345678910111213141516171819&gt; db.test.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;5abb3b5bce69c048be080199&quot;), &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;1&quot;&#125;&#123; &quot;_id&quot; : ObjectId(&quot;5abb3b5bce69c048be080120&quot;), &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;3&quot;&#125; 我们看到了两条记录，查询的语法请参考关于查询的文档。 参考https://docs.mongodb.com/manual/tutorial/insert-documents/。https://docs.mongodb.com/manual/reference/method/db.collection.insert/#db.collection.insert。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>插入</tag>
        <tag>文档</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法及常见用例：touch]]></title>
    <url>%2Flinux%2Fshell-command-touch.html</url>
    <content type="text"><![CDATA[touch命令用来创建文件，也可以更改和修改一个文件的时间戳。 概要touch [选项]... 文件... 描述touch命令用来创建文件，也可以更改和修改一个文件的时间戳。Linux中的每个文件都与时间戳相关联，而且每个文件都存储上次访问时间，上次修改时间，上次更改时间的信息。因为，无论何时创建一个新文件，访问或者修改现有文件，时间戳都会被自动更新。 命令选项Linux中的文件有三个时间： access time（atime）：访问时间，对一次文件的内容就会更新。例如cat，vi/vim，cp，touch命令。 modification time（mtime）：修改时间，对文件内容修改一次就会更新。例如touch，vi/vim命令。 status time（ctime）：状态改动时间。通过chmod/chown/chgrp等命令更改一次文件属性，通过touch准确地修改时间等，这个时间就会更新。例如mv，touch，chmod/chown/chgrp，vi/vim等命令。 touch命令选项： -a，只改变访问时间。 -c，如果文件不存在，那就不创建。 -d，更新访问时间和修改时间。 -m，只改变修改时间。 -r，将参照文件ref_file相应的时间戳作为指定文件file时间戳的新值。 -t，用指定的时间创建文件，格式是[[CC]YY]MMDDhhmm[.SS]。CCYY的范围在1969~2068之内。SS为秒数，范围在0~61之间，这样可以处理闰秒。由于系统的限制，早于1970年1月1日的时间是错误的。 示例：1. 创建空文件描述：若文件不存在，使用touch命令可以轻松地创建一个空文件，或是创建多个。如果文件已存在，那么文件的3个时间：修改时间（mtime）、状态改动时间（ctime）和访问时间（atime）都会被更新为当前时间。stat命令可以查看文件时间。命令：touch my_onestat my_onetouch my_one my_two my_three输出： 示例：2. 只改变文件的修改时间（mtime）和状态改动时间（ctime）描述：只改变my_three文件的修改时间为当前时间，同时状态改动时间会在命令执行后更新为当前时间。这个操作并不需要修改文件内容。-m选项只更改文件的修改时间。命令：touch -m my_three输出： 示例：3. 只改变文件访问时间（atime）和状态改动时间（ctime）描述：只改变my_three文件的访问时间为当前时间，同时状态改动时间会在命令执行后更新为当前时间。如果文件不存在，会创建新的空文件。-a选项只更改文件的访问时间。命令：touch -a my_three输出： 示例：4. 指定文件的访问时间和修改时间描述：同时设置文件的访问时间和修改时间为指定时间，同时会更新状态改变时间为当前命令执行后的时间。如果文件不存在，会创建新的空文件。-d选项同时改变文件的访问时间和修改时间。命令：touch -d &quot;2018-06-14 14:00:00&quot; my_three输出： 描述：将my_three文件的访问时间和修改时间修改成两天前。touch还支持像date命令那样修改文件的时间。命令：touch -d &quot;2 days ago&quot; my_three输出： 示例：5. 避免创建新文件描述：更新atime、ctime、mtime，如果文件不存在，-c选项不会创建新的文件。命令：touch -c leena输出： 示例：6. 使用另一个文件的时间戳描述：-r选项将my_three的时间戳作为my_two文件的时间戳的新值，这两个文件有相同的时间戳。命令：touch -r my_three my_two输出： 示例：7. 使用指定的时间戳创建一个文件描述：将my_four文件的时间戳指定为1997年6月14日17点00分55秒。时间格式是[[CC]YY]MMDDhhmm[.SS]。命令：touch -t 199706141700.55 my_four输出：]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法及常见用例：awk]]></title>
    <url>%2Flinux%2Fshell-command-awk.html</url>
    <content type="text"><![CDATA[awk命令在文件或字符串中基于指定规则浏览和抽取信息。 命令功能awk是一种小巧的编程语言及命令行工具。（其名称得自于它的创始人Alfred Aho、Peter Weinberger 和 Brian Kernighan姓氏的首个字母）。它非常适合服务器上的日志处理，主要是因为awk可以对文件进行操作，通常以可读文本构建行。awk命令在文件或字符串中基于指定规则浏览和抽取信息。awk抽取信息后，才能进行其他文本操作，awk脚本通常用来格式化文本文件中的信息。 命令格式有三种方式调用awk，第一种是命令行方式，例如：awk [-F field-separator] &#39;commands&#39; input-file(s)awk默认使用空格作为缺省的域分隔符。如果要浏览诸如passwd文件，此文件是以冒号作为分隔符，则必须指明-F选项。例如：awk -F : &#39;commands&#39; input-file第二种方式是将所有awk命令插入一个文件，并使awk程序可执行，然后用awk命令解释器作为脚本的首行，以便通过键入脚本名称来调用它。第三种方式是将所有的awk命令插入一个单独文件，然后调用：awl -f awk-script-file input-file(s)-f选项指明在文件awk-script-file中的awk脚本，input_file(s)是使用awk进行浏览的文件名。 awk脚本代码结构awk脚本的代码结构很简单，就是一系列的模式（pattern）和动作（action）。 12345678# commentPattern1 &#123; ACTIONS; &#125;# commentPattern2 &#123; ACTIONS; &#125;# commentPattern3 &#123; ACTIONS; &#125;# commentPattern4 &#123; ACTIONS; &#125; 扫描文档的每一行时都必须与每一个模式进行匹配比较，一次只匹配一个模式。 12this is line 1this is line 2 this is line 1这行会先Pattern1进行匹配，如果匹配成功，就会执行ACTIONS。然后this is line 1会和Pattern2进行匹配，如果匹配失败，就调到Pattern3进行匹配，以此类推。一旦所有的模式都匹配过了，this is line 2就会以同样的步骤进行匹配。其他的行也一样，直到读取完整个文件。这就是awk的运行模式。 数据类型awk仅有两个主要的数据类型：字符串和数字，它们可以相互转换。在ACTIONS部分使用=操作符给变量赋值，可以在任意时刻、任意地方声明和使用变量，也可以使用未初始化的变量，默认是空字符串。awk有数组类型，并且它们是动态的一维关联数组。 模式模式分为三大类：正则表达式、布尔表达式和特殊模式。 所有模式都是可选的，下面的脚本形式会对输入的每一行都会简单地执行ACRIONS。{ ACTIONS } 特殊的模式模式包括两个特殊字段：BEGIN和END。BEGIN在所有输入未被处理之前，即文本浏览动作之前进行匹配。可以初始化脚本变量和所有种类的状态的主要地方。END会在所有的输入都被处理完后，即完成文本浏览动作后进行匹配。可以在退出前进行清除工作和一些最后的输出。最后一类模式，要把它进行归类有点困难。它处于变量和特殊值之间，我们通常称它们为域（Field）。而且名副其实。 域1234567891011# According to the following line## $1 $2 $3# 00:34:23 GET /foo/bar.html# _____________ _____________/# $0 # Hack attempt?/admin.html$/ &amp;&amp; $2 == &quot;DELETE&quot; &#123;print &quot;Hacker Alert!&quot;;&#125; 域（默认地）由空格分隔。$0域代表了一整行的字符串。$1 域是第一块字符串（在任何空格之前），$2\$域是后一块，以此类推。awk执行时，其浏览域标记为$1, $2, $3…$n。这种方式称为域标识。使用$1, $3标识表示第1和第3域。使用$0$标识表示所有域。awk浏览到一新行时，即到达域的记录末尾，执行新记录下一行的读动作，重新设置域分隔。 动作最常用和最有用的行为： 123456789101112&#123; print $0; &#125; # prints $0. In this case, equivalent to &apos;print&apos; alone&#123; exit; &#125; # ends the program&#123; next; &#125; # skips to the next line of input&#123; a=$1; b=$0 &#125; # variable assignment&#123; c[$1] = $2 &#125; # variable assignment (array) &#123; if (BOOLEAN) &#123; ACTION &#125;else if (BOOLEAN) &#123; ACTION &#125;else &#123; ACTION &#125;&#125;&#123; for (i=1; i&lt;x; i++) &#123; ACTION &#125; &#125;&#123; for (item in c) &#123; ACTION &#125; &#125; awk里的变量都是全局变量。 函数函数的通用文档(regular documentation) 1&#123; somecall($2) &#125; 用户定义的函数： 123456789# function arguments are call-by-valuefunction name (parameter-list) &#123; ACTIONS; #same actions as usual&#125;# return is valid keywordfunction add (val) &#123;return val+1;&#125; 实用命令实例：0. 新建测试文件描述：新建一个device文件，其中(1)为序号，(2)为Android版本，(3)为访问时间，(4)为IP，(5)为访问次数。本文大部分实例根据这一文件进行说明。输出： 实例：1. 抽取域描述：打印第1个（序号）域和第2个（Android版本）域的内容。print用来输出其后跟着的内容，用大括号把print语句括起来，表示一个打印动作。输出： 实例：2. 打印所有记录描述：打印所有记录。$0代表所有域。命令：awk &#39;{print $0}&#39; device输出： 实例：3. 打印报告头描述：在序号和IP地址之间用一些空格使之更容易划分，也可以在域间使用tab键加以划分。本例中加入NO和IP两个信息头以及中划线，\n启动新行，并在\n下一行启动打印文本操作。打印信息头放置在BEGIN模式部分，因为打印信息头被界定为一个动作，必须用大括号括起来。在awk查看第一条记录前，信息头被打印。命令：awk &#39;BEGIN {print &quot;NO IP\n------------------------&quot;} {print $1&quot;\t&quot;$4}&#39; device输出： 实例：4. 打印信息尾描述：在末行加入end of report信息。END语句在所有文本处理动作执行完之后才被执行，在脚本中的位置是在主要动作之后。命令：awk &#39;BEGIN {print &quot;Version\n-------&quot;} {print $2} END {print &quot;end-of-report&quot;}&#39; device输出： 实例：5. 错误信息提示描述：如果将在awk命令中缺少一个双引号，awk将返回错误提示信息。命令：awk &#39;BEGIN {print &quot;Version\n-------&quot;} {print $2} END {print &quot;end-of-report}&#39; device输出： 注意：在碰到awk错误时，应从以下几点进行排查： 确保整个awk命令引用单引号括起来。 确保命令内所有引号成对出现。 确保用花括号括起动作语句，用圆括号括起条件语句。 可能忘记使用花括号。 描述：如果查询的文件不存在，将得到以下错误信息：命令：awk &#39;END {print NR}&#39; device.txt输出： 条件操作符实例：1. 匹配描述：如果field-4以数字4开头，打印它。如果条件满足，则打印匹配的记录行。符号~后紧跟正则表达式，使一域号匹配正则表达式，也可以使用if语句。awk的if后面的条件用()括起来。^尖角符号表示行首。命令：awk &#39;{ if ($4 ~ /^4/) print $0}&#39; device输出： 等同于： 实例：2. 精确匹配描述：精确匹配访问次数为1次的记录，确保不匹配访问次数为15次的记录。使用等号==，并用单引号括起条件，也可以使用if语句。命令：awk &#39;$5==&quot;1&quot; {print $0}&#39; device或者：awk &#39;{if($5==/1/) print $0}&#39; device输出： 实例：3. 不匹配描述：不匹配IP地址以4开头的记录。使用!~表示不匹配。命令：awk &#39;$4 !~ /^4/&#39; device或者：awk &#39;{ if ($4 !~ /^4/) print $0}&#39; device输出： 注意这里不能用!=，因为用引号或者/括起了^4，将只匹配4而不匹配49.65.119.165等。如果查询非49.65.119.165的记录，可做如下操作：awk &#39;$4 != &quot;49.65.119.165&quot;&#39; device 实例：4. 小于，小于等于，大于，大于等于描述：匹配访问次数小于序号的记录。同样的有小于等于（&lt;=），大于（&gt;），大于等于（&gt;=）。命令：awk &#39;$4 !~ /^4/&#39; device或者：awk &#39;{ if ($4 !~ /^4/) print $0}&#39; device输出： 实例：5. 设置大小写描述：匹配含有前面是i或I，后面是OS的记录。[]符号可匹配[]内任意字符或单词。命令：awk &#39;/[iI]OS/&#39; device输出： 实例：6. 任意字符描述：匹配Android版本，第八个字符是7，打印它。表达式/^…….7/表示行首前7个字符任务，第八个是7。命令：awk &#39;$2 ~ /^.......7/&#39; device输出： 实例：7. 或关系匹配描述：匹配IP地址以4或者3开头的记录。竖线符|意为两边模式之一。可以得到与[]表达式相同的结果。命令：awk &#39;$4 ~ /^(4|3)/&#39; device输出： 注意，在使用竖线符时，语句必须用圆括号括起来。另外，除了字符重复出现外，其他的正则表达式在awk中都是合法的。 实例：8. AND 描述：匹配Android版本在7.0以上，并且IP地址以4开头的记录。OR，非与之类似。命令：awk &#39;$2 ~ /^.......7/ &amp;&amp; $4 ~ /^4/&#39; device等同于：awk &#39;{ if ($2 ~ /^.......7/ &amp;&amp; $4 ~ /^4/) print $0} &#39; device输出： awk内置变量awk内置变量如下： 1234567891011BEGIN &#123; # Can be modified by the userFS = &quot;,&quot;; # Field SeparatorRS = &quot;n&quot;; # Record Separator (lines)OFS = &quot; &quot;; # Output Filed SeparatorORS = &quot;n&quot;; # Output Record Separator (lines)&#125;&#123; # Can&apos;t be modified by the userNF # Number of Fileds in the current Record (lines)NR # Number of Records seen so farARGV / ARGC # Script Arguments&#125; NF：支持记录域个数，在记录被读之后再设置。NR：已读的记录数。FILENAME：告知系统目前正在浏览的实际文件，因为awk可以同时处理许多文件。 实例：1. NF、NR、FILENAME 描述：所有记录被打印，并带有记录号（第二和第三列），并在最后输出文件名。使用NF变量显示每一条读记录中有多少个域（5个），使用NR显示已读的记录数，使用FILENAME显示正在处理的文件名。命令：awk &#39;{print NF,NR,$0} END {print FILENAME}&#39; device输出： 实例：2. 判断文件至少有一个记录 描述：先检查文件中至少有一个记录时才查询IP地址。命令：awk &#39;NR &gt; 0 &amp;&amp; $4 ~ /^4/&#39; device输出： 实例：3. 与echo结合使用 描述：将变量$PWD的返回值传入awk并显示其目录。需要指定域分隔符/。命令：echo $PWD | awk -F / &#39;{print $NF}&#39;输出： 描述：显示文件名。命令：echo &quot;/etc/vimrc&quot; | awk -F / &#39;{print $NF}&#39;输出： awk操作符 实例：1. 设置输入域到域变量名描述：赋值IP地址域为ip，版本域为version，查询版本大于7的记录，并打印IP地址和版本信息。命令：awk &#39;{ip=$4;version=$2; if (version ~ /*7*/) print ip&quot;&quot;version}&#39; device输出： 实例：2. 域值比较操作有两种方式测试数值域是否小于另一数值域。 在BEGIN中给变量名赋值。 在关系操作中使用实际数值。 描述：找出访问次数大于10次的所有记录。命令：awk &#39;{if ($5 &gt; 10) print $0}&#39; device输出： 实例：3. 修改数值域的值 当在awk中修改任何域时，实际输入文件是不可修改的，修改的只是保存在缓存里的awk副本，awk会在变量NR或NF变量中反映出修改痕迹。 描述：修改序号为6的记录，将其访问次数减一。命令：awk &#39;{if ($1==6) $5=$5-1; print $1, $2, $5 }&#39; device输出： 实例：4. 修改文本域 描述：修改序号为6的记录，将其版本修改为iOS11.2.3。修改文本域就是对其重新赋值。命令：awk &#39;{if ($1==6) ($2=&quot;iOS11.2.3&quot;); print $1, $2, $5 }&#39; device输出： 实例：5. 只显示修改记录 描述：只显示修改后序号为6的记录。命令：awk &#39;{if ($1==6) {$2=&quot;iOS11.2.3&quot;; print $2}; }&#39; device输出： 实例：6. 创建新的输出域 描述：创建新域6保存目前访问次数大于序号的减法值，表达式为’{$6=$5-$1}’，只打印其值大于零的序号和其新域值。在BEGIN部分加入tab键以对齐报告头。也可以赋给新域更有意义的变量名。命令：awk &#39;BEGIN {print &quot;IP\t Difference&quot;} {if ($5 &gt; $1) {$6=$5-$1; print $1 &quot;\t&quot; $6}}&#39; device输出： 实例：7. 增加列值 描述：使用+=累加访问次数的值。awk的每一个操作匹配时，如果没有说明打印记录，那默认会打印所有记录。命令：awk &#39;(total+=$5); END {print &quot;total visits : &quot; total}&#39; device输出： 实例：8. 文件长度相加 描述：查看当前目录中所有文件的长度及其综合，但要排除子目录，使用ls -l命令，然后管道输出到awk，awk首先剔除首字符d（/^[^d]/）的记录，然后将文件长度相加，并输出每一文件长度及在END部分输出所有文件的长度。命令：ls -l | awk &#39;/^[^d]/ {print $9&quot;\t&quot;$5} {total+=$5} END {print &quot;total KB: &quot; total}&#39;输出： 内置字符串函数 gsub类似于sed查找和替换。它允许替换一个字符串或字符为另一个字符串或字符，并以正则表达式的形式执行，第一个函数作用于记录$0，第二个gsub函数允许指定目标，如果未指定，默认是$0。index(s, t)函数返回目标字符串s中查询字符串t的首位置。length函数返回字符串s字符长度。match函数测试字符串s是否包含一个正则表达式r定义的匹配。split函数使用域分隔符fs，将字符串s划分为指定序列a。sprint函数类似于printf函数，返回基本输出格式fmt的结果字符串exp。sub(r, s)函数将用s代替$0中最左边最长的子串，该子串被（r）匹配。sub(s, p)返回字符串s在位置p后的后缀部分。substr(s, p, n)函数返回字符串s在位置p后长度为n的后缀部分。 实例：1. gsub 描述：匹配记录中访问时间为11:35的记录，修改为11:40。注意要用双引号括起来。命令：awk &#39;gsub(/11:35/, &quot;11:40&quot;) {print $0}&#39; device输出： 实例：2. index描述：匹配字符串Honey中，ney子串第一次出现的位置，即字符个数。命令：awk &#39;BEGIN {print index(&quot;Honey&quot;, &quot;ney&quot;)}&#39;输出： 实例：3. length 描述：匹配序号为6，第二个域的字符长度。也可以直接使用字符串。命令：awk &#39;$1==6 {print length($2) &quot;---&quot; $2}&#39; device输出： 实例：4. match 描述：match测试目标字符串是否包含查找字符的一部分，可以使用正则表达式。命令：在AWK中查找d，因其不存在，所以返回0。awk &#39;BEGIN {print match(&quot;AWK&quot;, /d/)}&#39;在AWK中查找K，因其存在，所有返回AWK中K出现的首位置字符数。awk &#39;BEGIN {print match(&quot;AWK&quot;, /K/)}&#39;在序号为6的记录中，查找Android的大版本号。awk &#39;$1==6 {print match($2, &quot;7&quot;)}&#39; device输出： 实例：5. split 描述：如果域中具有分隔符形式的字符串，使用split函数将其分隔，并保存到一个数组中，最后将数组的第一个元素打印出来。命令：awk &#39;BEGIN {print split(&quot;123#456#789&quot;, myarray, &quot;#&quot;)}&#39;输出： 实例：6. sub 描述：匹配所有Android，替换为android。注意只在模式第一次出现时进行替换操作。命令：awk &#39;sub(/Android/, &quot;android&quot;)&#39; device输出： 实例：7. substr 描述：匹配第二个域版本信息中，打印从第一个字符开始到第七个字符。如果给定的长度值远大于字符串长度，awk将从起始位置返回所有字符。另一种形式是返回字符串后缀或指定位置后面的字符。命令：awk &#39;$1==5 {print substr($2,1,7)}&#39; device输出： 实例：8. 从shell向awk传入字符串命令：使用管道将字符串powerful传入awk，返回其长度。echo &quot;powerful&quot; | awk &#39;{print length($0)}&#39;设置文件名为一变量，管道输出到awk，但会不带扩展名的文件名。STR=&quot;myawk.txt&quot; | echo $STR | awk &#39;{print substr($STR,1,5)}&#39;设置文件名为一变量，管道输出到awk，只返回其扩展名。TR=&quot;myawk.txt&quot; | echo $STR | awk &#39;{print substr($STR,7)}&#39; 输出： 字符转义 printf修饰符基本语法：printf([格式控制符], 参数)格式控制符通常在引号里。 awkprintf修饰符： awk printf格式： 实例：1. 字符转换描述：通过管道输出65到awk中，printf进行ASCII码字符转换。命令：echo &quot;65&quot; | awk &#39;{printf (&quot;%c\n&quot;, $0)}&#39;或者awk &#39;BEGIN {printf &quot;%c\n&quot;, 65}&#39;输出： 描述：数字1024转换为浮点数之后，被加入了六个小数点。命令：awk &#39;BEGIN {printf &quot;%f\n&quot;, 1024}&#39; 输出： 实例：2. 格式化输出 描述：BEGIN后的第一个花括号嵌入头信息，第二个花括号打印所有用户的IP地址和访问时间，要求IP地址左对齐，23个字符长度，后跟访问时间。命令：awk &#39;BEGIN {print &quot;IP\t\t\tTime&quot;} {printf &quot;%-23s %s\n&quot;, $4, $3}&#39; device 输出： 实例：3. 向一行awk命令传值 描述：在命令行中设置VISITS等于10，然后传入awk中，查询访问次数大于10的所有记录。命令：awk &#39;{if($5 &gt; VISITS) print $0} &#39; VISITS=10 device输出： 描述：用管道将df -k传入awk，然后抽出第四列，即剩余可利用空间容量。使用$4 ~ /^[0-9]/取得容量数值，最后对命令行if($4 &lt; TRIGGER)上变量TRIGGER的值进行查询。查看文件系统空间容量，观察其是否达到一定水平。因为要监视的已使用空间容量不断在变化，所以需要再命令行指定一个触发值。命令：df -k | awk &#39;($4 ~ /^[0-9]/) {if ($4 &lt; TRIGGER) printf &quot;%-15s %s\n&quot;,$6,$4}&#39; TRIGGER=930000输出： 描述：打印当前注册用户，并加入一定信息。命令：who | awk &#39;{print $1 &quot; is logged on&quot;}&#39;输出： 描述：传入环境变量LOGNAME，显示当前用户名。命令：who | awk &#39;{if ($1 == user) print $1&quot; you are connected to &quot; $2}&#39; user=$LOGNAME&quot;}&#39;输出： 实例：4. awk脚本文件 描述：第一行#! /usr/bin/awk -f告知脚本系统awk命令的位置。在脚本文件后键入文件名之前，需要先对脚本文件加入可执行权限。命令：chmod u+x user_tot.awkuser_tot.awk脚本文件： 描述：执行user_tot.awk脚本文件。命令：./user_tot.awk device输出： 实例：5. 在awk中使用FS变量 描述：从/etc/passwd文件中抽取第1和第5域，通过FS变量，指定冒号:分隔passwd文件域。第1域时账号名，第5域是账号所有者。命令：chmod u+x passwd.awk | ./passwd.awk /etc/passwd输出： 实例：6. 向awk脚本传值 向awk脚本传值与向awk一行命令传值的方式大体相同，格式为：awk script_file var=value input_file 描述：对比检查文件中域号和指定数字。注意不要忘了增加脚本的可执行权限。命令：chmod u+x fieldcheck.awk | ./fieldcheck.awk MAX=7 FS=&quot;:&quot; /etc/passwd输出： 描述：从du命令获得输入，并输出块和字节数。命令：chmod u+x duawk.awk | du /root | ./duawk.awk输出： 实例：9. awk数组 描述：用split将123#456#789划分开，并存入myarray数组，再使用循环打印各数组元素。命令：chmod u+x duawk.awk | du /root | ./duawk.awk输出： 实例：10. 处理由通配符指定的多个文件名 描述：打印当前目录中以.txt结尾的文件。nextfile告诉awk停止处理当前的输入文件。下一个输入记录读取来自下一个输入文件。命令：awk &#39;{ print FILENAME; nextfile } &#39; *.txtawk &#39;BEGIN{ print &quot;Starting...&quot;} { print FILENAME; nextfile }END{ print &quot;....DONE&quot;} &#39; *.txt输出：]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下shell命令用法及常见用例：sed]]></title>
    <url>%2Flinux%2Fshell-command-sed.html</url>
    <content type="text"><![CDATA[sed是stream editor（流式编辑器）的缩写，是一个非交互式的流编辑器，用于过滤或者转换文本。未完待续… 概要sed 选项… [脚本] [输入文件…] 描述sed编辑器被称作流编辑器(stream editor)，和普通的交互式文本编辑器恰好相反。在交互式文本编辑器中(比如vim)，你可以用键盘命令来交互式地插入、删除或替换数据中的文本。流编辑器则会在编辑器处理数据之前基于预先提供的一组规则来编辑数据流。sed编辑器可以根据命令来处理数据流中的数据，这些命令要么从命令行中输入，要么存储在一个命令文本文件中。sed编辑器会执行下列操作。(1) 一次从输入中读取一行数据。(2) 根据所提供的编辑器命令匹配数据。(3) 按照命令修改流中的数据。(4) 将新的数据输出到STDOUT。 在流编辑器将所有命令与一行数据匹配完毕后，它会读取下一行数据并重复这个过程。在流编辑器处理完流中的所有数据行后，它就会终止。 由于命令是按顺序逐行给出的，sed编辑器只需对数据流进行一遍处理就可以完成编辑操作。这使得sed编辑器要比交互式编辑器快得多，你可以快速完成对数据的自动修改。 理解这个命令使用起来有些复杂，复杂在于功能强大，需要逐步消化。 常见用例实例 替换input.txt文件中所有的“hello”为“world”，并且输出到output.txt中。 1sed &apos;s/hello/world/&apos; input.txt &gt; output.txt 这可能是最常用的例子了（至少在我工作这么多年的经验中），这里使用了sed的命令s。如果想输出到原文件的话，使用-i参数。 1sed -i &apos;s/hello/world/&apos; input.txt 这个在mac下表现会不一样，参考：https://blog.csdn.net/cuiaamay/article/details/49495885。 参考：https://www.gnu.org/software/sed/manual/sed.html。]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记：创建数据库]]></title>
    <url>%2Fmongodb%2Fmongodb-database-create.html</url>
    <content type="text"><![CDATA[MongoDB如何创建数据库。 前言 初衷：MongoDB的使用还是有一定难度的，官网讲解的一环牵扯一环，不容易一下子把握住重点，也不利于快速查询操作，所以整理一下。 这是之前写这篇笔记的初衷，现在发生了一些修改，现在本篇文章是作为MongoDB学习笔记的一部分，但也可以作为独立的一个MongoDB知识点学习的笔记。 本篇文章整理一下MongoDB如何创建数据库。 正文进入mongo shell（关于mongo shell，需要先安装并且启动好MongoDB）： 1234root@iZhp3fz3iqsadyes2s8ayeZ:~# mongoMongoDB shell version: 2.6.10connecting to: test...... 如果没有mongo这个命令，表示路径没有配置好。上面的显示表示这个MongoDB的版本是2.6.10，进入数据库后，默认会进入到test库。 使用以下的操作就可以切换数据库，如果数据库不存在，且会创建一个新的数据库：1use &lt;database&gt; 尖括号表示需要你替换的变量，就是你的数据库名称，关于数据库名称是有一些限制的，可以去查一下。 如果数据库存在，这条命令会切换到该数据库，如果不存在，则创建并切换到该数据库。 实例：创建一个数据库，名字为my_test。 12use my_testswitched to db my_test 这样，创建成功一个数据库，这种操作要比MySQL的创建数据库容易很多。 参考https://docs.mongodb.com/manual/core/databases-and-collections/]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>database</tag>
        <tag>create</tag>
        <tag>创建数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记：创建数据集合]]></title>
    <url>%2Fmongodb%2Fmongodb-collection-create.html</url>
    <content type="text"><![CDATA[完整的MongoDB学习笔记位于IT老兵博客。 MongoDB如何创建数据集合（collection）。 前言本篇文章整理一下MongoDB如何创建数据集合。 正文这节有点混乱，也有点尴尬，因为原本MongoDB就有些“没有规矩”。 关于快速创建一个集合，需要参考插入这一节，因为如果集合不存在的情况下，插入一条记录就会创建集合，这是非常方面的一种操作。 再举个例子，如下： 12345678910db.test.insert(&#123; &quot;_id&quot; : ObjectId(&quot;5abb3b5bce69c048be080199&quot;), &quot;meta&quot; : &#123; &quot;createAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;), &quot;updateAt&quot; : ISODate(&quot;2018-03-28T06:51:07.579Z&quot;) &#125;, &quot;a&quot; : &quot;1&quot;, &quot;b&quot; : &quot;1&quot;,&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;) 这样即会创建集合test，又会给这个集合插入一条记录。 如果非要规矩地创建（其实是可以设置一些选项），那么： 12345678910111213db.createCollection(&lt;name&gt;, &#123; capped: &lt;boolean&gt;, autoIndexId: &lt;boolean&gt;, size: &lt;number&gt;, max: &lt;number&gt;, storageEngine: &lt;document&gt;, validator: &lt;document&gt;, validationLevel: &lt;string&gt;, validationAction: &lt;string&gt;, indexOptionDefaults: &lt;document&gt;, viewOn: &lt;string&gt;, pipeline: &lt;pipeline&gt;, collation: &lt;document&gt;, writeConcern: &lt;document&gt;&#125; ) 参数 类型 描述 name 字符串 要创建的集合的名称。 options 文档 可选。一大堆选项，暂时没用到，将来再补充了。 参考https://docs.mongodb.com/manual/reference/method/db.createCollection/#db.createCollection。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>collection</tag>
        <tag>数据集合</tag>
        <tag>创建</tag>
      </tags>
  </entry>
</search>
